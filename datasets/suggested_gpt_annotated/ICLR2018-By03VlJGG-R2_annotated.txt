This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images.  A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens. \n\nThis paper reads well and the results appear sound.  Unfortunately, the contribution seems rather small to be accepted for ICLR.  This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results. \n\n* Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info.  Isn't there any existing benchmark where this could have an impact? \n* The much better performance of ConvE is worrying there.  It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those.  \n* Are we sure that the textual description do not explicitly contain the information of the triple to be predicted?  This would explain the massive gains in Yago. \n* For Table 8, the similarities are not striking.  What were the nearest neighboring posters in the original VGG space? They should not be that bad too. \n* The work on multimodal embeddings like \"Multimodal Distributional Semantics\" by Bruni et al. or \"Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception.\" by Kiela et al. could be discussed/cited[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-NEG],[ETH-NEU]]