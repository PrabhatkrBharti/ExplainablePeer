This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN.  Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model. \n\nThe underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique. \n\nI am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset.  As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset.  For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component. \n\nThis reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains. \n\nOverall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights. \n\n(extremely minor typo: \"One popular possibility from L2S is go the full reduction route down to binary classification\")[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]