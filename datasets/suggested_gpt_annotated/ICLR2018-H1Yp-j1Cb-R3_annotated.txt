The paper applies tools from online learning to GANs.  In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms.  The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game.  \n\nOverall the paper is very well written.  The theory is significant to the GAN literature, probably less so to the online learning community.  In practice, with deep D, trained by single gradient update steps for G and D, instead of the \"argmin\" in Algo 1., the assumptions of the theory break.  This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true.  However, this is where I have issues with the work:\n\n1) In all quantitative results, Chekhov GAN do not significantly beat unrolled GAN.  Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G.  So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G, or simply due to the fact that it considers historical D models (which could be motivated by sth other than the online learning theory). \n\n2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out.  I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed, or compare them experimentally.  \n\n3) In the current state, it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty.  I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison.  Alternatively please show WGAN-GP and/or other method results in at least one or two experiments using the evaluation methods in the paper.  \n\nIn summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small.  But in this case, it is not.  So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory, nor it drastically improves practical results that it could become the standard technique in the literature[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]