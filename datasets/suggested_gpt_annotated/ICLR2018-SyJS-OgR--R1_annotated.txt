This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way.  On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy.  The paper is interesting and easy to follow.  \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance.  Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency? \n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models.  It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3. \tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]