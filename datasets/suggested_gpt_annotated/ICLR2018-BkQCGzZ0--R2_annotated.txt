This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder.  \nHowever, the experiments are too weak to demonstrate the effectiveness of using discrete representations. \nThe design of the experiments on language model is problematic. \nThere are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper.  \n\n\nGenerally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations.  \nHowever, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart.  \nThe proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people.  \nHow is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model? \nThe qualitative evaluation on 'Deciperhing the Latent Code' is not enough either.  \nIn addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively. \nOne suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case. \n\nOverall, this paper is more suitable for the workshop track . It also needs a lot of more studies on related work.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]