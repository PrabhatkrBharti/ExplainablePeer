This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185).  The generative model defines the process that produces the dataset.  This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable.  This corresponds to a mixture of Neural Statisicians.  The authors suggest that such a model could help with disentangling factors of variation in data.  In the experiments they only consider training the model with the context selection variable and the data variables observed. \n\nUnfortunately there is minimal quantitative evaluation (visualizing 264 MNIST samples is not enough).  The only quantitative evaluation is in Table 1, and it seems the model is not able to generalize reliably to all rotations and all digits.  Clearly, we can't expect perfect performance, but there are some troubling results: 5.2 accuracy on non-rotated 0s, 0.0 accuracy on non-rotated 6s.  Every digit has at least one rotation that is not well classified, so this section could use more discussion and analysis.  For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled?  Moreover, the context selection variable \"a\" should be considered part of the dataset, and as such the paper should report how \"a\" was selected. \n\nThis model is a relatively simple extension of the Neural Statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation.  I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs.  Still, the paper as it stands is not complete, and I encourage the authors to followup with more thorough quantitative empirical evaluations.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]