The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets.  There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture.  The Related work part of this paper makes a good description of both topics.   \n\nI would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets).   The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM.  The presentation may raise some misunderstanding.  A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting.  \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks.  These two proposed neural network models seem performing well empirically.  \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]