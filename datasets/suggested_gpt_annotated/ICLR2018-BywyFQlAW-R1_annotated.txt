Overview:\nThis paper proposes an approach to curriculum learning, where subsets of examples to train on are chosen during the training process.  The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq. 2).  The set is optimized to be as hard as possible (maximize loss), which results in a min-max problem.  This is in turn optimized (approximately) by alternating between gradient-based loss minimization and submodular maximization.  The theoretical analysis shows that if the loss is strongly convex, then the algorithm returns a solution which is close to the optimal solution.  Empirical results are presented for several benchmarks. \nThe paper is mostly clear and the idea seems nice.  On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below). \n\nComments:\n- The theoretical result (thm. 1) studies the case of full optimization, which is different than the proposed algorithm (running a fixed number of weight updates).  It would be interesting to show results on sensitivity to the number of updates (p). \n- The algorithm requires tuning of quite a few hyperparameters (sec. 3). \n- Approximating a cluster with a single sample (sec. 2.3) seems rather crude.  There should be some theoretical and/or empirical study of its effect on quality of the solution. \n\nMinor/typos:\n- what is G(j|G\\j) in eq. (9)? \n- why cite Anonymous (2018) instead of Appendix...? \n- define V in Thm. 1.\n- in eq. (4) it may be clearer to denote g_k(w). Likewise in eq. (6) \\hat{g}_\\hat{A}(w), and in eq. (14) \\tilde{g}_{\\cal{A}}(w). \n- figures readability can be improved.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]