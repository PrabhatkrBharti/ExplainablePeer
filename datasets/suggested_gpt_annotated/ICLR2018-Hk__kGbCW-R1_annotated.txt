The article proposes to use dense skip-connections on the \"vertical\" (between-layers) connections of recurrent networks.  Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to. \n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks. \n\nI am not convinced by the presented results for the following reasons:\n1) the paper introduces two concepts - the dense skip-connections and the multi-head attention.  Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention? \n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines.  What happens for deeper and narrower baselines that have a similar number of parameters? \n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \"jumps\" in the training curves as signs of learning rate anneal).  Maybe this is what helps?  I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated? \n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written. \n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial. \n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEG],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEU]]