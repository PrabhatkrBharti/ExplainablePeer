[ =========================== REVISION ===============================================================]\nI am satisfied with the answers to my questions.  The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating.  However I am fine accepting it. \n[ ============================== END OF REVISION =====================================================]\n\nThis paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net).  Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization).  First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD. \n\nOverall the paper is well written.  The authors first introduce their suggested loss function and then go into details about what inspired its creation.  I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful \n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated.  On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used.  I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework. \n- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.  Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched.  \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem \n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better. \n\nMinor: fix margins in formula 2.7[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEU]]