This paper is about low-precision training for ConvNets.  It proposed a \"dynamic fixed point\" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format.  The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K.  According to the paper, this is the first time such kind of performance are demonstrated for limited precision training. \n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance. \n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]