Summary: \n\nThe authors consider the use of attention for sensor, or channel, selection.  The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels.  Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features.  The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data. \n\nReview:\n\nThe paper reads well,  but as a standard application of attention lacks novelty.  The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017).  Furthermore, while their approach is sold as a general sensor fusion technique,  most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase.  Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better!  The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better!  http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed\u2026\n\nOverall Assessment: \n\n In summary, the paper lacks novelty wrt technique, and as an \u201capplication-of-attention\u201d paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed.  As such, I recommend that the paper be rejected. \n\n\nAdditional comments: \n\n-\tThe experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised?  Were the baselines with concatenated features optimized independently?  Why is there no multi-channel baseline for the GRID results?  \n-\tIssue with noise bursts plot (Input 1+2 attention does not sum to 1) \n-\tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)\u2026\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]