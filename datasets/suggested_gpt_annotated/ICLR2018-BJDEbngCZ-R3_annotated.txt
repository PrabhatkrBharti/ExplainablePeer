The paper studies the global convergence for policy gradient methods for linear control problems.  \n(1) The topic of this paper seems to have minimal connection with ICRL.  It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully.  \n\n(2) I am not convinced if the main results are novel.  The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming.  The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration.  \n\n(3) The main results of this paper seem technical sound.  However, the results seem a bit limited because it does not apply to neural-network function approximator.  It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted.  I might have missed something here.  I strongly suggest that these results be submitted to a more suitable venue.\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]