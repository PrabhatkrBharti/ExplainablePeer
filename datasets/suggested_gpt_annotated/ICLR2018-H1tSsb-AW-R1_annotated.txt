This paper presents methods to reduce the variance of policy gradient using an action dependent baseline.  Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.  The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free \n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines. \n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case. \n(4) suggests using MC marginalization and also using the \"average\" action to improve computational feasibility \n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance \n\nThe suggested methods are empirically evaluated on a number of settings.  Overall action-dependent baseline outperform state-only versions.  Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate.  Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance. \n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.  In light of such results, one might change the policy space to enforce such structure. \n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.  Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state? \n- Eqn 11 should use \\simeq. \n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?  Consider expanding on that in section 4.5. \n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).  How would you train such baselines? \n- Figure 4 is impossible to read in print.  The fonts are too small for the numbers and the legends[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]