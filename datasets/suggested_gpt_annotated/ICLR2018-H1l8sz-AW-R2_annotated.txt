I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision. \nIt seems that the authors have made an effort to accommodate reviewers' comments.  I upgraded the rating. \n\n-----------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper considers the use of natural gradients for learning.  The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training.  The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost. \n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions. \n2. Regularizing the objective with KL div. seems promising, but expensive. \n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix. \n4. However, computing and inverting the Fisher information matrix is computationally expensive. \n5. One solution is to approximate the solution F^{-1} J using gradient descent.  However, still we need to calculate F.  There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher'). \n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \"good\" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective.  \n7. No large matrices need to be formed or inverted, however more passes needed per outer step. \n\nImportance:\nSomewhat lack of originality and poor experiments lead to low importance. \n\nClarity:\nThe paper needs major revision w.r.t. presenting and highlighting the new main points.  E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS). \n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks.  Beyond this, the paper does not provide any futher original idea.  So, slight to no novelty. \n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help?  This would require the combination of two Hessian matrices. \n\n2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results.  \n\nOverall:\nRejection[[CLA-POS],[JUS-POS],[DEP-NEG],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]