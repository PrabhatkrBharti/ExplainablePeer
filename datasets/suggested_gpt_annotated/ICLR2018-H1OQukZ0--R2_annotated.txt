\n# Summary of paper\nThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method.  \n\n# Summary of review\nI find the contribution to be incremental, and the validation weak.  Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution.  None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject.  \n\n# Detailed comments\n\n1. The distinction between parameters and hyperparameters (section 3) should be revised.  First, the definition of parameters should not include the word parameters.  Second, it is not clear what \"parameters of the regularization\" means.  Typically, the regularization depends on both hyperparameters and parameters.  The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are \"begin learnt\", just from different datasets. \n\n2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct.  This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015). \n\n3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to.  theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used). \n\n4. The hypothesis are not correctly specified.  Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated. \n\n5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this.  While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm. \n\n# Minor\n\nmissing . after \"hypergradient exactly\". \n\n\"we could optimization the hyperparam-\" (typo). \n\nReferences:\n Justin  Domke.    Generic  methods  for  optimization-based modeling.  In\nInternational Conference on Artificial Intelligence and Statistics, 2012[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]