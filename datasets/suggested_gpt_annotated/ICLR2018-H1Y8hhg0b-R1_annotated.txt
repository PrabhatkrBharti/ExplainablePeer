This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization.  The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations.  Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components.  The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods.  \n\nPros: \n\n- The paper is clearly written, self-contained and a pleasure to read.  \n- Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization \n\nCons: \n\n- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices  \n- It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc.    \n- If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]