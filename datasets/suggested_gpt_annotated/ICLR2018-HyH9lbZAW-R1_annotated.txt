The authors adapts stochastic natural gradient methods for variational inference with structured inference networks.  The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter.  The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters.  In the experiments the authors generally show improved convergence over SVAE. \n\nThe idea seems promising  but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.  The main motivation seems to be that it is easier to optimize. \n\n- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta.  Might this also suffer from non-convergence issues like you argue SVAE does?  Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward? \n- It was not clear to me why we should use a Gaussian approximation for the \\theta_NN parameters?  The prior might be Gaussian but the posterior is not?  Is this more of a simplifying assumption? \n- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.  Some examples of related work missing in this area is \"Variational Sequential Monte Carlo\" by Naesseth et al. (2017) / \"Filtering Variational Objectives\" by Maddison et al. (2017) / \"Auto-encoding sequential Monte Carlo\" Le et al. (2017). \n-  Section 2.1, paragraph nr 5, \"algorihtm\" -> \"algorithm\"\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]