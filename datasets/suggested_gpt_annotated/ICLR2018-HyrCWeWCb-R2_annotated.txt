This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time.  The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. \n\nThe major weakness of this paper is the unclear presentation.  For example, the algorithm is never fully described, though a handful variants are discussed.   How the off-policy version is implemented is missing.  \n\nIn experiments, why the off-policy version of TRPO is not compared.  Comparing the on-policy results, PCL does not show a significant advantage over TRPO.  Moreover, the curves of TRPO is so unstable, which is a bit uncommon.  \n\nWhat is the exploration strategy in the experiments?  I guess it was softmax probability.  However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. \n\nAnother issue is the discussion of the entropy regularization in the objective function.  This regularization, while helping exploration, do changes the original objective.  When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance.  Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]