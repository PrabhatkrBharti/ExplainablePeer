The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments.  The memory consists of a 2D array and includes trainable read/write mechanisms.  The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation).  The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved).  The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield.  \n\nMy main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture.  The downsampling experiment in Section 5.1 seems to indicate the contrary: downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location.  I'm concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g. for optimization reasons).  The short experiment in Appendix E seems to try and answer this question, but it's results are anecdotal at best.  \n\nIf good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes.  My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds, why bother with deep RL here? [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]