This paper suggests a reparametrization of the transition matrix.  The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks. \n\nThe paper is well-written and authors explain related work adequately.  The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.  The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.  \n\nI have two comments on the experiment section:\n\n- Choice of experiments.  Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.  For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs.  For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported. \n\n- Stopping condition. \ The plots suggest that the optimization has stopped earlier for some models.  Is this because of some stopping condition or because of gradient explosion?  Is there a way to avoid this? \n\n- Quality of figures.  Figures are very hard to read because of small font.  Also, the captions need to describe more details about the figures.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]