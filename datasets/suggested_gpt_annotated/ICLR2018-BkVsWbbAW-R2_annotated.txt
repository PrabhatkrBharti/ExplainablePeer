This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime.  Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction.  The method is tested on standard sequential MNIST variants as long as a class incremental variant.  Superior performance to recent baselines (e.g. EWC) is reported in several cases.  Interesting parallels with human cortical and hippocampal learning and memory are discussed. \n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines.  The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though.  There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL.  Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains.  In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to \u2018negative interference\u2019 between tasks [1, 2].  I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting.  The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL. \n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]