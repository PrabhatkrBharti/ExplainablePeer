The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference.  The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference).  The experiments\nperformed show that the Bayesian view of batch normalization performs similarly\nas MC dropout in terms of the estimates of uncertainty that it produces. \n\nQuality:\n\nI found the quality to be low in some aspects.  First, the description of what\nis the prior used by batch normalization in section 3.3 is unsatisfactory.  The\nauthors basically refer to Appendix 6.4 for the case in which the weight decay\npenalty is not zero.  The details in that Appendix are almost none, they just\nsay \"it is thus possible to derive the prior...\".  \n\nThe results in Table 2 are a bit confusing.   The authors should highlight in\nbold face the results of the best performing method.  \n\nThe authors indicate that they do not need to compare to variational methods\nbecause Gal and Ghahramani 2015 compare already to those methods.   However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data.   In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54 \n\nTherefore, the claim that \n\n\"Since we have established that MCBN performs on par with MCDO, by proxy we\nmight conclude that MCBN outperforms those VI methods as well.\"\n\nis not valid. \n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015).  However, Gal and Ghahramani\n(2015) actually follow Hern\u00e1ndez-Lobato and Adams, 2015 so the correct\nreference should be the latter one. \n\nClarity:\n\nThe paper is clearly written and easy to follow and understand. \n\nI found confusing how to use the proposed method to obtain estimates of\nuncertainty for a particular test data point x_star.  The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch. \nHow is this actually done in practice? \n\nOriginality:\n\nThe proposed contribution is original.  This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal. \n\nSignificance:\n\nThe paper's contributions are significant.  Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant.  Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n[[CLA-POS],[JUS-NEG],[DEP-POS],[FAI-NEG],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-POS],[ETH-NEU]]