The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields.  To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs.  The grid tensors are matricized and the ranks of the resultant matrices are compared.  The paper obtains a lower bound on the rank of the resultant grid tensors , and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs.  Assuming that the result carries over to ConvNets, I find this result to be very interesting.   While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same . This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]