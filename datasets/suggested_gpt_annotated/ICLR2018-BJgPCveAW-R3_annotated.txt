The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters, \n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices. \nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers. \n2) The authors reject the technique of 'Deep compression' as being impractical.  I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network. \n\np3. What does 'normalized' mean?  Batch-norm? \np3. Are you using an L2 weight penalty?  If not, your fully-connected baseline may be unnecessarily overfitting the training data. \np3. Table 1. Where do the choice of CL Junction densities come from?  Did you do a grid search to find the optimal level of sparsity at each level? \np7-8. I had trouble following the left/right & front/back notation. \np8. Figure 7. How did you decide which data points to include in the plots?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]