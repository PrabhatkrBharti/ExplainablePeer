This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions.  It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions.  It also evaluates the number of regions of small networks during training.  \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar.  \n\nThe improved lower bound given in Theorem 6 is very modest but neat.  Theorem 5 follows easily from this.  \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel.  \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks.  It presents experiments during training and with varying network sizes.  These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training.  \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]