An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated. \n\nThe approach is based on `profile coefficients\u2019 which are learned for every channel in a convolution layer, or for every column in the fully connected layer.  Based on the magnitude of this profile coefficient, which determines the importance of this `filter,\u2019 individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner. \n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn\u2019t fully utilize the deep net performance.  To address this issue a `loss aggregation\u2019 is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used. \n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime. \n\nSummary:\n\u2014\u2014\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method.  The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing.  In addition, the writing should be improved as it is often ambiguous.  See below for details.\n\nReview:\n\u2014\u2014\u2014\u2014\u2014\n1. Experiments are only provided on very small datasets. According to my opinion, this isn\u2019t sufficient to illustrate the effectiveness of the proposed approach.  As a reader I wouldn\u2019t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2.  Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent.  More specifically, while the profile coefficient is applied for every input element in Eq. (1), it\u2019s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer\u2019 is more confusing than clarifying.\n\n3.  The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot products\u2019. While sentences like those can be deciphered, they aren\u2019t that appealing. \n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc. \n\n5. Algorithm 1 and Algorithm 2 call functions that aren\u2019t described/defined. \n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]