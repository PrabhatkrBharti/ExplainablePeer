This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches.  The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence.  The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.  It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs.  \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction.  On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning.  As such, I am borderline on its acceptance. \n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \"chrisc\") that has significantly higher EM/F1 scores than the proposed model.  Why is this result not compared to in Table 1?  \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?  This may allow you to avoid truncation altogether.  Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient. \n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions? \n- What is the average number of sentences per document?  It's hard to get an idea of how reasonable the chosen truncation thresholds are without this. \n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased.  Did the authors try truncating after more words (e.g., 10k)? \n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works. \n- \"Krasner\" misspelled multiple times as \"Kramer\"[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]