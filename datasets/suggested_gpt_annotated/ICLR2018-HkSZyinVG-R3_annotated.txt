This is an emergency review, as the replacement of an overdue review.  \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit.  This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor.  \n\nHowever, the motivation above is not justified well.  No theoretic results are present to support this design.  Figure 4 shows the only experimental results to \u201csupport\u201d the motivation.  However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU.  As a result, the motivation does not seem clear. \n\nThe shift variables seem only useful when they are not shared for different pixels.  Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used).  The question is if it is worth adding so many pixel-wise parameters.  Moreover, the proposed formulation does not seem useful for the fully connected layer at any time.  \n\nThe experiments are limited.  Only the basic LeNet and another network are considered on Cifar-100.  The results are not as good as the state-of-the-art.  More importantly, the proposed activation functions reduce the errors only a bit (<0.5%).  Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n[[CLA-POS],[JUS-NEG],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEU],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEU]]