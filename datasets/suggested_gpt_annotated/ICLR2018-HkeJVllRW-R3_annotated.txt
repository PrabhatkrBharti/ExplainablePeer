Summary:\nThis paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep networks.  In this method, two new types of kernels are developed, namely the spatial-wise and channel-wise sparse-complementary kernels.  The authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the parameters.  By adding more filters or layers in the model while keeping the same FLOPs and parameters, the models with the proposed method outperform the regular convolution models.  The paper is easy to follow and the idea is interesting.  However, the novelty of the paper is limited and the experiments are not sufficient. \n\nStrengths:\n1. The authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolution.  \n\n2. The authors implement the proposed sparse-complementary convolution on NVIDIA GPU and achieved competitive speed under the same computational load to regular convolution. \n\n3. The authors demonstrated that, given the same resource budget, the wider networks with the proposed method are more efficient than the deeper networks due to the nature of GPU parallel mechanism. \n\nWeak points:\n\n1. The novelty of this paper is limited.  The main idea is to design complementary kernels that cover the same receptive field as the regular convolution.  However, the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernels.  Moreover, the experiments are not sufficient to support the arguments.  For example, how is the performance of a model containing SW-SC or CW-SC without deepening or widening the networks?  Without such experiment, it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernels. \n\n2. The relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear.  Which kernel is better and how to choose between them in a deep network?  There is no experimental proof in the paper. \n\n3. The proposed two kernels introduce sparsity in the spatial and channel dimension, respectively.  The two methods are used separately.  Is it possible to combine them together? \n\n4. The proposed method only considers the \u201c+-shape\u201d and \u201cx-shape\u201d sparse pattern.  Given the same receptive field with multiple complementary kernels, is the kernel shape important for the training?  There is no experimental result to verify this. \n\n5. As mentioned in the paper, there are many methods which introduce sparsity in the convolution layer, such as \u201crandom kernels\u201d, \u201clow-rank approximated kernels\u201d and \u201cmixed-shape kernels\u201d.  However, there is no experimental comparison with these methods. \n\n6. In the paper, the author mentioned another sparse-complementary baseline (sc-seq), which applies sparse kernels sequentially.  It yields smaller receptive field than the proposed method when the model depth is very small.  Indeed, when the model goes deeper, the receptive field becomes very close to that of the proposed method.  In the experiments, it is strange that this method can also achieve comparable or better results.  So, what is the advantage of the proposed \u201csc\u201d method compared to the \u201csc-seq\u201d method? \n\n\n8. Figure 5 is hard to understand.  This figure only shows that training shallower networks is more effective than training the deeper networks on GPU.  However, it does not mean training the wider networks is more efficient than training the deeper ones[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEU]]