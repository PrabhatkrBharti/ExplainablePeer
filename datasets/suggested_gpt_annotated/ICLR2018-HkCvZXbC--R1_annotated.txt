Summary: This paper studied the conditional image generation with two-stream generative adversarial networks.  More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label.  During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation.  An auxiliary \u201clabel difference cost\u201d was further introduced to encourage class information captured by the foreground generator.  Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline. \n\n== Novelty/Significance ==\nControllable image generation is an important task in representation learning and computer vision.  I also like the unsupervised learning through gating function and label difference cost . However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs). \n\n== Detailed comments ==\nThe proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region.  Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability . For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet.  \n\nGiven the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable.  However, I didn\u2019t see such detailed analysis as in the other papers on controllable image generation. \n\nIn Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp.  It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better) . Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned.  Is it possible to use all the binary attributes in the dataset. \n\nAlso, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks) .\n\nOverall, I believe the paper is interesting but not ready for publication.  I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation . Hopefully, the suggested studies will improve the quality of the paper in the future submission. \n\n== Presentation ==\nThe paper is readable but not well polished . \n\n-- In Figure 1, the \u201cG1\u201d on the right should be \u201cG2\u201d;\n-- Section 2.2.1, \u201cX_f\u201d should be \u201cx_f\u201d;\n-- the motivation of having \u201cz_v\u201d should be introduced earlier;\n-- Section 2.2.4, please use either \u201calpha\u201d or \u201c\\alpha\u201d but not both;\n-- Section 3.3, the dataset information is incorrect: \u201c20599 images\u201d should be \u201c202599 images\u201d;\n\nMissing reference:\n- - Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017. \n-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.\ n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]