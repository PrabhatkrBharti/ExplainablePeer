The manuscript introduces the sensor transformation attention networks, a generic neural architecture able to learn the attention that must be payed to different input channels (sensors) depending on the relative quality of each sensor with respect to the others.  Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown. \n\nFirst of all, I was surprised on the short length of the discussion on the state-of-the-art.  Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others). \n\nSecond, from a purely methodological point of view, STANs boil down to learn the optimal linear combination of the input sensors.  There is nothing wrong about this, but perhaps other more complex (non-linear) models to combine data could lead to more robust learning. \n\nThird, the experiments with synthetic noise are significant to a reduced extend.  Indeed, adding Gaussian noise to a replicated input is too artificial to be meaningful.  The network is basically learning to discard the sensor when the local standard deviation is high.  But this is not the kind of noise found in many applications, and this is clearly shown in the performances on real data (not always improving w.r.t state of the art).  The interesting part of these experiments is that the noise is not stationary, and this is quite characteristic of real-world applications.  Also, to be fair when discussion the results, the authors should say that simple concatenation outperforms the single sensor paradigm. \n\nI am also surprised about the baseline choice.  The authors propose a way to merge/discard sensors, and there is no comparison with other ways of doing it (apart from the trivial sensor concatenation).  It is difficult to understand the benefit of this technique if no other baseline is benchmarked.  This mitigates the impact of the manuscript. \n\nI am not sure that the discussion in page corresponds to the actual number on Table 3, I did not understand what the authors wrote.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]