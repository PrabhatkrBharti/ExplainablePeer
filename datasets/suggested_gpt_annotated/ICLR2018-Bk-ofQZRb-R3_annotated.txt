This is an interesting idea, and written clearly.  The experiments with Baird's and CartPole were both convincing as preliminary evidence that this could be effective.  However, it is very hard to generalize from these toy problems.  First, we really need a more thorough analysis of what this does to the learning dynamics itself.  Baring theoretical results, you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directly.  I think ideally, I would want to see this on Atari or some of the continuous control domains often used.  If this allows the removing of the target network for instance, in those more difficult tasks, then this would be a huge deal. \n\nAdditionally, I do not think the current gridworld task adds anything to the experiments, I would rather actually see this on a more interesting linear function approximation on some other simple task like Mountain Car than a neural network on gridworld.  The reason this might be interesting is that when the parameter space is lower dimensional (not an issue for neural nets, but could be problematic for linear FA) the constraint might be too much leading to significantly poorer performance.  I suspect this is the actual cause for it not converging to zero for Baird's, although please correct me if I'm wrong on that. \n\nAs is, I cannot recommend acceptance given the current experiments and lack of theoretical results.  But I do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it. \n\nPros:\nSimple, interesting idea\nWorks well on toy problems, and able to prevent divergence in Baird's counter-example \n\nCons:\nLacking in theoretical analysis or significant experimental results[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-POS],[ETH-NEU]]