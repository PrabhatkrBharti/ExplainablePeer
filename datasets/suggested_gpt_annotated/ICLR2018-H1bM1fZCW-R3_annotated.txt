The paper addresses an important problem in multitask learning.  But its current form has several serious issues.  \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible.  There are many things unclear. For example:\n\n-  it starts with talking about multiple tasks, and then immediately talks about a \"filter F\", without defining what the kind of network is being addressed.  \n\n- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks.  It is not clear how it is used. In particular, it is not clear how it is used to \"update the task weights\ "\n\n- Equation 2 seems sloppy. \u201cj\u201d appears as a free index on the right side, but it doesn\u2019t appear on the left side.  \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality. \n\nThe toy experiment is not convincing.  \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task.  This is different from the sum of the original losses, which seems to be the one used to train the \u201cequal weight\u201d baseline.  A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged. \n\nThe experiments on NYUv2 involves non-standard settings, without a good justification.  So it is not clear if the proposed method can make a real difference on state of the art systems.  \n\nAnd the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth) . However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting \u2014 it can in fact magnify gradients of certain tasks and cause over-training and over-fitting.  So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets. [[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEU]]