The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices.   They compare this\nbound with a similar recent bound proved by Bartlett, et al.   While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions. \n\nThe analysis is elegant.   It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent. \n\nVery nice paper. \n\nI guess that networks including convolutional layers are covered by\ntheir analysis.   It feels to me that these tend to be sparse,  but that\ntheir analysis still my provides some additional leverage for such\nlayers.   Some explicit discussion of convolutional layers may be\nhelpful.  [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]