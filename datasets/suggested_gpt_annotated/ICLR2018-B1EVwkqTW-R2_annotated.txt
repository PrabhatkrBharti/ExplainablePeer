Make SVM great again with Siamese kernel for few-shot learning  \n\n** PAPER SUMMARY **\n\nThe author proposes to combine siamase networks with an SVM for pair classification.  The proposed approach is evaluated on few shot learning tasks, on omniglot and timit.   \n\n\n** REVIEW SUMMARY **\n\nThe paper is readable  but it could be more fluent.  It lacks a few references and important technical aspects are not discussed.  It contains a few errors.  Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results.  Overall, the contribution is modest at best.v\n\n** DETAILED REVIEW **\n\nOn mistakes, it is wrong to say that an SVM is a parameterless classifier.  It is wrong to cite (Boser et al 92) for the soft-margin SVM.  I think slack variables come from (Cortes et al 95).  \"consistent\" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2.  You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel . \n\nOn incompleteness, it is not obvious how the classifier is used at test time.  Could you explain how classes are predicted given a test problem?  The setup of the experiments on TIMIT is extremely unclear.  What are the class you are interested in?  How many classes and examples does the testing problems have?  \n\nOn clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons.  You need to mention explicitely somewhere that (w,\\theta) are optimized jointly.  The sentence \"this paper investigates only the one versus rest approach\" is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class.  So you use a binary SVM, not one versus rest.  You need to find a better justification for using L2-SVM than \"L2-SVM loss variant is considered to be the best by the author of the paper\", did you try classical SVM and found them performing worse?  Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3? \n\nOn empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is.  I suspect it might be speaker identification.  So I will focus on the omniglot experiments.  \n\nFew-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]\n\nand the reference therein give a few more recent baselines than your table.  Some of the results are better   This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results.   Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-POS]]