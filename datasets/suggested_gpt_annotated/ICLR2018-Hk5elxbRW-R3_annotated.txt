This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks.  The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \"temperature parameter\".  The paper is well organized and clearly written.  The idea deserves a publication. \n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity.  When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-NEU],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]