SUMMARY\nThe paper deal with the problem of RL.   It proposes a non-parametric approach that maps trajectories to the optimal policy.   It avoids learning parameterized policies.   The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it. \n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different. \n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.  \n\nHow do you execute a trajectory?  Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly[[CLA-NEU],[JUS-NEG],[DEP-NEG],[FAI-NEU],[CON-NEG],[ENG-NEU],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEU]]