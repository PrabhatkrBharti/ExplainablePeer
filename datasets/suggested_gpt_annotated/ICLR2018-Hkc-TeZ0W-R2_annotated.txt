In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model\u2019s runtime.  However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step.  In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation.  Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder.  \n\nOverall speaking, this work is quite interesting.  However, it also has several limitations, as explained below.\ n\nFirst, the computational cost of the proposed method seems very high.  It may take more than one day on 320-640 GPUs for training (I did not find enough details in this paper, but the training complexity will be no less than the in [1]).  This makes it very hard to reproduce the experimental results (in order to verify it), and its practical value becomes quite restrictive (very few organizations can afford such a cost). \n\nSecond, as the author mentioned, it\u2019s hard to compare the experimental results in this paper wit those in [1] because different hardware devices and software versions were used.  However, this is not a very sound excuse.  I would encourage the authors to implement colocRL [1] on their own hardware and software systems, and make direct comparison.  Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is . In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements .\n\n [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. https://arxiv.org/pdf/1706.04972.pdf \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEU],[NOV-POS],[ETH-NEU]]