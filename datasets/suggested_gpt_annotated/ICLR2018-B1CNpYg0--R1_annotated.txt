This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.  The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings.  The method leads to better performance than using no external resources,  but not as high performance as using Glove embeddings.   The paper is clearly written, and has useful ablation experiments.   However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word.   As the authors note, this kind of character level modelling has been used in many previous works.   \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,   but I don\u2019t know the area well enough to make specific suggestions   \n- I\u2019m a little skeptical about how often this method would really be useful in practice.   It seems to assume that you don\u2019t have much unlabelled text (or you\u2019d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well.   All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.  \n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.   It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]