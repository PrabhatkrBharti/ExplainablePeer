This paper presents an image-to-image cross domain translation framework based on generative adversarial networks.  The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved.  The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision. \n\n\nNegatives:\n1.) The task of exact correspondence identification seems contrived.  It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing. \n2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method.   More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method. \u201d \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework. \n2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches. \n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases. \n2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]