Summary:\n\nThe paper proposes to learn new priors for latent codes z  for GAN training.   for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .  To fix this the paper proposes to learn a second GAN to learn the prior distributions of \"real latent code\" of the first GAN.  The first GAN then uses the second GAN as prior to generate the z codes.  \n \nQuality/clarity:\n\nThe paper is well written and easy to follow. \n\nOriginality:\n\npros:\n-The paper while simple sheds some light on important problem with the prior distribution used in GAN. \n- the second GAN solution trained on reverse codes from real data is interesting  \n- In general the topic is interesting, the solution presented is simple but needs more study \n\ncons:\n\n- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement.  \n- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder) \n\nGeneral Review:\n\nMore experimentation with the latent codes will be interesting: \n\n- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator?  Is this data low rank?  how does this change depending on the dimensionality of the latent codes?  Maybe adding plots to the paper can help. \n\n- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.   Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior? \n\n- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc?  Maybe also rotating the codes with the singular vector matrix V or \\Sigma^{0.5} V? \n\n- What architecture did you use for the prior generator GAN? \n\n- Have you thought of an end to end way to learn the prior generator GAN?  \n\n****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]