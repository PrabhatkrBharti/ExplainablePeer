[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner.  In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents.  To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations.  After generating both parts in parallel, they are combined using alpha blending to compose the final image.  This generated image is then sent to the discriminator.  The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA.  The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately. \n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part.  Corresponding to these two parts are two generators.  To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images.  As usual, the label information was added as the input of generator and discriminator as well. \n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2.  Based on this, outputs of Gc should not change much when flipping the input labels. \n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other.  Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2. \n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem.  However, to my opinion, completing it without any supervision is challenging and meaningless.  In this paper, the authors proposed a conditional way to generate images compositionally.  It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017). \n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model.  However, there are several weaknesses existed:\n\n1.  There is no quantitative evaluation and comparisons.  From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance.  The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples.  To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance.  Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation.  Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images.  This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier. \n\n2. The authors should try more complicated datasets, like CIFAR-10.  Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs.  It is easy to train since its low resolution, but also means a lot since it a relative complicated scene.  I would suggest the authors also run the experiments on CIFAR-10. \n\n3. The authors did not perform any ablation study.  Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models.  As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum.  I strongly suggest the authors add some ablation studies.  The authors should at least compare with one-layer conditional GAN.  \n\n4. The proposed model merely showed two-layer generation results.  There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6).  The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration. \n\n5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10.  I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017). \n\n6. I did not understand very well the label difference loss in (5).  Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image.  G2 takes all the duty  can still obtain a lower L_ld.  From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information. \n\n7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations. \n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN.  It generates images in a layer-wise manner.  To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties.  In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA.  However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study.  Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEU]]