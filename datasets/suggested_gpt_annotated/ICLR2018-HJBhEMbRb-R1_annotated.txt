Deep neural networks have found great success in various applications.  This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach.  Specifically, the authors develop a Fourier-based generalization bound.  Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions.  Numerical experimental results are also presented to verify the theory. \n\n(1) The scope is a bit limited.  The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers?  Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable.  What would happen if Y is random or the activation is ReLU? \n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W.  It would be intersting to present some analysis regarding the gradient w.r.t. W. \n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function.  However, no comparison is clearly made.  It would be better if the authors could explain this more? \n\nIn summary, the application domain of the theoretical results seems a bit restricted. \n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]