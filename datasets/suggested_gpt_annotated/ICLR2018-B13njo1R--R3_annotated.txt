Hi, \n\nThis was a nice read.  I think overall it is a good idea.  But I find the paper lacking a lot of details and to some extend confusing.  \nHere are a few comments that I have:\n\nFigure 2 is very confusing for me.  Please first of all make the figures much larger.  ICLR does not have a strict page limit, and the figures you have are hard to impossible to read.  So you train in (a) on the steps task until 350k steps?  Is (b), (d),(c) in a sequence or is testing moving from plain to different things?  The plot does not explicitly account for the distillation phase.  Or at least not in an intuitive way.  But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted.  \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines.  Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all.  So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough).  \n\nWhere exactly is input injection used?  Is it experiments from figure 3.  What input is injecting?  What do you do when you go back to the task that doesn't have the input, feed 0?  What happens if 0 has semantics ?  \n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix.  And do try to copy a bit more of them in the main text where reasonable.  \n\nWhat is the role of PLAID?  Is it to learn a continual learning solution?  So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills?  Will this be feasible?  Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning?  \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?   And even then maximize transfer with respect to what?  Frames required from the environment?  If that are you reusing the frames you used during training to distill?  Can we afford to keep all of those frames around?  If not we have to count the distillation frames as well.  Also more baselines are needed.  A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way.   Or at least have a good argument of why this is suboptimal compared to PLAID.  \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score.  But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read.  Providing at least one more baseline (if not more considering the other things cited by them). \n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]