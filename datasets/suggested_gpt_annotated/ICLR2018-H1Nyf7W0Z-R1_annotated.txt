The paper proposes another training objective for training neural sequence-to-sequence models.  The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p.  The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively. \n\nThe paper has significant writing issues.  In Paragraph \u201cMaximum Likelihood\u201d, page 2, the formalization of the studied problem is unclear.  Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only?   In the former case, the statement \u201cx is uniformly sampled from X\u201d does not make sense because X is practically infinite.  Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation.  If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set.  Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used. \n\nThe proposed method is evaluated on just one dataset.  Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al.  The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. \n\nTo sum up, I can not recommend the paper to acceptance,  because (a) an important baseline is missing  (b) there are serious writing issues[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEU],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEU]]