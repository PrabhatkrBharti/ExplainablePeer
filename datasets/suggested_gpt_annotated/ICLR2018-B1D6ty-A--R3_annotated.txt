In this paper an alternating optimization approach is explored for training Auto Encoders (AEs). \nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase. \nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD.  The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs. \n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading.  While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer. \nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply. \nThe authors should mention this point. \n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.  \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.  Concretely: Is there any known theory for such objectives?  What guarantees can we hope to achieve? \n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice .\n\nThe experimental part is satisfactory, and seems to be done in a decent manner.  \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm. \nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]