The authors present a testing framework for deep RL methods in which difficulty can be controlled along a number of dimensions, including: reward delay, reward sparsity, episode length with terminating rewards, binary vs real rewards and perceptual complexity.  The authors then experiment with a variety of TD and MC based deep learners to explore which methods are most robust to increases in difficulty along these dimensions.  The key finding is that MC appears to be more robust than TD in a number of ways, and in particular the authors link this to domains with greater perceptual challenges.  \n\nThis is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition.  The authors discover some interesting characteristics of MC based Deep-RL which may influence future work in this area, and dig down a little to uncover the principles a little.  The testing framework will be made public too, which adds to the value of this paper.  I recommend the paper for acceptance and expect it will garner interest from the community. \n\nDetailed comments\n  \u2022 [p4, basic health gathering task] \"The goal is to survive and maintain as much health\nas possible by collecting health kits... The reward is +1 when the agent collects a health kit and 0 otherwise. \" The reward suggests that the goal is to collect as many health kits as possible, for which surviving and maintaining health are secondary. \n  \u2022 [p4, Delayed rewards] It might be interesting to have a delay sampled from a distribution with some known mean.  Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not. \n  \u2022 [p4, Sparse rewards] I am not sure it is fair to say that the general difficulty is kept fixed.  Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed. \n  \u2022 [p6] \"Dosovitskiy & Koltun (2017) have not tested DFP on Atari games. \" Probably fairer/safer to say: did not report results on Atari games.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]