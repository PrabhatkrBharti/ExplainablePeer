*Summary*\n\nThe paper proposes using batch normalisation at test time to get the predictive uncertainty.  The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer.  This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time.  Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout. \n\n*Originality and significance*\n\nAs far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1.  However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction.  \n\nIt is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w). \n\nFrom a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small \\lambda] -- what is the rationale of this?  Can this be explained by comparing the variational free-energy.  \n\nThe experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017).  The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method. \n\n*Clarity*\nThe paper is in general well written and easy to understand.  \n\n*Additional comments*\n\nPage 2: Monte Carlo Droput --> Dropout\nPage 3 related work: (Adams, 2015) should be (Hernandez-Lobato and Adams, 2015)[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]