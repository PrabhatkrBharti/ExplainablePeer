Pros: \nThe paper proposes a \u201cbi-directional block self-attention network (Bi-BloSAN)\u201d for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient.  The paper is written clearly and is easy to follow.  The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency.  The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off.  \n\nCons:\n- Methodology of the paper is very incremental compared with previous models.  \n- Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper.  \n- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN. \n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart.  This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks. \n- The main originality of paper is the block style. However, the paper doesn\u2019t analyze how and why the block brings improvement.  \n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEG],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-POS]]