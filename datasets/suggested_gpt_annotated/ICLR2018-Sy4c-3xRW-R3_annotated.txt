The paper discusses dropping out the pre-softmax logits in an adaptive manner.  This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout.  In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference.  The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour.  A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1.  I would have liked to have seen results on ImageNet.  I don't find (the too small) Figure 2 to be compelling evidence that \"our dropmax effectively prevents\noverfiting by converging to much lower test loss\".  The test loss in question looks like a noisy version of the base test loss with a slightly lower mean.  There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage.  Figure 3 illustrates the idea nicely.  Which of the MNIST models from Table 1 was used?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]