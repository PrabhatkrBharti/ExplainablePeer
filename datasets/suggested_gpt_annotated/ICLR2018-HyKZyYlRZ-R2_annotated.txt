The paper describes a neural end-to-end architecture to solve multiple tasks at once.   The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks.   The engineering endeavor is impressive,  but the paper has little scientific value.   Below are a few suggestions to make the paper stronger. \n\nIt is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input.   One way to analyze whether this happens is to predict the identity of the task from the hidden vectors.   If this is the case, how to prevent it from happening?   If this does not happen, what is being shared across tasks?   This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space. \n\nWhy multitask learning help the model perform better is still unclear.   If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples.   The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning? \n\nIf the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal.   If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task? \n\nReporting per-token accuracies, such as those in Table 2, is problematic.   It's unclear how to compute per-token accuracies for structured prediction tasks, such as speech recognition, parsing, and translation.   Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task.   The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-NEU],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]