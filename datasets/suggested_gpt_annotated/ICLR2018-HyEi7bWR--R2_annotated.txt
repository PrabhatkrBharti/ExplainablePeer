This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal.  They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks. \n\nI think the paper is well-written.   Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method. \n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments? \n\n2- How sensitive is the results to the number of -1 in the diagonal matrix? \n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs.  In MNIST experiment, for example, better numbers are reported for larger LSTMs.  I think matching the number of hidden units could be helpful.  Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000.  I appreciate if authors can provide more results in these settings.\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]