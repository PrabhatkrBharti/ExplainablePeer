The paper proposes a modified approach to RL, where an additional \"episodic memory\" is kept by the agent.  What this means is that the agent has a reservoir of n \"states\" in which states encountered in the past can be stored.  There are then of course two main questions to address (i) which states should be stored and how  (ii) how to make use of the episodic memory when deciding what action to take.  \n\nFor the latter question, the authors propose using a \"query network\" that based on the current state, pulls out one state from the memory according to certain probability distribution.  This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory.  Intuitively, one can see why this may be advantageous as one gets some information from the past.  (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.) \n\nThe first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights.  The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose.  However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past) . There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.  \n\nThere is also a toy example created to show that this approach works well compared to the RNN based approaches. \n\nPositives:\n\n- An interesting new idea that has potential to be useful in RL \n- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks) \n\nNegatives:\n- The math is fudged around quite a bit with approximations that are not always justified \n- While overall the writing is clear, in some places I feel it could be improved . I had a very hard time understanding the set-up of the problem in Figure 2.  [In general, I also recommend against using figure captions to describe the setup. ]\n- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]