The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples.  In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer.  In practice the class means are \"learned\", yet regularised towards the batch class means. \n\nMy main concern with the paper is in the theoretical underpinning of the work.  From the title a Bayesian approach is suggested, while in practice a rather standard softmax classifier is learned, albeit with a different regulariser (last layer is regularised towards batch class means).  Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a \"Gaussian\", and thus the class probabilities are the reseponsibilities proportional to the class Gaussian.  To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix.  Taking away a large part of the Gaussian distribution.  The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier).  \n\nA second concern is the experimental exploration.  First of all, it is unclear if the method works much better for the tail than the standard softmax.  That is not apparent from the results.  For example, Fig 4 shows -except for CIFAR10- not a clear relation between class index and proposed relative improvement, it is also unclear if there is just a difficult class (eg at index 150), or that the experiment has been repeated several times.  Moreover, when the performance becomes more stable for the classes in the tail, I'd have expected that the standard deviation of the mean class accuracy would decrease, from the results there is no difference between Softmax and the proposed method: 44 +/-1 for Softmax (miniImageNet) to 41 +/-1 for the proposed NCM approach.  In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better.  However, I'm a little unsure about these conclusions, what is the unregularized version exactly doing, how is it different from a standard softmax? \n\nRemaining (minor) remarks:\n- It is unclear how iCaRL has been used - it has been proposed as an iterative classification method. \n- Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors? \n- Figure 4: Gain -> Relative performance\n- The batch size must have a great influence on the functioning of the regularisation (especially when there are many classes, in that case just a single example counts for the class mean). This is not explored in the paper.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]