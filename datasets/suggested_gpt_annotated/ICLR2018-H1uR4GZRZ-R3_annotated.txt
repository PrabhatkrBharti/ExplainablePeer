The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations.  Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking. \n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks.  On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness.  For example, how does this compare to random perturbation (say, zero-mean) of the weights?  This adds stochasticity as well so why and why not this work?  The authors do not give any insight in this regard. \n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner.  The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else)[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]