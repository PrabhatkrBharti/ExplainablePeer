This paper analyzes the loss function and properties of CNNs with one \"wide\" layer, i.e., a layer with number of neurons greater than the train sample size.  Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums.  I like the presentation and writing of this paper.  However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.  The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.  This is not surprising.  It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. [[CLA-NEU],[JUS-NEU],[DEP-NEU],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]