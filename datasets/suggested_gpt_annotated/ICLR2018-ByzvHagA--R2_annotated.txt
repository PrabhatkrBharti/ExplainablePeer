\nI think the first intuition is interesting.  However I think the benefits are not clear enough.  Maybe finding better examples where the benefits of the proposed regularization are stressed could help.  \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal.  I do not see a clear novelty in the proposal.   \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA.  As shown in [Szegedy et al 2014, \"Intriguing properties of neural networks\"] adding an extra linear transformation does not change the expressive power of the representation.     \n\n\n- \"Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate... \"\n\nThe first paragraph is confusing since jumps from total correlation to correlation without making clear the differences. \nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that. \n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance.  I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.    \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting. \n\n- The first experiment (dim red) is not clear to me.  The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10.  Also the data problem is extremely simple, and it is not clear the didactic benefit of using it.  I think a much more complicated data would be more interesting.  Besides L_1 is not well defined.  If it is L_1 norm on the output coefficients the comparison is misleading.  \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits .\n\n- Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal.  The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples.  However the MSE obtained when not using regularization is the same (or even smaller) than when using it[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]