This paper presents an iterative approach to sparsify a network already during training.  During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold.  This is a big advantage when training is performed on hardware with computational limitations, in comparison to \"post-hoc\" sparsification methods, that compress the network after training. \nThe method is derived by considering the \"rewiring\" of an (artificial) neural network as a stochastic process.  This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach.  References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\n Especially the stochastic gradient method in [2] is strongly related to the existing approach. \n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.  The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-  By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n - The method is specifically designed for online learning with limited hardware ressources.\n\n Negative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C).  Especially the results on MNIST suggest that this method is most advantageous for very high compression levels.  However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n - A detailled discussion of the relation to previously existing very similar work is missing (see above) \n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure.  Readibility could be slightly increased by putting the figures on the respective pages.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]