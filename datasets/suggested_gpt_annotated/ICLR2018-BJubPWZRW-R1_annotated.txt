This paper presents a so-called cross-view training for semi-supervised deep models.  Experiments were conducted on various data sets and experimental results were reported.\n\nPros:\n* Studying semi-supervised learning techniques for deep models is of practical significance. \n\nCons:\n* The novelty of this paper is marginal.  The use of unlabeled data is in fact a self-training process.  Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval.  \n* The proposed approach suffers from a technical weakness or flaw.  For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling.  However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,\u201chorse\u201d) may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse).  Such a flaw may affect the final performance of the proposed approach. \n* The word \u201cview\u201d in this paper is misleading.  The \u201cview\u201d in this paper is corresponding to actually sub-regions in the images \n* The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2, which reduces the practical significance of the proposed approach. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEG],[CON-NEG],[ENG-POS],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEU]]