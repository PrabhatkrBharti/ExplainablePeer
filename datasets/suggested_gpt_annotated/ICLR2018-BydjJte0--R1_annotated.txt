The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach).  It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy.  As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable,  yet Table 2 shows that batch size can be predicted with much higher accuracy than chance . It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately . That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization .\n\nThe selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries?  One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy. \n\nIn table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io. \n\nIn the ImageNet classifier family prediction, how different are the various families from each other?  Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader. \n\nOverall the results seem interesting,  but without more insights it's difficult to judge how generally useful they are[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]