This paper proposes a multi-view semi-supervised method.  For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap.  Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data. \n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here).  As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term.  Otherwise, the model can't learn anything. \n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss).  In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?  If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1.  One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]