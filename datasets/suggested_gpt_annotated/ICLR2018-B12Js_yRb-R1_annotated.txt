\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. \n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals. \n\nClarity:\n- The paper is well written and clarity is good.  Figure 2 & 3 helps the readers understand the core algorithm. \n\nPros:\n- De-duplication modules of inter and intra object edges are interesting. \n- The proposed method improves the baseline by 5% on counting questions. \n\nCons:\n- The proposed model is pretty hand-crafted.  I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016). \n- One major bottleneck of the model is that the proposals are not jointly finetuned.  So if the proposals are missing a single object, this cannot really be counted.  In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals.  The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is. \n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain. \n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017). \n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).  This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance. \n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions. \\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL \\n\nConclusion:\n- I feel that the motivation is good,  but the proposed model is too hand-crafted.  Also, key experiments are missing: 1) NMS baseline  2) Comparison with VQA counting work  (Chattopadhyay et al., 2017).  Therefore I recommend reject. \n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017. \n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016. \n\nUpdate:\nThank you for the rebuttal.  The paper is revised and I saw NMS baseline is added.  I understood the reason not to compare with certain related work.  The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy.  However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]