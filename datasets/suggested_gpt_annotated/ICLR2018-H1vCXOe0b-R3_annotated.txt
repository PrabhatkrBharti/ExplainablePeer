The paper develops a technique to understand what nodes in a neural network are important\nfor prediction.  The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples.  \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem.  One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected?  \n\nOverall, the results didn't warrant the complexity of the method.  The results are neat, but \nI couldn't tell why this approach was better than others. \n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEU]]