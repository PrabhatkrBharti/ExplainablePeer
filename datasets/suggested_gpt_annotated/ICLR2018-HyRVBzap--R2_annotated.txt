This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space.  This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense.  Note that a similar strategy has been used in the recent past under the name of stability training.  The proposed method works well on CIFAR and MNIST datasets.  My main concerns are:\n\n\t- The adversarial objective and the stability objective are potentially conflicting.  Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label).  Have the authors considered this issue?  Can they elaborate more on how they with this? \n\n\t- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data.  Did the authors try such experiment?  It would be interesting to see these results.  \n\nLastly, The insights regarding label leaking are not compelling.   Label leaking is not a mysterious phenomenon.  An adversarially trained model learns on two different distributions.  Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model.  Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models.  I lean on accept side.  \n\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEU]]