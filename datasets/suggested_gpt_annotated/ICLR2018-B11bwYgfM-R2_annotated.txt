This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j). \n\nThe matrix completion approach is based on robust PCA.  When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds).  This can be computationally demanding. \n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering.  However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable. \n\nThere have been a number of MTL methods based on task clustering.  For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI) \n[2] A dirty model for multi-task learning (NIPS) \n[3] Clustered multi-task learning: A convex formulation (NIPS) \n[4] Convex multitask learning with flexible task clusters (ICML) \n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model.  This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model).  However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not. \n\nFor few-shot learning, the authors mentioned that the \\alpha's are adaptable parameters but did not mention how they are adapted. \n\nExperimental results are not convincing. \n- Comparison with existing clustered MTL methods mentioned above are missing. \n- As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported. \n- As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEU]]