This paper proposes an interesting  approach to prune a deep model from a computational point of view.  The idea is quite simple as pruning using the connection in the batch norm layer . It is interesting to add the memory cost per channel into the optimization process.  \n\nThe paper suggests normal pruning does not necessarily preserve the network function.  I wonder if this is also applicable to the proposed method and how can this be evidenced.  \n\nAs strong points, the paper is easy to follow and does a good review of existing methods.  Then, the proposal is simple and easy to reproduce and leads to interesting results . It is clearly written (there are some typos / grammar errors).  \n\nAs weak points:\n1) The paper claims the selection of \\alpha is critical but then, this is fixed empirically without proper sensitivity analysis.  I would like to see proper discussion here.  Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere.  \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A). \n\nIn section 4, in the algorithmic steps . How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network? \n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]