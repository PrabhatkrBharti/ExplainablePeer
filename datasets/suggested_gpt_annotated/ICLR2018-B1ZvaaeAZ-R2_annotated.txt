This is a well-written paper with good comparisons to a number of earlier approaches.  It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs.  Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper. \n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction. \n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design.  It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today.  \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit) \n- Moving the state of the art in low precision forward \n- Strong potential impact, especially on constrained power environments (but not limited to them) \n- Uses same hyperparameters as original training, making the process of using this much simpler. \n\nCons/Questions\n- They mention not quantizing the first and last layer of every network.  How much does that impact the overall compute?  \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model?  This could be interesting for low power case, even if the \"effective compute\" is larger than the baseline.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]