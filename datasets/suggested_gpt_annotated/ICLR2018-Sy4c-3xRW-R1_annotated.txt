This paper propose an adaptive dropout strategy for class logits.  They learn a distribution q(z | x, y) that randomly throw class logits.   By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks.  They learn the dropout distribution by variational inference with concrete relaxation.  \n\nOverall I think this is a good paper.  The technique sounds, the presentation is clear and I have not seen similar paper elsewhere  (not 100% sure about the originality of the work though).  \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak.  Only on CIFAR100 the proposed approach is much better than other approaches.  I would like to see the results on more datasets.  Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-NEU],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]