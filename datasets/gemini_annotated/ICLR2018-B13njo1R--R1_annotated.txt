This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially.  The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task.  The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.  \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network? \n- What data do you use for the distillation?  Section 4.1 states\"We use a method similar to the DAGGER algorithm\", but what is your method.  If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?  \n- I do not understand the purpose of \"input injection\" nor where it is used in the paper.   \n\nStrengths:\n- The method is simple but novel.   The results support the method's utility.  \n- The testbed is nice; the tasks seem significantly different from each other.  It seems that no reward shaping is used. \n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker. \n\nWeaknesses:\n- Figure 2: the plots are too small. \n- Distilling may hurt performance ( Figure 2.d) \n- The method lacks details (see Questions above) \n- No comparisons with prior work are provided.  The paper cites many previous approaches to this but does not compare against any of them.  \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.  \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work.  The method is clear  but not precisely described.  The results are promising.  I think that this is a good approach to the problem that could be used in real-world scenarios.  With some filling out, this could be a great paper.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]