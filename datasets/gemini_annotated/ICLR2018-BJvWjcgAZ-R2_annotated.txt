The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update.  The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards.   This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.).   Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max.  \n\nThe paper is written in a clear way.    The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there.   I think this should have been one of the baselines to compare to for that reason.  \n\nThe experimental results seem promising in the illustrative MNIST domain.   Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]