The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.  The paper is rigorous and ideas are clearly stated.  The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.  My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.  I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n\n1. The framework uses the class information, i.e., \u201conly data samples from the normal class are used for training\u201d, but it is still considered unsupervised.  Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.  I would like to see a plot of the sample energy as a function of the number of data points.  Is there an elbow that indicates the threshold cut?  Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data). \n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?  \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results? \n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model).  Those approaches should at least be discussed in the related work, if not compared against. \n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.  Could you provide a comparison with EM? \n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant?  Does it well describe the new space?  Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?  Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians. \n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies  ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets. \n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate  ...\u201d - it is not clear to me, it does look better than the other ones, but not clear.   If there is a clear separation from a different view, show that one instead.   We don't need the same view for all methods.   \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic! \n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.  \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean? \n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE.  Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.  They are the best in terms of precision. \n4. Is the error in Table 2 averaged over multiple runs? If yes, how many? \n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented.  It can be further improved as mentioned in the comments. \n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read.  \n\nOriginality \u2013 Fairly original, but it still needs some work to justify it better.  \n\nSignificance \u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]