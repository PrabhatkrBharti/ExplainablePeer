This paper presents two methods for imposing a margin on discriminative loss functions, one which uses the margin between the reference transcription and alternatively hypothesized transcriptions (LMLM), and another which compares all alternative candidates and uses a margin between those with a better system objective (WER or bleu) and those with a worse system objective (rank-LMLM).   Some interesting results on the development set show the importance of things like warm starting on large language model training data.   The methods presented here could be of interest to those training language models for use in specific systems, and the paper reads reasonably clearly .\n\nThe principal shortcoming of the paper is that there was essentially no effort to establish that the baseline systems that are being improved through reranking via these methods are decent baselines for such a use, or to really specify these systems in a way that would allow for replication of the results being presented in the paper.   Sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselines,  yet such information is sorely lacking in this paper.   Further, the speech data sets, Fisher and Wall St. Journal, have what would seem to be very high word error rates versus what should be possible with standard open-source speech recognizers such as Kaldi.    For example, by referencing a page that attempts to establish the state of the art on standard data sets (https://github.com/syhw/wer_are_we), we can find links to papers by Povey et al (http://www.danielpovey.com/files/2016_interspeech_mmi.pdf) and the Deep 2 paper in your citations, which themselves include baselines from other papers that cut the error rate in half versus even your best scoring systems, let alone your baselines.   Similarly, your Bleu score on Vietnamese to English translation is way below what were reported (even by the organizer baseline) for the IWSLP conference where the data became available: https://github.com/magizbox/underthesea/wiki/SOTA-Machine-Translation:-IWSLT-2015\n\nGranted, the competing systems also were outperformed by the organizer baseline for that task at IWSLT 2015, but not by the degree to which your system is.    Again, your best performing system (using your new methods) has performance far below the worst reported competing system.    The cavalier presentation of specific details regarding your baseline systems (which is critical for any sort of replicability) and the uniformly weak performance of these systems relative to widely reported results, leads me to discount the probability that your methods would actually result in improvements on truly solid baselines.   I would have preferred one domain experiment carried out with appropriately rock solid documentation of the ball-park competitive baseline system to these results. \n\nOverall, the method is interesting and the dev set experiments were informative,  but ultimately the experiments were not. \n\nRevision:  Having read the author response for this paper, I am encouraged by the updated baseline for WSJ and the additional explication about the competing Fisher systems.   The additional information about the systems included in section 4.4 is pretty nominal, though, and I worry about the ability of others to replicate these results.   I would have felt better about the results if there were reported results from other papers included here, instead of the authors' attempt to create a baseline from the given data, which may or may not (as we have seen) represent a strong enough baseline from which to draw conclusions.   That is to say that I still have some reservations (though less than I had before).   I am including this additional information in my review, for use by the area chair, but otherwise leaving my assessment as-is.\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]