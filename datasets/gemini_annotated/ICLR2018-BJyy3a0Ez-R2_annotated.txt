The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system.  Because the layer-wise decoupling, it can easily be applied for distributed training of the model.  The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results. \n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,  it is a shame to see the overall quality of the paper very weak.  The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP).  The experiments are also very weak.  Important information on the experiment settings are missing, e.g., how the model is parallelized. \n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD.  It needs to be better defined.[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]