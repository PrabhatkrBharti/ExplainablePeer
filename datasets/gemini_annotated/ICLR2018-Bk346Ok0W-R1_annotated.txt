This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism.  \n\nPros:\nOne of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions.  This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup.  \n\nCons:\nThe idea seems to be simple and does not have significant originality.  Also, the paper does not clearly mention the attention mechanism part, and needs some improvement.  \n\nComments:\n-\tThe paper mainly focuses on the soft sensor selection.  However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important. \n-\tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, \"A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming,\" IEEE Journal of Selected Topics in Signal Processing.  This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection.  It\u2019s better to discuss about this paper as well as a reference. \n-\tSection 2: better to explain about how to obtain attention scores z in more details. \n-\tFigure 3, experiments of Double audio/video clean conditions: I cannot understand why they are improved from single audio/video clean conditions.Need some explanations.  \n-\tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features. \n-\tSection 3.2 Dataset \u201cAs for TIDIGIT\u201d: \u201cAs for GRID\u201d(?) \n-\tSection 4 Models \u201cThe parameters of the attention modules are either shared across sensors (STAN-shared) or not shared across sensors (STAN- default). \u201d: It\u2019s better to explain this part in more details, possibly with some equations. It is hard to understand the difference.\n\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]