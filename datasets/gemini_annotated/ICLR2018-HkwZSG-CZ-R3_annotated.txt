Language models are important components to many NLP tasks.  The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.  This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.  The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization. \n\nPros:\n--The paper is very well written and easy to follow.  The ideas build up on each other in an intuitive way. \n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know. \n--The maths is very rigorous. \n--The experiment section is thorough. \n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).  In Table 2 the baselines have a lower capacity.  This is an unfair comparison \n--I suspect the proposed approach is slower than the baselines.  There is no mention of computational cost.  Reporting that would help interpret the numbers.  \n\nThe SOTA claim might not hold if baselines are given the same capacity.  But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]