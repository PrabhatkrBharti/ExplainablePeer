\nIn this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together.  The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem.  They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity.  Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity.  Empirically, they show the performances are better than random feature and the LKRF.  \n\nI like the way they handle the nonconvexity component of the model.  However, there are several issues need to be addressed.  \n\n1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly.  \n\n2, In the paper, there is an assumption about the peak of random feature \"it is a natural assumption on realistic data that the largest peaks are close to the origin\".  I was wondering where this assumption is used?  Could you please provide more justification for such assumption? \n\n3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning.  Please clarify this in the paper explicitly.  \n\n4, The experiment is weak.  The algorithm is closely related to boosting and MKL, while there is no such comparison.  Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison.  \n\nSuggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]