This paper presents a neural architecture for converting natural language queries to SQL statements.  The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary.  The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization).  Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model.  Overall, even though the architecture is not very novel,;  the paper is well-written and the results are strong;  as such, I'd recommend the paper for acceptance. \n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?  Could the output grammar be extended to support joins, for instance?  As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful.  \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?  In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]