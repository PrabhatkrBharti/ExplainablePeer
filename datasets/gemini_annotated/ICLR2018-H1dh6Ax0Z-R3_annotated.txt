\n This was an interesting read.   \n\nI feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does.  Just because the transition function is shared and the model could learn to construct a tree, when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour.  More to a point. I think the search tree perspective is interesting,  but isn\u2019t this just a deeper model with shared weights? And a max operation?  It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right?  Is there any specific attempt to visualize or understand the embeddings inside the tree?  The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards?  I think this is a pitfall that many deep network papers fall, where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution (where the latent have prescribed semantics). I would argue that is rarely the case.  When the system is learned end-to-end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree. And this is not by showing final performance on a game.  If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should.  \nIgnoring my verbose comment, another view is that the baseline are disadvantaged to the treeQN, because they have less parameters (and are less deep which has a huge impact on the learnability and expressivity of the deep network)[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]