The authors propose a new sampling based approach for inference in latent variable models.  They apply this approach to multi-modal (several \"intentions\") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks.  \n\nThe proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled.  The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior.  The objective to be maximized is a lower bound to 1/alpha * the likelihood.  \n\nQuality: The empirical results (including a video of an actual robotic arm system performing the task) looks good.  This reviewer is a bit sceptical to the methodology.  I am not convinced that the proposed bound will have low enough variance.  It is mentioned in a footnote that variational autoencoders were tested but that they failed.  Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework.  Also, the recently proposed continuous relaxation of random variables seemed relevant.  \n\nClarity: The paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribution. \n\nSignificance: Hard to say. New method proposed and shown to work well in one case.  Too early to tell about significance. \n\nPro:\n1. Challenging and relevant problem solved better than other approaches. \n2. New latent variable model bound that might work better than classic approaches. \nCon:\n1. Not entirely convincing that it should work better than already existing methods. \n2. Missing some investigation of the properties of the estimator on simple problem to be compared to standard methods[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]