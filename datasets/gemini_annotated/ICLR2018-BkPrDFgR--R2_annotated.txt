Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks. \n- introduces a simple branch-and-bound approach. \n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one. \n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference. \n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read. \n\nSoundness: As far as I can tell, the work is sound. \n\nNovelty: This is in my opinion the weakest point of the paper.  There isn't really much novelty in the work.  The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is).  The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well). \n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods.  Unfortunately, there isn't much insight to be gained from them.  I couldn't see any emerging trend/useful recommendations (like \"if your problem looks like X, then use algorithm B\").  This is unfortunately often the case when dealing with combinatorial search/optimization[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]