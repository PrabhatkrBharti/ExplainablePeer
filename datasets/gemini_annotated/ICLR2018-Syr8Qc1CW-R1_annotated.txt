This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes.  \n\nPros: \n+ The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel.  \n+ A theoretical guarantee of the efficiency of an aspect of the proposed method is given. \n\nCons: \n- The results are not very appealing visually.  The results from the proposed method do not seem much better than the baselines. What is the objective for the images in Fig. 4?  For example I'm looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top-left but adding the attributes of choice. \n- Quantitative results are missing.  \n- Some unclarity in the description of the method; see below. \n\nQuestions/other:\n- What is meant by \"implicit\" models?  By \"do not anchor a specific meaning into the disentanglement\"?  By \"circumscribed in two image domains\"?   \n- Why does the method require two images?  \n- In the case of images, what is a dominant vs recessive pattern?  \n- It seems artificial to enforce that \"the attribute-irrelevant part [should] encode some information of images\".  \n- Why are (1, 0) and (1, 1) not useful pairs? \n- Need to be more specific: \"use some channels to encode the id information\". \n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]