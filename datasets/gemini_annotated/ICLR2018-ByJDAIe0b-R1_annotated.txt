This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling.  The authors propose a reservoir sampling algorithm for drawing samples from the memory.  Some theoretical guarantees for the efficiency of reservoir sampling are provided.  The whole algorithm is tested on a toy problem with 3 repeats.  The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm. \n\nThe paper is well written and easy to understand.  Typos didn't influence reading.  It is a novel setup to consider reservoir sampling for episodic memory.  The theory part focuses on effectiveness of drawing samples from the reservoir.  Physical meanings of Theorem 1 are not well represented.  What are the theoretical advantages of using reservoir sampling?  \n\nFour simple, shallow neural nets are built as query, write, value, and policy networks.  The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network.  It is not clear the better performance comes from reservoir sampling or other differences.  Moreover, the hyperparameters are not optimized on different architectures. It is hard to justify the empirically better performance without hyperparameter tuning.  The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]