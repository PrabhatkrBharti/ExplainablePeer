# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples. \n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem.  However, this contribution is far from correctly addressing the problem.  The Algorithm is poorly described and crucial parts of the algorithm are very confusing.  Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors.  \n\n# Detailed comments\n* Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable.  To make things worse, some variables are simply not defined.  For example, I can't find the definition of D.  From the context it seems to be the number of layers in the network (I shouldn't need to guess).  \n\n* From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables?  This is nonstandard and confusing. \n\n* Again, from Algorithm 1, it is not clear which parts can be performed asynchronously.  It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their \"continuous-propagation\" factorization?   \n\n* Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process.   \n\n* The proof, despite being a trivial application of existing work, has obvious flaws.   After equation (17) it is stated that \"the left-hand side is independent of x_{k, m, l}\" which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term).   \n\n* The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L = L_d?) and which is impossible to verify in practice (T is not known).   This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent.   It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities. \n\n* As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4). \n\nIn appendix:\n\n * Assumption 3, 4: Why is upper superindex d?  In any case, be consistent, most of the time these are used but then its stated \"for all Theta\" (whithout superindex)\n * Proposition: what is L? is L = L_d? \n\n\nOther\n\n  * Assumption 5: decay -> delay?\n\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]