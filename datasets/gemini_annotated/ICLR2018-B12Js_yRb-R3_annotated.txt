This paper tackles the object counting problem in visual question answering.  It is based on the two-stage method that object proposals are generated from the first stage with attention.  It proposes many heuristics to use the object feature and attention weights to find the correct count.   In general, it treats all object proposals as nodes on the graph.  With various agreement measures, it removes or merges edges and count the final nodes.  The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset.  The experimental results on counting are promising.   Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems.   Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications.  I thus believe the overall contribution is not sufficient for ICLR. \n\nPros:\n1. Well written paper with clear presentation of the method.  \n2. Useful for object counting problem. \n3. Experimental performance is convincing.  \n\nCons:\n1. The application range of the method is very limited.  \n2. The technique is built on a lot of heuristics without theoretical consideration.  \n\nOther comments and questions:\n\n1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel.   It may also lead to simpler solutions.  For example, it can be used for deduplication using A (eq 1) as the similarity matrix.  \n\n2. Can the author provide analysis on scalability the proposed method?  When the number of objects is very large, the graph could be huge.  What are the memory requirements and computational complexity of the proposed method?   \nIn the end of section 3, it mentioned that \"without normalization,\" the method will not scale to an arbitrary number of objects.  I think that it will only be a problem for extremely large numbers.  I wonder whether the proposed method scales.  \n\n3. Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result?  Theoritically, it solves the soft attention problems.  \n\n4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.  \n\n[1] Kulesza, Alex, and Ben Taskar. \"Determinantal point processes for machine learning.\" Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012): 123-286.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]