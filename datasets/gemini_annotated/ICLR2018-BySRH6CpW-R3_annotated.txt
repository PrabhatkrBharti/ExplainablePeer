This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE.  Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel.   From the experiments, we can see that the proposed method is effective.  \n\nIt seems that there could be more things to show in the experiments part.  For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs.  Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram.  \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand.  Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order.  Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]