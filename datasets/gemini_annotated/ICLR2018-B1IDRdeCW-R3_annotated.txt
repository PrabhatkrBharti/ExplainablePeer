This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector.  It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \"Dot Product Proportionality Property. \" It also proposes \"Generalized Binarization Transformation\" for the first layer of a neural network. \n\nIn general, I think the paper is written clearly and in detail.  Some typos and minor issues are listed in the \"Cons\" part below. \n\nPros:\nThe authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer.  Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. \n\nCons:\n* it seems that there are quite some typos in the paper, for example:\n    1. Section 1, in the second contribution, there are two \"then\"s. \n    2. Section 1, the citation format of \"Bengio et al. (2013)\" should be \"(Bengio et al. 2013)\". \n* Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD.  \n* Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear. \n* In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer?  I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]