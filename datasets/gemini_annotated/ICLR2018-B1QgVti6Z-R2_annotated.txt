This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs).  Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.  The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.  2) Uniform convergence of the empirical risk to population risk.  3) Generalization bound based on stability.  The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations. \n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points.  Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW.  Should the risk bound only depends on the dimensions of the matrix W? \n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations.  Thus, the authors need to refine BM's result for comparison.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]