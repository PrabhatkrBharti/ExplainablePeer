This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks.  I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.  Any smooth function can be approximated by such networks. \n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN. \n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces \n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm. \n\nAmong these results (1), (2), (4) are sort of known in the literature.  This paper extends the existing results in some subtle ways.  For (1), the authors show that the DNN has a tighter bound on the depth. For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum. \n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU.  Other than that, I don't get much more insight from the theoretical result.  When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient.  Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough. \n\nResult (3) is more interesting as it is a new result.  The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.   However, the construction seems artificial and these functions don't seem to be visually very complex. \n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks.  The results might be of theoretical interest[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]