Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH.  I found the paper relatively creative and generally well-founded and well-argued. \n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product.  \n\nExperiments: appreciated the wall clock timings. \n\nSGD comparison: \u201cfixed learning rate. \u201d Didn't see how the initial (well constant here) step size was tuned?  Why not use the more standard 1/t decay? \n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?  Legend backwards? \n\nWhy were so many of the chosen datasets have so few training examples? \n\nPaper is mostly very clearly written,  though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]