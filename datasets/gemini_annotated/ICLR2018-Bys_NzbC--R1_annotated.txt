The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks.  Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16.  The results seem to show that a delayed application of the regularization parameter leads to improved classification performance. \n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning.  In the latter case, a stronger parameter is applied, followed by reduced regularization parameter.  One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization.  It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on. \n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]