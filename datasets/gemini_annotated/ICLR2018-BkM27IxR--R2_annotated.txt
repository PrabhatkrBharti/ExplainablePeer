This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks.  This work is an extended version of [1], aiming to address the high-dimensional problem. \n\n\n\nStrengths:\n\nThe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms. \nThe proposed method has better robustess in different tasks and different batch size setting. \nThe invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG. \n\n\nWeaknesses:\n\n1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch.  More iterations should be taken and the log-scale style figure is suggested.  \n\n2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments. \n\n3. \u201cThe average recent iterate\u201c described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose \u201c3\u201d, and the effectiveness of different choices should be discussed, as well as the \u201c24\u201d used in state features. \n\n4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size?  Or how to figure out a coordinate group? \n\n5. The caption in Figure 1,3, \u201cwith 48 input and hidden units\u201d should clarify clearly. \nThe curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only. \n\n6. typo: sec 1 parg 5, \u201ccurrent iterate\u201d -> \u201ccurrent iteration\u201d. \n\n\nConclusion:\n\nSince RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem.  With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm.  Sufficient experiment results show that the proposed method has better convergence rate than [1].  But comparing to [1], this paper has limited contribution. \n\n[1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]