The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one \"slow\" trained as usual and one \"fast\" that gets updated in every time-step based on the slow network.  The authors generalize this idea in a nice  way and present results on 1 experiment.  On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original.  On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors.  The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors.  For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from \"Attention is All You Need\").  Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]