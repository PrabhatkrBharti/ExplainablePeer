Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems.  The paper studies locally open maps, which preserve the local minima geometry.  Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map.  Theorem 3 provides conditions under which the multiplication X*Y is a locally open map.  For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima.   \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept.  However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks.  Further the claims all over the paper, comparing with the existing works. are over the top and not justified.  I believe the paper needs a significant rewriting .\n\nThe results are not a strict improvement over existing works.  For neural networks, Nguyen and Hein (2017) assume the link function is differentiable.  This paper assumes the link function is invertible.  Both papers can handle sigmoid/tanh, but cannot handle ReLU. \n\nResults for linear networks are not an improvement over existing works.  Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point.   Results are also written in a confusing way as stating each critical point is a saddle or a global minima.  Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly. \n\nProof of Lemma 7 seems to have typos/mistakes.  What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}? \n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W.[[CLA-POS],[JUS-NEG],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]