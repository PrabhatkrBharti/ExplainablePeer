The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. \n\n1.  The paper misses some more recent reference, e.g. [a,b].  The author should also have a discussion on them. \n\n2. Indeed, AlexNet is a good seedbed to test binary methods.  However, it is more interesting and important to test on more advanced networks.  So, I wish to see a section on testing with Resnet and GoogleNet. \n\nIndeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures. \" So, please show that. \n\n3. The paper wants to find a good trade-off on speed and accuracy.  The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy? \n\nMy concern is that one-bit system is already complicated to implement.  Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?  One example is Section 4 in [Courbariaux et al. 2016]. \n\n4. Is trade-off between 1 to 2 bits really important?  \n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1).  Is such improvement really important? \n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b].  Extremely low bit neural network: Squeeze the last bit out with ADMM[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]