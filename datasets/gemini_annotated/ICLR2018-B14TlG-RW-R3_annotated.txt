This paper presents a reading comprehension model using convolutions and attention.  This model does not use any recurrent operation but it is not per se simpler than a recurrent model.  Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.   On SQuAD dataset, their results show some small improvements using the proposed augmentation technique.  Their best results, however, do not outperform the best results reported on the leader board. \n\nOverall, this is an interesting study on SQuAD dataset.  I would like to see results on more datasets and more discussion on the data augmentation technique.  At the moment, the description in section 3 is fuzzy in my opinion.  Interesting information could be:\n- how is the performance of the NMT system?  \n- how many new data points are finally added into the training data set? \n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]