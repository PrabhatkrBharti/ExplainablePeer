The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks.  This is an important area and findings in this paper are interesting! \n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments. \n- Could we look at the two distributions of inputs that each neuron tries to separate?  \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results). \n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition.  2014\n\n+ Clarity: The paper is easy to read.  A few minor presentation issues:\n- ReLu --> ReLU\n\n+  Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014). \n\n+ Significance:\nWhile the results are interesting,  the contribution is not significant as the paper misses an important explanation for the phenomenon.  I'm not sure what key insights can be taken away from this[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]