This paper proposes a model for learning to generate data conditional on attributes.  Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time. \n\nThe model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before). \n\nHowever, this paper is mercilessly difficult to read.  The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (\u201cconcepts\u201d and \u201ccontext\u201d are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper. \n\nFor the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine \u201ccontexts\u201d that were not observed during training.  The experiments show that the generation component is quite effective,  but this is an obvious missing step. \n\nAnyway, some other related work:\nLample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes.  But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up.[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]