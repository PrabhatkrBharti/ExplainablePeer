Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \"luckiness\" of the license plate number.   The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings.  Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.   The proposed pipeline converts each character to an embedding with the only sentence of description being \"Each character is converted by a lookup table to a vector representation, known as character embedding.\"    Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined.  Finally they find a \"a feature vector for each plate by summing up the output of the last recurrent layer overtime. \ and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates.  In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better.  \n\nSpecific Comments on Sections: \nComments: Sec 1,2\nIn these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data. \n\nSec 3\nThe author does not mention the following reference: \"Deep learning for stock prediction using numerical and textual information\" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks. \n\nSec 4\nWhat are the characters embedded with? This is important to specify.  Is it Word2vec or something else?  What does the lookup table consist of?  References should be added to the relevant methods.  \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison.  LSTMs as well could have been a point of comparison.  \n\nSec 6\n Nothing too insightful is said about the RNN Model.  \n\nSec 7\nThe ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table. \n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN.  However comparisons are lacking and the paper is not presented very scientifically.   The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]