This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data.  The authors formulate the learning problem as a minimax problem which tries to choose diverse example and \"hard\" examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function.  The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration.  The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem.  The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on.  The belief is that this learning style mimics human learners .\n\nPros:\n- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas. \n- This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results. \n- Even through the submodular objective is only approximately solvable, it still translates into a convergence result. \n- The experimental results seem to be complete for the most part.  They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried. \n\nCons:\n- The main algorithm MCL is only a hueristic.  Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner. \n- It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL, and it seems unclear when the algorithm converge and how to increase/decrease it over iterations. \n- The objective function also seems somewhat non-intuitive.  Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well. \n- It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups).  This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets. \n\nOverall, I would like to see if the paper could have been stronger empirically.  Nevertheless, I do think there are some interesting ideas theoretically and algorithmically.  For this reason, I vote for a borderline accept[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]