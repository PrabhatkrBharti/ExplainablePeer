Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess.  \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity.  \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones. \nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar. \nThe authors evaluate their methods on two tasks, program generation and molecule generation.  \n\nTheir method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model.  \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.   \nHowever, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this.  \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though.  \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format? \n\nTypos:\n- Page 5: \"while in sampling procedure\" -> \"while in the sampling procedure\" \n- Page 6: \"a deep convolution neural networks\" -> \"a deep convolutional neural network\" "\n- Page 6: \"KL-divergence that proposed in\" -> \"KL-divergence that was proposed in\" " \n- Page 6: \"since in training time\" -> \"since at training time\"" \n- Page 6: \"can effectively computed\" -> \"can effectively be computed\"" \n- Page 7: \"reset for training\" -> \"rest for training\" "[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]