The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach . They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined.  The authors also show that these inferred quantities can be used to generate more effective attacks against the targets. \n\nThe paper is generally well written and most details for reproducibility are seem enough . I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing . It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening.  \n\nAside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters . I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent . It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case . \n\nI have found it hard to understand what table 3 in section 4.2 actually means . It seems to say for instance that a model is trained on 2 and 3 layers then queried with 4 and the accuracy only slightly drops . Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ?  \n\nMy main main concern is extrapolation out of the training set which is particularly important here. I don't find enough evidence in 4.2 for that point.  One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added . If these are better than random and the perturbations are more successful it would be a much more compelling story. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]