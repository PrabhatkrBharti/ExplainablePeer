This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long). \n\nOverall, I think it is a nice demonstration that non-recurrent models can work so well,  but I also don\u2019t find the results strikingly surprising.  It is also a bit hard to get the main takeaway messages.  It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547).  But all the other components seem to have been demonstrated previously in other papers.  \n\nAn important feature of this model is it is easier to parallelize and speed up the training/testing processes.  However, I don\u2019t see any demonstration of this in the experiments section. \n\nAlso, I am a bit disappointed by how \u201ccascades\u201d are actually implemented.  I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on).  The proposed model just simply sums up all the occurrences of candidate answers throughout the full document.  3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity.  \n\nI am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]