This paper mainly focuses on the square loss function of linear networks.  It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks.  Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks.  As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed.  In general, this manuscript is well written.    \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.  \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn\u2019t require assumptions on parameter dimensions and data matrices.  The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community. \n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation. \n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental.  The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016).   \n2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted.  This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.  The behaviors of linear networks and practical (deep and nonlinear) networks are very different.  Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community. \n\nMinors:\nThere are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]