In conventional boosting methods, one puts a weight on each sample.  The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.   Thus the learned weak learner at this round will make different mistakes. \nThis idea however is difficult to be applied to deep learning with a large amount of data.  This paper instead designed a new boosting method which puts large weights on the category with large error in this round.    In other words samples in the same category will have the same weight \n\nError bound is derived.   Experiments show its usefulness  though experiments are limited\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]