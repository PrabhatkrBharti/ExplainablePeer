The paper presents an interesting framework for bAbI QA.   Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences).   The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index.   While the argument makes sense, it is not clear to me why one cannot simply index the original text.  The additional encode/decode mechanism seems to introduce unnecessary noise.   The framework does include several components and techniques from latest recent work, which look pretty sophisticated.  However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven. \n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams \n\nCons:\n  1. The overall justification is somewhat unclear \n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n[[CLA-POS],[JUS-NEU],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEU],[ETH-NEG]]