The main idea of this paper is to replace the feedforward summation\ny = f(W*x + b)\nwhere x,y,b are vectors, W is a matrix\nby an integral\n\\y = f(\\int W \\x + \\b)\nwhere \\x,\\y,\\b are functions, and W is a kernel.  A deep neural network with this integral feedforward is called a deep function machine.  \n\nThe motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \\x, then one encounters the curse of dimensionality as one obtains finer and finer discretization.  The idea of functional PCA is to view \\x as a function is some appropriate Hilbert space, and expands it in some appropriate basis.  This way, finer discretization does not increase the dimension of \\x (nor its approximation), but rather improves the resolution.  \n\nThis paper takes this idea and applies it to deep neural networks.  Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea.  This approach amounts to a change of basis - and therefore the resolution invariance is not surprising.  In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components).  Unfortunately, this was not done. I suspect that in this case, the results would be very similar. \n\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]