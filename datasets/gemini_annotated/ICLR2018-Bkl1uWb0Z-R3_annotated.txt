This paper induces latent dependency syntax in the source side for NMT.  Experiments are made in En-De and En-Ru. \n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).  In light of this, I see very little novelty in this paper.  The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.  Seems thin for a ICLR paper. \n\nCaption of Fig 1: \"subject/object\" are syntactic functions, not semantic roles. \n\nI don't see how the German verb \"orders\" inflects with gender...  Can you post the gold German sentence? \n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing. \n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper. \n\nWhy is hard attention (sec 3.3) necessary?  It's not differentiable and requires sampling for training.  This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017). \n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing. \n\nIn table 3, it would be very helpful to display the English source. \n\nTable 4 is confusing.  The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset? \n\nComparison with predicted parses by Spacy are by no means \"gold\" parses... \n\nMinor comments:\n- Sec 1: \"... optimization techniques like Adam, Attention, ...\" -> Attention is not an optimization technique, but part of a model; \n- Sec 1: \"abilities not its representation\" -> comma before \"not\"[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]