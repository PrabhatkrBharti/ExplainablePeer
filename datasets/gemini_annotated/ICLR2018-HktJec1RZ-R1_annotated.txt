The paper introduces a neural translation model that automatically discovers phrases.   This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.  However, the clarity of the paper could be improved. \n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others? \n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?  In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]