The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text.  For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph.  The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information. \n\nThe paper is written well, good to understand, and technically sound.  I especially liked the general idea of using multiple modalities to improve embeddings of relational data.  This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings.  The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods. \n\nWith regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks.  This reference should be included in the related work.  The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty.  This claim needs to be discussed clearer as it is not clear to me why this would be the case. \n\nWith regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations.  However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models.  It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3).  Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. \"two of the predicted titles for Die Hard have something to do with dying and being buried\"). \n\nFurther comments:\n- The proposed method to incorporate numerical data seems quite ad hoc.  What are the motivations for this particular approach? \n- Are the image features fixed or learned?  In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet). \n- p.3: We use an appropriate encoder is repeated twice. \n- Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics. \n\n\n[1] Thoma et al: \"Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics\", 2017[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]