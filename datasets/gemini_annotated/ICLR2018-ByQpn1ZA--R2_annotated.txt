Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches.  The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments . \n\nClarity: The paper is well-written and clear . The authors could be more concise when reporting results . I would suggest keeping the main results in the main body and move extended results to an appendix. \n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs . More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails.  They also show experimentally that penalized gradients stabilize the learning process. \n\nSignificance: The problems the authors consider is worth exploring further . The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally . However, publishing this  work is in my opinion premature for the following reasons:\n\n- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap ;\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory ;\n- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]