Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers.  While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized.  As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100. \n\nOne thing that is currently not very clear to me is about the rotational symmetry.  The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer.   The rotation of the filters (as 2D images or images with depth) seem to be quite different from \"rotating\" a general N-dim vectors in an abstract Euclidean space.   It would be helpful to make the description here more explicit and clear.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]