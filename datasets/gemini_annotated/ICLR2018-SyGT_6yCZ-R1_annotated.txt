This paper deals with early stopping but the contributions are limited.  This work would fit better a workshop as a preliminary result, furthermore it is too short.  Following a short review section per section. \n\nIntro: The name SFC is misleading as the method consists in stopping early the training with an optimized learning schedule scheme.  Furthermore, the work is not compared to the appropriate baselines. \n\nProposal: The first motivation is not clear.  The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks.  Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking. \nThe second motivation, w.r.t. IB seems interesting  but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done. \n\nThe section 3 is quite long and could be compressed to improve the relevance of this experimental section.  All the accuracies(unsup dict, unsup, etc) on CIFAR10/CIFAR100 are reported from the paper (Oyallon & Mallat, 2015), ignoring 2-3 years of research that leads to new numerical results.  Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example.  \n\nFinally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here. \n\nI believe re-thinking new learning rate schedules is interesting,  however I recommend the rejection of this paper.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]