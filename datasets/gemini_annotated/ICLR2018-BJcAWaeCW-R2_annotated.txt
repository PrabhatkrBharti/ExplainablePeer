Quality: The work has too many gaps for the reader to fill in.  The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.  I am not sure how this is achieved in this work.  The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.  Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.  How can such a limited technique be generalizing?  There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\ n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.  It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,  although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.  From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function.  If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?  This is a key point not discussed in the manuscript.  \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data.  And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output? \n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster.  There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work.  There is no contribution in the GAN / neural network aspect.  It is also unclear whether the model generalizes.  I don\u2019t think this is a good fit for ICLR. \n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases.  The work itself, however, falls short of the goal.  At best the generator seems to be working but I fear it is overfitting.  The contribution for ICLR is rather minimal, unfortunately[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]