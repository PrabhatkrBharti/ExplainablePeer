This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution.  The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator.  Lastly, they incorporate attention for large visual inputs.  \n\nThe unimodal claim for distribution without randomness is weak.  The distribution \ncould be replaced with a normalizing flow.  The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it. \n\nMoreover, it really felt like the biased importance sampling approach should be \ncompared to a formal inference scheme.  I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC. \n\nHow important is using the pretrained weights from the deterministic RNN? \n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]