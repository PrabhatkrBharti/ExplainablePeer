The authors describe a method for encoding text into a discrete representation / latent space . On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT. \n\nThe proposed method seems effective, and the proposed DSAE metric is nice, though it\u2019s surprising if previous papers have not used metrics similar to normalized reduction in log-ppl . The datasets considered in the experiments are also large, another plus.  However, overall, the paper is difficult to read and parse, especially since low-level details are weaved together with higher-level points throughout, and are often not motivated. \n\nThe major critique would be the qualitative nature of results in the sections on \u201cDecipering the latent code\u201d and (to a lesser extent) \u201cMixed sample-beam decoding. \u201d These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3.  Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]