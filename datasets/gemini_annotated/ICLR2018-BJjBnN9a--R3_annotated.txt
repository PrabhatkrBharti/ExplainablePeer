This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases.  A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs. \n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.  Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters.  Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN. \n\nWhile the idea is interesting and might be a good alternative to standard CNNs,  the paper falls short in terms of providing experimental validation that would demonstrate the latter point.  It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.  They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results.  The CIFAR-10, STL-10, and SVHN results are disappointing.  CCNNs do not outperform the prior CNN results listed in Table 2,3,4.  Moreover, these tables do not even cite more recent higher-performing CNNs.  See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper. \n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?   To be competitive, deeper CCNNs would likely need to be trained.   I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.   Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?   Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.  \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table.   This would assist in understanding tradeoffs in the design space.  \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?   If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?  \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN?   Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training.   Such dependence for CCNNs appears to be a weakness in comparison.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]