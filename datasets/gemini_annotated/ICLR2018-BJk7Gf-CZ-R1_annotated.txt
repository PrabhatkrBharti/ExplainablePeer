The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks.  The paper is an extension of Kawaguchi'16.  It also provides some sufficient conditions for the non-linear cases.  \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16.  I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it.  As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique.  Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum.  This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]