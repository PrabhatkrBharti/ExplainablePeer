The paper proposes a piecewise linear activation function that is build on ELU.  In general it was an OK paper and there are many to be improved.\n\n+ Novelty seems minor.  In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable.  The idea proposed in the paper is just a stack of \"better\" experiments.  Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape? \n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated.  In  section 3, the shifted version, \\delta, is abruptly proposed only based on \"results presented in 4.1\" could improve learning.  This is not a professional ML paper looks like.  \n\n+ Experiment not strong to support the idea.  Experiments are only conducted in CIFAR100.  This is obviously not enough.  In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins).  And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%. \n\nAlso, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR. [[CLA-POS],[JUS-NEG],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-NEG],[ETH-NEG]]