The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.  The activation function keeps the activation roughly zero-centered.  \n\nIn general, this is an interesting direction to explore, the idea is interesting,;  however, I would like to see more experiments. \n\n1. The authors tested out this new activation function on RNNs.  It would be interesting to see the results of the new activation function on LSTM. \n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers.  For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.   \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling. \n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm. \n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without. \n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]