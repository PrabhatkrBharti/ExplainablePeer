This paper presents a lifelong learning method for learning word embeddings.   Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings. \n\nThe general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.   However, this submission needs to be improved in terms of clarity and its experiments. \n\nIn terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why.   When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful.   What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?   The paper states the purpose of the meta learning is \"to learn a general word context similarity from the first m domains\", but I was never sure what this meant.   Further, some of the paper's pseudocode includes unexplained steps like \"invert by domain index\" and \"scanco-occurrence\".   \n\nIn terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works.   First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered.   Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy,;  see :\n\"Think Globally, Embed Locally\u2014Locally Linear Meta-embedding of Words\", Bollegala et al., 2017\nhttps://arxiv.org/pdf/1709.06671.pdf\n\nThat paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission. \n\nBeyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach.   What if we just selected the one past domain that was most similar to the new domain, by some measure?   And how does the performance of the technique depend on the setting of m?   Investigating some of these questions would help us understand how well the approach works and in which settings. \n\nMinor:\n\nSecond paragraph, GloVec should be GloVe\n\n\"given many domains with uncertain noise for the new domain\" -- not clear what \"uncertain noise\" means, perhaps \"uncertain relevance\" would be more clear. \n\nThe text refers to a Figure 3 which does not exist, probably means Figure 2.   I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2 \n\nWhen m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary. \n\n\"from the first m domain corpus\" -> \"from the first m domains\"? \n\n\"may not helpful\" -> \"may not be helpful\"\n\n\"vocabularie\" -> \"vocabulary\"\n\n\"system first retrieval\" -> \"system first retrieves\". \n\nCOMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines.   The concatenation does fairly comparably to LL in Tables 3&4.   LL wins by a bit more in Table 2.   Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]