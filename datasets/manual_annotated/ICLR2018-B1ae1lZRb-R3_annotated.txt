Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation. \nScheme A consists of training a high precision teacher jointly with a low precision student.  Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode. \n\nReview:\nThe paper is well written.  The experiments are clear and the three different schemes provide good analytical insights. \nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n Comments:\nTensorflow citation is missing.\n Conclusion is short and a few directions for future research would have been useful.[[CLA-POS],[JUS-POS],[DEP-NEG],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]