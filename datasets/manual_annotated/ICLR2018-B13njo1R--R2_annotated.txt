This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum. \n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty. \n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain. \n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case. \n- Are the experiments single runs?  Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour. \n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks? \n- How were the network architecture and network size chosen, especially for the multitasker?  Would policies generalize to later tasks better with larger, or smaller networks? \n- Was any kind of regularization used, how does it influence task performance vs. transfer? \n- I find figure 1 (c) somewhat confusing.  Is performance maintained only on the last 2 tasks, or all previously seen tasks?  That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]