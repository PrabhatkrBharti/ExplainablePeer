In this paper, a model is built for reading comprehension with multiple choices.  The model consists of three modules: encoder, interaction module and elimination module.  The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations.  Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored. \n\nThe elimination module is interesting,  but the usefulness of \u201celimination\u201d is not well justified for two reasons.  First, the improvement of the proposed model over the previous state of the art is limited.  Second, the model is built upon GAR until the elimination module, then according to Table 1 it seems to indicate that the elimination module does not help significantly (0.4% improvement).  \n\nIn order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e. after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy).  Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps. \n\nOther issues:\n\n1) Is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $\\tilde{x}_i$?  Even though the authors find the orthogonal vectors, they\u2019re gated summed together very soon.  It would be better to show how much \u201celimination\u201d and \u201csubtraction\u201d effect the final performance, besides the effect of subtraction gate.\n\n2)  A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.\n\n 3) $c_i$ in page 5 is not defined.  What\u2019s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function? \n\n4) It would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance, besides the case study in Figure 3 .\n\nMinor issues:\n\n1) In Eqn. (4), it would be better to use a vector as the input of softmax.\n\n 2) It would be easier for discussion if the authors could assign numbers to every equation.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]