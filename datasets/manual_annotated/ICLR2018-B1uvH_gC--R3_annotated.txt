The key contribution of the paper is a new method for nonlinear dimensionality reduction.  \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling.  While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry.  The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets).  \n\nThe paper revisits mostly familiar ideas.  The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is.  This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution. \n\nMoreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice.  However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with \"holes\" in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc).  \n\nFinally, the experimental results are somewhat uninspiring.  It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x.  \n\nThe horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEU],[ENG-NEU],[ACC-NEU],[CST-NEU],[NOV-NEU],[ETH-NEU]]