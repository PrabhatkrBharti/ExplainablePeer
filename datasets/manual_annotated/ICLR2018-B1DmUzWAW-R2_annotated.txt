The authors propose a model for sequence classification and sequential decision making.  The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution.  The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making. \n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control). \n\nThe experiment section definitely demonstrate the effort put into this work.  However, my primary concern is that the model seems somewhat lacking in novelty.  Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO.  The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding.  I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making.  I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information). \n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning.  I think this information would be very useful to the community in terms of what to take away from this paper.  In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains. \n\nOverall, I am inclined to accept this paper on the basis of its experimental results.  However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain). \n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation \n- In the experiment section for Omniglot, when the authors say \"1200 classes for training and 432 for testing\", it sounds like the authors are performing zero-shot learning.  How does this particular model generalize to classes not seen during training?[[CLA-POS],[JUS-POS],[DEP-NEG],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-POS]]