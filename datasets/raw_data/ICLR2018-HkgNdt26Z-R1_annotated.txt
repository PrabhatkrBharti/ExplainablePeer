 "This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input.[[PDI-NEU,DAT-NEU,EXP-NEU,MET-NEU], [null]]  For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better.[[RWK-NEU,EXP-NEU,MET-NEU], [CMP-NEU,EMP-NEU]] \nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved.[[RWK-NEU,PDI-NEU], [null]] \n\nBasically the motivation and method is good, the drawback of this paper is\nits narrow scope and lack of necessary explanations.[[RWK-NEG,PDI-POS], [CLA-NEG,EMP-NEG]]  Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \"general English\".[[RWK-NEU], [null]]  Why is this necessary?[[RWK-NEU], [null]]  Why not\n  just using better enough basic English and the text of the target user?[[RWK-NEU], [null]] \n\n- To achieve the goal above, huge data (not the \"portion of the general English\") should be communicated over the network.[[RWK-NEG,PDI-NEU], [SUB-NEG,EMP-NEG]]  Is this really worth doing? [[RWK-NEU], [null]] If only\n  \"the portion of\" general English must be communicated, why is it validated?[[RWK-NEU], [null]] \n\n- For measuring performance, authors employ keystroke saving rate.[[RWK-NEU,RES-NEU], [EMP-NEU]]  For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation.[[RWK-NEU], [null]]  Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important.[[RWK-NEU,MET-NEU], [EMP-NEU]] \n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models.[[RWK-NEU,MET-NEU], [EMP-NEU]]  Are there any\n  comparison with these previous efforts?[[RWK-NEU], [CMP-NEU]] \n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so.[[RWK-NEU], [IMP-NEU,REC-NEU,EMP-NEU]]  I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts.[[OAL-NEU], [REC-NEU]] \n\nMinor:\n- t of $G_t$ in page 2 is not defined so far.\n- What is \"gr\" in Section 2.2[[RWK-NEG], [EMP-NEG]]