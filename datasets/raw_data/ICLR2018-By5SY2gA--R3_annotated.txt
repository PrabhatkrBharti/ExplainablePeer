 "This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings.[[INT-NEU], [null]]  The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection.[[MET-NEU], [null]] \n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning.[[RWK-NEU], [SUB-POS]]  The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work.[[RWK-NEU], [CMP-NEU]]  The main originality seems to be captured in Algorithm 1, which computes the strength between two words.[[MET-NEU], [NOV-NEU]]  Unlike prior work, this is a real-valued instead of a binary quantity.[[RWK-NEU,MET-NEU], [CMP-POS]]  Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques.[[MET-NEU], [NOV-NEU,EMP-NEU]]  In this light, the performance relative to the baselines is particularly important.[[RWK-NEU], [CMP-NEU]]  From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods. [[RWK-NEU,TNF-NEG], [IMP-NEG,CMP-NEG]] I therefore cannot recommend this paper for publication."[[OAL-NEG], [REC-NEG]]