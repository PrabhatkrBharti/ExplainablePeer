 "Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games.[[INT-NEU], [null]]  The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS.[[MET-NEU], [null]] \n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning.[[MET-NEU], [null]] \n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space.[[MET-NEU], [null]] \n2. Evaluates how informative the latent states are via state reconstruction.[[MET-NEU], [null]] \n3. trains variatns of a forward model f on the hidden states of the various learned agents.[[EXP-NEU], [null]] \n4. evaluates different f within MCTS for MiniRTS.[[MET-NEU,ANA-NEU], [null]] \n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods.[[PDI-POS], [EMP-POS]] \n- The experimental setting is very non-trivial and novel.[[EXP-POS], [NOV-POS]] \n\nCon:\n- The manuscript is unclear in many parts -- this should be greatly improved.[[OAL-NEG], [CLA-NEG]] \n1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?).[[MET-NEG], [EMP-NEG]]  Which forward model is trained from which model-free agent?[[MET-NEU], [EMP-NEU]] \n2. How is the forward model / value function used in MCTS?[[MET-NEU], [EMP-NEU]]  I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together.[[MET-NEG], [EMP-NEG]] \n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X.[[MET-NEU], [EMP-NEU]]  Yet it is unclear how this informs us about the quality of the learned forward models f.[[MET-NEU], [EMP-NEG]]  It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models.[[MET-NEU,ANA-NEU], [EMP-NEG]]  Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8.[[TNF-NEG], [EMP-NEG]] \n\n- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent.[[RES-NEG,ANA-NEG], [EMP-NEG]]  Is this because of the MCTS approach?[[MET-NEU], [EMP-NEU]]  Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis[[PDI-POS,ANA-NEG], [EMP-NEG]] .\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model?[[MET-NEU], [EMP-NEU]] \n- What is the error of the raw h-predictions?[[MET-NEU], [EMP-NEU]]  Only the state-reconstruction error is shown now.[[RES-NEU], [PNF-NEU]] \n- Figure 6 / sect 4.2: which model-free agent is used?[[TNF-NEU], [EMP-NEU]]  Also fig 6 misses captions.[[TNF-NEG], [PNF-NEG]] \n- Figure 8: scrambled caption.[[TNF-NEG], [PNF-NEG]] \n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case?[[RWK-NEU,MET-NEU], [EMP-NEU]]