 "The authors propose a new exploration algorithm for Deep RL[[INT-NEU,MET-POS], [null]] . They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style.[[MET-NEU], [null]] \n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper.\[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG]] n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper). [[MET-POS,RES-POS], [null]] Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise? [[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU]] \n-How does the distribution of Q values look like during different phases of learning?[[RES-NEU,ANA-NEU,TNF-NEU], [IMP-NEU]] \n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2.[[MET-NEU], [null]] \n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)?[[MET-NEU], [null]] \n\nMinor things:\n-Missing propto in Eq 7[[MET-NEU], [PNF-NEG]] ?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n"[[OAL-NEU], [PNF-NEG]]