 "Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH.[[INT-NEU,PDI-NEU,MET-NEU], [null]]  I found the paper relatively creative and generally well-founded and well-argued.[[OAL-POS], [EMP-POS]] \n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product. [[OAL-POS], [EMP-POS]] \n\nExperiments: appreciated the wall clock timings.[[EXP-POS], [EMP-POS]] \n\nSGD comparison: \u201cfixed learning rate.[[MET-POS], [EMP-POS]] \u201d Didn't see how the initial (well constant here) step size was tuned?[[EXP-NEU], [EMP-NEU]]  Why not use the more standard 1/t decay?[[MET-NEU], [EMP-NEU]] \n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?[[MET-NEU], [EMP-NEU]]  Legend backwards?[[MET-NEU], [EMP-NEU]] \n\nWhy were so many of the chosen datasets have so few training examples?[[DAT-NEU], [SUB-NEU]] \n\nPaper is mostly very clearly written,[[OAL-POS], [CLA-POS]]  though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n"[[OAL-NEG], [CLA-NEG,PNF-NEG]]