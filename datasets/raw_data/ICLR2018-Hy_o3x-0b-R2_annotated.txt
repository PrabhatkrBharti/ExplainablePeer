 "The description of the proposed method is very unclear.[[MET-NEG], [EMP-NEG]]  From the paper it is very difficult to make out exactly what architecture is proposed.[[MET-NEG], [EMP-NEG]]  I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior?[[MET-NEG], [EMP-NEG]]  Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training.[[EXP-NEG,MET-NEG], [EMP-NEG]]  I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?[[EXP-NEU,MET-NEU], [EMP-NEU]]  That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns?[[MET-NEG,TNF-NEU], [PNF-NEU]] \n\n* The word \"layers\" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder.[[EXP-NEU], [PNF-NEU]]  E.g. \"The top stochastic layer z_L in FAME is a fully-connected dense layer\".[[MET-NEU], [null]]  No, z_L is a vector of latent variables.[[MET-NEU], [null]]  Are you saying the encoder produces it using a fully-connected layer?[[EXP-NEU], [EMP-NEU]] \n* Section 2.2 starts talking about \"deterministic layers h\".[[MET-NEU], [null]]  Are these part of the encoder or decoder?[[EXP-NEU], [EMP-NEU]]  What is meant by \"number of layers connecting the stochastic latent variables\"?[[MET-NEU], [CNT]] \n* Section 2.3: What is meant by \"reconstruction data\"?[[DAT-NEU], [null]] \n\nIf my understanding of the method is correct, the novelty is limited.[[MET-NEG], [NOV-NEG]]  Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.[[RWK-NEU,MET-NEU], [CMP-NEU]]  The reported likelihood results are very impressive though, and would be reason for acceptance if correct.[[RES-POS,OAL-POS], [REC-POS,EMP-POS]]  However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood.[[DAT-NEG], [EMP-NEG]]  There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound.[[EXP-NEG,MET-NEG], [EMP-NEG]]  Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \"eps\": this is not guaranteed to give the right result.[[EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG]]  Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y).[[EXP-NEG], [EMP-NEG]]  Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain)."[[EXP-NEG], [EMP-NEG]]