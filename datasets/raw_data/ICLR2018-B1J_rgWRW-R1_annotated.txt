 "This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks.[[MET-NEU,RES-NEU], [null]]  I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.[[MET-NEU,RES-NEU], [null]]  Any smooth function can be approximated by such networks.[[MET-NEU,RES-NEU], [null]] \n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN.[[MET-NEU,RES-NEU], [null]] \n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces[[MET-NEU], [null]] \n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm.[[MET-NEU,RES-NEU], [null]] \n\nAmong these results (1), (2), (4) are sort of known in the literature.[[RES-NEU], [null]]  This paper extends the existing results in some subtle ways.[[RES-NEU], [EMP-NEU]]  For (1), the authors show that the DNN has a tighter bound on the depth.[[RES-NEU], [null]] For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum.[[MET-NEU,RES-NEU], [null]] \n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU.[[RES-POS], [EMP-POS]]  Other than that, I don't get much more insight from the theoretical result.[[RES-NEG], [EMP-NEG]]  When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient.[[MET-NEU,RES-NEG], [EMP-NEG]]  Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough.[[MET-NEG], [SUB-NEG,EMP-NEG]] \n\nResult (3) is more interesting as it is a new result.[[RES-POS], [NOV-POS]]  The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.[[MET-NEU], [EMP-NEU]]   However, the construction seems artificial and these functions don't seem to be visually very complex.[[MET-NEG,RES-NEG], [EMP-NEG]] \n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks.[[FWK-POS,OAL-POS], [NOV-POS]]  The results might be of theoretical interest,[[RES-POS], [EMP-POS]]