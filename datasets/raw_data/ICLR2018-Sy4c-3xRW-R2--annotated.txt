 "Pros\n- The proposed model is a nice way of multiplicatively combining two features :\n  one which determines which classes to pay attention to, and other that\nprovides useful features for discrimination.[[MET-POS], [EMP-POS]] \n\n- The adaptive component seems to provide improvements for small dataset sizes[[DAT-POS,MET-POS], [EMP-POS]]   and large number of classes.[[DAT-POS,MET-POS], [EMP-POS]] \n\nCons\n- \"One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the\n  classification and the gradients are not back-propagated from it.[[MET-NEG], [EMP-NEG]] \" : This does\nnot seem to be true.[[MET-NEG], [EMP-NEG]]  Even if the logits are zero, the class would have a\nnon-zero probability and would receive gradients.[[RES-NEU], [EMP-NEG]]  Do the authors mean\nexp(o_t(x;w)) = 0 ?[[MET-NEU], [EMP-NEU]] \n\n- Related to the above, it should be clarified what is meant by dropping a\n  class.[[MET-NEU], [EMP-NEU]]  Is its logit set to zero or -\\infty ?[[MET-NEU], [EMP-NEU]]  Excluding a class from the\nsoftmax is equivalent to having a logit of -\\infty, not zero.[[MET-NEU], [EMP-NEU]]  However, from the\nequations in the paper it seems that the logit is set to zero.[[MET-NEU], [EMP-NEU]]  This would not\nresult in excluding the unit.[[RES-NEU], [EMP-NEU]]  The overall effect would just be to raise the\nmagnitude of logits across the entire softmax.[[RES-NEU], [EMP-NEU]] \n\n- It seems that the model benefits from at least two separate effects - one is\n  the attention mechanism provided by the sigmoids, and the other is the\nstochasticity during training.[[EXP-POS,MET-POS], [EMP-POS]]  Presently, it is not clear if only one of the\ncomponents is providing most of the benefits, or if both things are useful.[[RES-NEG], [EMP-NEG]]  It\nwould be great to compare this model to a non-stochastic one which just has the\nmultiplicative effects applied in a deterministic way (during both training and\ntesting).[[MET-NEU], [EMP-NEU]] \n\n- The objective of the attention mechanism that sets the dropout mask seems to\n  be the same as the primary objective of classifying the input, and the\nattention mechanism is prevented from solving the task by adding an extra\nentropy regularization.[[EXP-NEU,MET-NEU], [EMP-NEU]]  It would be useful to explain more why this is needed.[[EXP-NEU,MET-NEU], [SUB-NEU]] \nWould it not be fine if the attention mechanism did a perfect job of selecting\nthe class ?[[MET-NEU], [EMP-NEU]] \n\nQuality\nThe paper makes relevant comparisons and is overall well-motivated.[[OAL-POS], [SUB-POS,CMP-POS]]  However,\nsome aspects of the paper can be improved by adding more explanations.[[OAL-NEU], [SUB-NEU]] \n\nClarity\nSome crucial aspects of the paper are unclear as mentioned above.[[OAL-NEG], [CNT]] \n\nOriginality\nThe main contribution of the paper is similar to multiplicative gating.[[RWK-NEU], [CMP-NEU]]  The\nadded stochasticity and the model ensembling interpretation is probably novel.[[MET-POS], [NOV-POS]] \nHowever, experiments are insufficient to determine whether it is this novelty\nthat contributes to improved performance or just the gating.[[EXP-NEG], [NOV-NEG]] \n\nSignificance\nThis paper makes incremental improvements and would be of moderate interest to\nthe machine learning community.[[FWK-POS], [IMP-POS]] \n\nTypos :\n- In Eq 3, the numerator has z_t. Should that be z_y ?[[MET-NEU], [CLA-NEG]] \n- In Eq 5, the denominator has z_y.[[MET-NEU], [CLA-NEG]]  Should that be z_t ?"[[MET-NEU], [CLA-NEG]]