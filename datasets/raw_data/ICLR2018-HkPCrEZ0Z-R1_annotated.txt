 "The paper studies a combination of model-based and model-free RL.[[INT-NEU,MET-NEU], [null]]  The idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learning.[[PDI-NEU,EXP-NEU,MET-NEU], [null]]   Some parts of the paper lack clarity and the empirical results need improvement to support the claims (see details below). [[RES-NEG,OAL-NEG], [CLA-NEG,EMP-NEG]]   \n\nClarity \n- The main idea of the proposed method is clear.[[PDI-POS], [CLA-POS]]  \n- Some notations and equations are broken.[[TNF-NEG], [PNF-NEG]]  For example: \n(1) The definition of \\bar{A} in Section 4 is broken.[[RWK-NEU], [null]]  \n(2) The overall objective in Section 5 is broken.[[RWK-NEU], [null]]  \n(3) The computation of w in Algorithm 2 is problematic.[[RWK-NEG,MET-NEG], [IMP-NEG]]  \n- Some details of the experiments/methods are confusing.[[EXP-NEG,MET-NEG], [IMP-NEG]]  For example: \n(1) The step number k is dynamically determined by a short line search as in Section 4 ``Dynamic Rollout\u2019\u2019, but later in the experiments (Section 6) the value of k is set to be 2 uniformly.[[RWK-NEU,PDI-NEU,MET-NEU,BIB-NEU], [null]]  \n(2) Only the policy and value networks specified.[[RWK-NEG], [EMP-NEG]]  The forward models are not specified.[[MET-NEG], [IMP-NEG,EMP-NEG]]   \n(3) In algorithm 1, what exact method is used in determining if \\mu is converged or not?[[RWK-NEU,MET-NEU], [null]]  \n\nOriginality\nThe proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm.[[PDI-NEU,MET-NEU], [EMP-NEU]]  An empirical comparison could be helpful but not provided.[[PDI-NEG], [CMP-NEG,EMP-NEG]]  \n\nThe idea of the proposed method is related to the classic Dyna methods from Sutton.[[RWK-NEU,PDI-NEU,EXP-NEU], [EMP-NEU]]  A discussion on the difference would be helpful.[[EXT-NEU], [CNT]]  \n\nSignificance\n- The paper could compare against other relevant baselines that combine model-based and model-free RL methods, such as SVG (stochastic value gradient).[[RWK-NEU,EXP-NEU,MET-NEU], [CMP-NEU]]  \n- To make a fair comparison, the results in Table 1 should consider the amount of data used in pre-training the forward models.[[DAT-NEU,EXP-NEU,RES-NEU,TNF-NEU], [CMP-NEU,EMP-NEU]]  Current results in Table 1 only compare the amount of data in policy learning.[[DAT-NEU,RES-NEU,TNF-NEU], [null]]   \n- Figure 3 is plotted for just one random starting state.[[TNF-NEU], [null]]  The Figure could have been more informative if it was averaged over different starting states.[[TNF-NEG], [SUB-NEG]]   The same issue is found in Figure 2.[[TNF-NEU], [null]]   It would be helpful if the plots of other domains are provided.[[RWK-NEG], [SUB-NEG,EMP-NEG]]  \n- In Figure 2, even though the diff norm fluctuates, the cosine similarity remains almost constant.[[PDI-NEU,MET-NEU,TNF-NEU], [null]]  Does it suggest the cosine similarity is not effective in measuring the state similarity?[[PDI-NEU,MET-NEU], [null]]  \n- Figure 1, 4 and 5 need confidence intervals or standard errors.[[FWK-NEG], [SUB-NEG]]  \n\nPros:\n- The research direction in combining model-based and model-free RL is interesting.[[PDI-POS,MET-POS], [IMP-POS]] \n- The main idea of the proposed method is clear.[[PDI-POS], [CLA-POS]]  \n\nCons:\n- Parts of the paper are unclear and some details are missing.[[RES-NEG], [CLA-NEG,SUB-NEG]]  \n- The paper needs more discussion and comparison to relevant baseline methods.[[EXP-NEG,OAL-NEG], [SUB-NEG,CMP-NEG]]   \n- The empirical results need improvement to support the paper\u2019s claims.[[RES-NEG], [SUB-NEG,EMP-NEG]]