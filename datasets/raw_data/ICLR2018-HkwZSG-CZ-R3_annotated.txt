 "Language models are important components to many NLP tasks.[[EXT-NEU], [null]]  The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.[[EXT-NEU], [null]]  This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.[[INT-NEU,PDI-NEU], [null]]  The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization.[[PDI-NEU], [null]] \n\nPros:\n--The paper is very well written and easy to follow.[[OAL-POS], [CLA-POS]]  The ideas build up on each other in an intuitive way.[[PDI-POS,MET-POS], [EMP-POS]] \n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know.[[PDI-POS,OAL-POS], [NOV-POS]] \n--The maths is very rigorous.[[MET-POS], [EMP-POS]] \n--The experiment section is thorough.[[EXP-POS], [EMP-POS]] \n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).[[RWK-NEU], [CMP-NEU]]  In Table 2 the baselines have a lower capacity.[[RWK-NEG,EXP-NEG,TNF-NEG], [CMP-NEG]]  This is an unfair comparison[[RWK-NEG,EXP-NEG], [CMP-NEG]] \n--I suspect the proposed approach is slower than the baselines.[[RWK-NEG,MET-NEG], [CMP-NEG]]  There is no mention of computational cost.[[ANA-NEG], [SUB-NEG]]  Reporting that would help interpret the numbers.[[ANA-NEU], [SUb-NEU]]  \n\nThe SOTA claim might not hold if baselines are given the same capacity.[[RWK-NEG,EXP-NEG], [EMP-NEG]]  But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR."[[OAL-POS], [REC-POS]]