 "This paper propose a simple method for guarding trained models against adversarial attacks. [[RWK-NEU,PDI-NEU,MET-NEU], [EMP-NEU]] The method is to prune the network\u2019s activations at each layer and renormalize the outputs.[[RWK-NEU,MET-NEU], [EMP-NEU]]  It\u2019s a simple method that can be applied post-training and seems to be effective.\[[EXP-NEU,MET-POS,BIB-NEU], [EMP-POS]] n\nThe paper is well written and easily to follow.[[OAL-POS], [CLA-POS]]  Method description is clear.[[RWK-NEU,MET-POS], [CLA-POS]]  The analyses are interesting and done well.[[RWK-POS,ANA-POS], [IMP-POS]]  I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods.[[OAL-NEU], [null]] \n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images.[[RWK-NEU,MET-NEU,TNF-NEG], [SUB-NEG]]  Would be good to know if the authors have any intuition why is that the case.[[PDI-NEU,EXT-NEU], [null]] \n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text.[[BIB-POS,EXT-NEU], [null]] \n\nI would be interested to know more about the intuition behind the proposed method.[[RWK-NEG,PDI-NEG,MET-NEG], [SUB-NEG,EMP-NEG]]  It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method.[[EXT-NEU], [null]] \n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial.[[RWK-NEU,EXP-NEG], [SUB-NEG,EMP-NEG]] \n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014.[[BIB-NEU,EXT-NEU], [null]]  Would be good to see the performance against other forms of adversarial attacks as well if they exist.[[RWK-NEG,EXP-NEG,ANA-NEG], [SUB-NEG]]