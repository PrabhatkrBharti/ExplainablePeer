 "After reading the rebuttal:\n\nThis paper does have encouraging results.[[RES-NEG], [EMP-NEG]]   But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons.[[RWK-NEG,MET-NEG], [CMP-NEG]]   The writing also needs to be improved.[[OAL-NEG], [CLA-NEG]]   So I think the paper is not ready for publication and my opinion remains.[[OAL-NEG], [REC-NEG]]  \n===========================================================\n\nThis paper presents an algorithm for few shot learning.[[MET-NEU], [null]]   The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels).[[MET-NEU], [null]]   I think the idea of representation learning using a somewhat artificial task makes sense in this setting. \n\nI have several concerns for this submission.[[MET-NEG], [EMP-NEG]]  \n1. I am not very familiar with the literature of few shot learning.[[RWK-NEU], [null]]   I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.[[RWK-NEU,MET-NEU], [CMP-NEU]]   Multilingual Distributed Representations without Word Alignment. ICLR 2014.[[RWK-NEU], [null]] \nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin.[[RWK-NEU,MET-NEU], [CMP-NEU]]  This approach is useful even when there is only weak supervision to provide the \"similarity/dissimilarity\" information.[[MET-POS], [EMP-POS]]  I wonder how does this approach compare with the proposed method.[[MET-NEU], [CMP-NEU]] \n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU]]  I do not understand why the compared methods are not consistently used in both experiments.[[EXP-NEG,MET-NEG], [EMP-NEG]]  Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is.[[MET-NEG], [EMP-NEG]]  It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy.[[RWK-POS,MET-POS], [EMP-POS]]  It would be nice to understand/visualize what information have been extracted in the representation learning phase.[[MET-NEU], [EMP-NEU]]  \n\n3. Relatively minor: The writing of this paper is readable,[[OAL-POS], [CLA-POS]]  but could be improved.[[OAL-NEG], [CLA-NEG]]  It sometimes uses vague/nonstandard terminology (\"parameterless\") and statement.[[OAL-NEG], [CLA-NEG]]  The term \"siamese kernel\" is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs.[[MET-NEG,RES-NEG], [EMP-NEG]]  In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning."[[MET-NEU], [SUB-NEU]]