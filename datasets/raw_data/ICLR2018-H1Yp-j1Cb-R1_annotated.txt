 "It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing[[RWK-NEU,MET-NEG], [EMP-NEG]] . Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game). [[RWK-NEU,MET-NEG], [EMP-NEG]] The present paper proposes to obtain mixed strategy through an online learning approach.[[PDI-NEU,MET-NEU], [EMP-NEU]]  Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss.[[RWK-NEU,EXP-NEU,MET-NEU,RES-NEU], [IMP-NEU,EMP-NEU]]  One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M)[[RWK-POS,EXP-POS,RES-POS], [SUB-POS,EMP-POS]] .In particular, the concave player plays the FTRL algorithm with standard L2 regularization term.[[RWK-NEU,MET-NEU], [EMP-NEU]]  The regret of concave player can be bounded using existing result for FTRL.[[RWK-NEU,EXP-NEU,RES-NEU], [EMP-NEU]] The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically. [[RWK-NEU,EXP-NEU], [null]] Then a lemma by Kalai and Vempala can be used.[[RWK-NEU,EXP-NEU], [EMP-NEU]]  The theory part of the paper is reasonable and quite well written.[[RWK-POS,EXP-POS], [CLA-POS,SUB-POS]]  \n\nBased on the theory developed, the paper presents a practical algorithm.[[RWK-POS,EXP-POS], [SUB-POS,IMP-POS,EMP-POS]]  Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration.[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [CMP-NEU,EMP-NEU]]  The paper claims that this may help to prevent model collapsing.[[INT-NEU,PDI-NEU,MET-NEU], [IMP-NEU,EMP-NEU]] \n\nHowever, the experimental part is less satisfying.[[RWK-NEG,EXP-NEG], [EMP-NEG]]  From figure 2, I don't see much advantage of Checkhov GAN.[[MET-NEG,TNF-NEG], [IMP-NEG]]  In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA).[[RWK-NEG,EXP-NEG], [EMP-NEG]] The paper didn't really compare other popular GAN models, especially WGAN and its improved version[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG]] , which is already quite popular by now and should be compared with.[[RWK-POS,MET-POS], [CMP-POS]] \n\nOverall, I think it is a borderline paper.[[INT-NEU,RWK-NEU,OAL-NEU], [APR-NEU,REC-NEU]] \n\n-------------------------\nI read the response and the new experimental results regarding WGAN.[[RWK-NEU,EXP-NEU,MET-NEU], [EMP-NEU]] \nThe experimental results make more sense now.[[RWK-POS,EXP-POS], [IMP-POS,EMP-POS]] \nIt would be interesting to see whether the idea can be applied to more recent GAN models and still perform better[[RWK-POS,PDI-POS,MET-POS], [IMP-POS,EMP-POS]] .\nI raised my score to 7.[[EXT-POS], [null]]