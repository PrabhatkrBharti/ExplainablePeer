 "This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling.[[INT-POS], [null]]  The authors propose a reservoir sampling algorithm for drawing samples from the memory.[[MET-NEU], [null]]  Some theoretical guarantees for the efficiency of reservoir sampling are provided.[[MET-NEU], [EMP-NEU]]  The whole algorithm is tested on a toy problem with 3 repeats.[[EXP-NEU], [null]]  The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm.[[MET-NEU], [CMP-POS]] \n\nThe paper is well written and easy to understand.[[OAL-POS], [CLA-POS]]  Typos didn't influence reading.[[OAL-NEU], [CLA-NEU]]  It is a novel setup to consider reservoir sampling for episodic memory.[[PDI-POS], [NOV-POS]]  The theory part focuses on effectiveness of drawing samples from the reservoir.[[MET-NEU], [null]]  Physical meanings of Theorem 1 are not well represented.[[MET-NEG], [EMP-NEG]]  What are the theoretical advantages of using reservoir sampling?[[MET-NEU], [EMP-NEU]]  \n\nFour simple, shallow neural nets are built as query, write, value, and policy networks.[[MET-NEU], [null]]  The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network.[[RWK-NEU], [SUB-NEU,CMP-NEU]]  It is not clear the better performance comes from reservoir sampling or other differences.[[MET-NEU], [EMP-NEU]]  Moreover, the hyperparameters are not optimized on different architectures. It is hard to justify the empirically better performance without hyperparameter tuning.[[MET-NEG], [SUB-NEU,EMP-NEU]]  The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments.[[EXP-NEU], [SUB-NEG,EMP-NEG]]