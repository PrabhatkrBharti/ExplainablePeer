 "This paper introduces a new architecture for end to end neural machine translation.[[INT-NEU], [null]]  Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right.[[EXP-NEU,MET-NEU], [null]]  \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.[[MET-NEU], [null]]   \n\nThe idea is nice.[[PDI-POS], [EMP-POS]]  The proposed approach does not rely on attention based model.[[MET-NEU], [null]]  This opens nice perpectives for better and faster inference.[[MET-POS], [EMP-POS]]  \n\nMy first concern is about the architecture description.[[MET-NEU], [EMP-NEU]]  For instance, the swan part is not really stand alone.[[MET-NEG], [EMP-NEG]]  For reader who does not already know this net, I'm not sure this is really clear.[[MET-NEG], [PNF-NEG]]  Moreover, there is no link between notations used for the swan part and the ones used in the reordering part.[[MET-NEG], [PNF-NEG]]  \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence.[[MET-NEU], [EMP-NEU]]  Maybe you could motivate your choice at this point.[[MET-NEU], [null]]  This is the main contribution of the paper, since swan already exists.[[MET-NEU], [NOV-NEU]] \n\nFinally, the experimental part shows nice improvements [[EXP-POS], [EMP-POS]] but: 1/ you must provide baseline results with a well tuned phrase based mt system;[[RWK-NEU,EXP-NEU], [SUB-NEU,CMP-NEU]]  2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. "[[DAT-NEG], [SUB-NEG]]