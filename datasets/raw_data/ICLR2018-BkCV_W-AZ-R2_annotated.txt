 "This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL.[[INT-NEU], [null]]  Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better.[[MET-POS], [EMP-POS]]   The results is interesting and novel.[[RES-POS], [NOV-POS,EMP-POS]]   This paper should be accepted.[[OAL-POS], [REC-POS]]  \n\nThe presentation of the paper can be improved a bit.[[OAL-NEG], [PNF-NEG]]   Much of the notation introduced in section 3.1 is not used later on.[[CNT], [PNF-NEG]]   There seems to be a bit of a disconnect before and after section 3.3.[[CNT], [PNF-NEG]]  The algorithm in deep RL could be explained a bit better.[[MET-NEG], [EMP-NEG]] \n\nThere are some papers that could be connected.[[RWK-NEG,BIB-NEG], [SUB-NEG]]  Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments.[[RWK-NEU], [CMP-NEU]] \n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals.[[MET-NEU], [SUB-NEU]] \n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train.[[EXP-NEG], [EMP-NEG]]  An experiment illustrating this effect could be illuminating.[[EXP-NEU], [SUB-NEU]] \n\nCan the proposed approach help when we have recurrent value functions?[[MET-NEU], [EMP-NEU]]  Since recurrence does not guarantee that all information needed is captured.[[MET-NEG], [EMP-NEG]] \n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al.[[BIB-NEU], [null]] \n\nPotential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?"[[CNT], [CLA-NEG]]