 "The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states.[[INT-NEU], [null]]  The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes.[[PDI-NEU], [null]]  The proposed technique is validated both empirically and theoretically.[[MET-NEU], [null]]  \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN.[[EXP-POS], [EMP-POS]]  Nonetheless, there are some criticisms than can be made of both the method and the evaluations:\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning.[[EXP-NEG,MET-NEG], [EMP-NEG]]  Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally.[[EXP-NEG,MET-NEG], [EMP-NEG]]  There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable.[[MET-NEG], [EMP-NEG]]  No empirical results on the effect of the parameter are given.[[RES-NEG], [EMP-NEG]] \n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning.[[EXP-POS,MET-POS,RES-POS], [EMP-POS]] The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies.[[PDI-NEU], [null]]   This problem does not seem to be really solved by this method.[[MET-NEG], [EMP-NEG]]  Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier.[[EXP-NEG,MET-NEG], [EMP-NEG]]  While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties.[[EXP-NEG,MET-NEG], [EMP-NEG]]  As such the method wouldn\u2019t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state.[[EXP-NEG], [EMP-NEG]]  It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited.[[EXP-NEG], [SUB-NEG,EMP-NEG]]  \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method.[[MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU]]  The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]).[[MET-NEG], [SUB-NEG,CMP-NEG]] \n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  \u2018if the fear model reaches arbitrarily high accuracy\u2019 or \u2018if the probability is negligible\u2019.[[MET-NEG], [CLA-NEG]] \n\nIt is wasn\u2019t clear to me that the properties described in Theorem 1 actually hold.[[MET-NEG], [CLA-NEG]]  The motivation in the appendix is very informal and no clear derivation is provided.[[OAL-NEG], [SUB-NEG]]  The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states.[[MET-NEG], [EMP-NEG]]  However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states.[[MET-NEU], [EMP-NEU]]   It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty.[[MET-NEU], [EMP-NEU]]  It is therefore not clear to me that any claims can be made about its performance without additional assumptions.[[MET-NEG], [EMP-NEG]] \n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state.[[MET-NEU], [null]]  Any optimal policy would therefore need to spend some time e in the danger state, on average.[[MET-NEU], [null]]  A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards.[[MET-NEG], [EMP-NEG]]  E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax.[[MET-NEU], [EMP-NEU]]  By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin).[[MET-NEG,RES-NEU], [EMP-NEG]]   This seems to contradict the theorem. [[MET-NEG], [EMP-NEG]] It wasn\u2019t clear what assumptions the authors make to exclude situations like this.[[MET-NEG], [EMP-NEG]] \n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \"Improved deep reinforcement learning for robotics through distribution-based experience retention,\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952.\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835."[[BIB-NEU], [null]]