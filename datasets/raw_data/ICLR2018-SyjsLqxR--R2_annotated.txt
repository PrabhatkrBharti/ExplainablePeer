 "This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples.[[INT-NEU,PDI-NEU], [null]]  It also analyses how adversarial training affects detection.[[PDI-NEU], [null]] \n\nThe robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem.[[RES-POS], [EMP-POS]]  The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect.[[MET-POS,RES-NEU], [SUB-NEU,EMP-POS]]  It's also unclear from which epoch the adversarial examples were generated from in figure 5.[[TNF-NEG], [PNF-NEG]]  Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios.[[MET-NEU,RES-NEU], [SUB-NEU]] \n\nThe results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surprising.[[RWK-NEG,RES-NEG], [EMP-NEG]]  There is also no analysis of what happens for adversarial examples for the detector.[[ANA-NEU], [EMP-NEU]] \n\nAlso, it's not clear from section 3.1 what inputs are used to generate the adversarial examples.[[MET-NEG], [EMP-NEG]]  Are they a random sample across the whole dataset?\n\nFinally, the paper spends significant time on describing MaxMin and MinMax and the graphical visualizations but the paper fails to show these graphical profiles for real models."[[MET-NEG], [EMP-NEG]]