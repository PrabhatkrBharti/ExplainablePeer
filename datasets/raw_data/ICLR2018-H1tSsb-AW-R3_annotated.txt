 "The paper proposes a variance reduction technique for policy gradient methods.[[INT-NEU], [null]]  The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines.[[INT-NEU], [null]] \n\n\nThe writing and organization of the paper is very well done. [[OAL-POS], [CLA-POS]] It is easy to follow, and succinct while being comprehensive.[[OAL-POS], [CLA-POS]]  The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively. [[RWK-POS,MET-POS], [EMP-POS]] There is only one mostly minor issues with the algorithm development and the experiments need to be more polished. [[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0.[[MET-NEU], [EMP-NEU]]  This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions).[[MET-NEU], [EMP-NEU]]  However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0.[[MET-NEU], [EMP-NEU]]  Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms.[[PDI-POS], [EMP-POS]]  The paper would be a much stronger contribution, if the experiments could be improved.[[EXP-NEU], [EMP-NEU]]  \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc.[[EXP-NEU], [EMP-NEU]] \n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper.[[RWK-NEU,EXP-NEU,TNF-NEU], [EMP-NEU]]  While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline.[[RWK-NEU,EXP-NEU], [EMP-NEU]]  What was the goal here?[[RWK-NEU,EXP-NEU], [EMP-NEU]] \n- In general, the graphs are difficult to read; fonts should be improved and the graphs polished.[[TNF-NEG], [PNF-NEG]]  \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline?[[RWK-NEU,MET-NEU], [EMP-NEU]] \n- It'd be great if Plot (a) and (b) in Figure 5 are swapped.[[TNF-NEU], [PNF-NEU]] \n\nOverall I think the idea proposed in the paper is beneficial.[[PDI-POS], [null]]  Better discussing the strong theoretical assumption should be incorporated.[[MET-NEU], [EMP-NEU]]  Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner.[[RWK-NEU,EXP-NEU], [EMP-NEU]]  Particularly with some clarity on the experiments, I would be willing to increase the score.[[EXP-NEU], [REC-NEU,EMP-NEU]]  \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained?[[RWK-NEU,MET-NEU], [EMP-NEU]]  This should be explicitly shown, at least in the appendix.[[RWK-NEU], [PNF-NEU]]  \n2. The listed site for videos and additional results is not active.[[DAT-NEU], [PNF-NEG]] \n3. Some typos\n- Section 2 - 1st para - last line: \"These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.\".[[MET-NEG], [CLA-NEG]] \n- Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i}).[[MET-NEG], [CLA-NEG]]  \n- Section 4.2 - \\hat{Q} is just Q in many places.[[MET-NEG], [CLA-NEG]]