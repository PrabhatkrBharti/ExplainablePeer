 "This paper presents methods to reduce the variance of policy gradient using an action dependent baseline.[[INT-NEU], [null]]  Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.[[RWK-NEU,MET-NEU], [EMP-NEU]]  The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free[[RWK-NEU,MET-NEU], [null]] \n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines.[[RWK-NEU,MET-NEU], [null]] \n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case.[[RWK-NEU,MET-NEU], [null]] \n(4) suggests using MC marginalization and also using the \"average\" action to improve computational feasibility[[MET-NEU], [null]] \n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance[[MET-POS], [null]] \n\nThe suggested methods are empirically evaluated on a number of settings.[[EXP-NEU,MET-NEU], [EMP-NEU]]  Overall action-dependent baseline outperform state-only versions.[[MET-POS], [EMP-POS]]  Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate. [[MET-NEG], [EMP-NEG]] Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance.[[MET-NEU], [EMP-NEU]] \n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.[[MET-POS], [EMP-POS]]  In light of such results, one might change the policy space to enforce such structure.[[RES-POS], [IMP-POS]] \n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.[[MET-NEU], [EMP-NEU]]  Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state?[[MET-NEU], [EMP-NEU]] \n- Eqn 11 should use \\simeq.[[MET-NEU], [EMP-NEU]] \n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?[[MET-NEU], [EMP-NEU]]  Consider expanding on that in section 4.5.[[MET-NEU], [null]] \n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).[[EXP-NEU,ANA-NEG], [EMP-NEG]]  How would you train such baselines?[[EXP-NEU], [EMP-NEU]] \n- Figure 4 is impossible to read in print.[[TNF-NEG], [PNF-NEG]]  The fonts are too small for the numbers and the legends.[[OAL-NEG], [PNF-NEG]]