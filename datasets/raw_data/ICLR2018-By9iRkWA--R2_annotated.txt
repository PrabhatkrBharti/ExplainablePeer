 "Summary: The paper introduces \"Phase Conductor\", which consists of two phases, context-question attention phase and context-context (self) attention phase.[[INT-NEU], [null]]  Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector.[[MET-NEU], [null]]  The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention.[[MET-NEU], [EMP-NEU]] \n\n\nNote: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard.[[EXT-NEU], [null]] \n\n\nPros:\n- The model achieves SOTA on SQuAD among published papers.[[MET-NEU], [EMP-NEU]] \n- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel.[[MET-POS], [NOV-POS]]  Visual analysis of the attention map is convincing.[[TNF-POS], [EMP-POS]] \n- The paper is overall well-written and clear.[[OAL-POS], [CLA-POS]] \n\nCons:\n- Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers.[[RWK-NEG,MET-NEU,BIB-NEU], [NOV-NEU,EMP-NEU]]  For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector).[[MET-NEG], [EMP-NEG]] \n- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt.[[RES-NEU,TNF-POS], [EMP-NEG]]  For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer.[[RES-NEU,TNF-NEU], [EMP-NEU]]  While this may be statistically significant, it is a very small gain nonetheless.[[RES-NEU], [IMP-POS]] \n- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art.[[RWK-NEU,RES-NEU], [CMP-POS]]  I think this is a valuable engineering contribution,;[[PDI-POS], [null]]  but I feel that it is not well-suited / sufficient for ICLR audience.[[OAL-NEG], [APR-NEG]]  \n\n\nQuestions:\n- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor?[[DAT-NEU], [null]] \n\n\nErrors:\n- page 2 last para: \"gives an concrete\" -> \"gives a concrete\"\n- page 2 last para: \"matching\" -> \"matched\"\nFigure 1: I think \"passage embedding h\" and \"question embedding v\" boxes should be switched.\n- page 7 3.3 first para: \"evidence fully\" -> \"evidence to be fully\".[[OAL-NEU], [CLA-NEG]] \n\n\n[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015."[[BIB-NEU], [null]]