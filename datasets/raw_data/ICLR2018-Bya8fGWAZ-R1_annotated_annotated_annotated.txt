 "The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training.[[RWK-NEU], [NULL]] \n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way[[MET-NEU], [null]] \n\nThe paper is fairly clear and these extensions are reasonable[[INT-POS], [CLA-POS]] .  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact[[MET-NEG], [IMP-NEG]] .  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.[[RWK-NEU], [CMP-POS]]  So, making improvements to help solve grid-worlds better is not so motivating[[MET-NEG], [IMP-NEG]]   The work on dynamic environments was an interesting step:  it would have been interesting to see how the \"models\" learned for the dynamic environments differed from those for static environments.[[MET-POS,ANA-POS], [CMP-POS]]