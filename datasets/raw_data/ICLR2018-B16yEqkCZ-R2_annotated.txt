 "\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.[[INT-NEU,PDI-NEU], [null]]   The fear model is trained in parallel to predict catastrophic states.[[PDI-NEU], [null]]    Its output is used to penalize the Q learning target.[[PDI-NEU], [null]]  \n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states.[[MET-NEG], [EMP-NEG]]   Because it does not experience it any more.[[MET-NEG], [EMP-NEG]]   Shouldn\u2019t the agent stop learning at some point in time?[[MET-NEU], [EMP-NEU]]   Why does it need to keep collecting good data? [[MET-NEU], [EMP-NEU]]  How about giving more weight to catastrophic data (e.g., replicating it)\[[DAT-NEU,MET-NEU], [EMP-NEU]] n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation?[[MET-NEU], [EMP-NEU]] \n\nWhy not specify catastrophic states with a large negative reward?[[MET-NEU], [EMP-NEU]] \n\nIt seems that catastrophe states need to be experienced at least once.[[MET-NEU], [EMP-NEU]] \nIs that acceptable for the autonomous car hitting a pedestrian?\n"[[MET-NEU], [EMP-NEU]]