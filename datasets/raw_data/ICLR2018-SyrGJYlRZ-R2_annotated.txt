 "The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin).[[INT-NEU,MET-NEU], [null]]  They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications.[[PDI-NEU,MET-NEU], [null]] \n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence.[[MET-NEG], [EMP-NEG]]  There are two major problems with this approach:[[MET-NEG], [EMP-NEG]] \n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition.[[MET-POS], [EMP-POS]]  Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions.[[MET-NEG,RES-NEG], [EMP-NEG]] \n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading.[[MET-NEG], [EMP-NEG]]  This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case.[[MET-NEG], [EMP-NEG]] \nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case.[[MET-NEU], [EMP-NEU]] \n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension).[[MET-NEU], [EMP-NEU]] \nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions.[[MET-NEU], [EMP-NEU]] \nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed.[[MET-NEG], [EMP-NEG]] \nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me.[[EXP-NEG], [SUB-NEG,EMP-NEG]] \n\nIn contrast to the theoretical part, the experiments seems very encouraging.[[EXP-POS], [EMP-POS]]  Showing YF to perform very well on several deep learning tasks without (or with very little) tuning.[[EXP-NEU,MET-NEU], [EMP-NEU]]  Again, this seems a bit magical or even too good to be truth.[[EXP-NEG,MET-NEG], [EMP-NEG]]  I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n"[[EXP-NEU,MET-NEU], [EMP-NEU]]