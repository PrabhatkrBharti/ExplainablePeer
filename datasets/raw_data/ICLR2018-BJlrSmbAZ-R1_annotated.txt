 "This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization.[[INT-NEU,PDI-NEU], [null]] \n\nThere is a lot going on in this paper.[[OAL-NEU], [null]]  Although the overall presentation is clean,[[OAL-POS], [PNF-POS]]  there are few key shortfalls (see below).[[OAL-NEG], [null]]  Overall, the reported functionality is nice,[[OAL-POS], [EMP-POS]]  although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive).[[EXP-NEG,RES-NEG], [EMP-NEG]] \n\nSome open questions that I find crucial:\n\n* How exactly is the \u201cstochastic forward-pass\u201d performed that gives rise to the moment estimates?[[MET-NEU], [EMP-NEU]]  This step is the real meat of the paper, yet I struggle to find a concrete definition in the text.[[CNT], [null]]  Is this really just an average over a few recent weights during optimization?[[MET-NEU], [EMP-NEU]]  If so, how is this method specific to batch normalization?[[MET-NEU], [EMP-NEU]]   Maybe I\u2019m showing my own lack of understanding here, but it\u2019s worrying that the actual sampling technique is not explained anywhere.[[MET-NEG], [EMP-NEG]]   This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here?[[MET-NEU], [EMP-NEU]]   In Bayesian Dropout, there is an explicit variational objective.[[MET-NEU], [null]]   Here, this is replaced by an implicit regularizer.[[MET-NEU], [null]]    The argument in Section 3.3 seems rather weak to me.[[MET-NEG], [EMP-NEG]]    To paraphrase it: If the prior vanishes, so does the regularizer. Fine.[[MET-NEG], [EMP-NEG]]  But what's the regularizer that's vanishing?[[MET-NEU], [null]]  The sentence that \"the influence of the prior diminishes as the size of the training data increases\" is debatable for something as over-parametrized as a DNN.[[DAT-NEU,EXP-NEU], [EMP-NEU]]  I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior.[[EXP-NEU], [null]] \n\n* I\u2019m confused about the statements made about the \u201cconstant uncertainty\u201d baseline.[[RWK-NEG], [null]]  First off, how is this (constant) width of the predictive region chosen?[[MET-NEU], [EMP-NEU]]  Did I miss this, or is it not explained anywhere?[[MET-NEU], [EMP-NEU]]  Unless I misunderstand the definition of CRPS and PLL, that width should matter, no?[[MET-NEU], [EMP-NEU]]  Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is \u201cquite a reasonable baseline\u201d.[[RWK-NEU], [null]]  That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?).[[MET-NEG], [EMP-NEG]]  \nOn a related point: What would Figure 2 look like for the constand uncertainty setting?[[TNF-NEU], [PNF-NEU]]  Just a horizontal line in blue and red?[[TNF-NEU], [PNF-NEU]]  But at which level?[[TNF-NEU], [PNF-NEU]] \n\nI like this paper.[[OAL-POS], [null]]  It is presented well (modulo the above problems), and it makes some strong points.[[OAL-POS], [PNF-POS]]  But I\u2019m worried about the empirical evaluation, and the omission of crucial algorithmic details.[[MET-NEG], [EMP-NEG]]  They may hide serious problems."[[PDI-NEG], [EMP-NEG]]