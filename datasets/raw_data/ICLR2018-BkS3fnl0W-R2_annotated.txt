 "The idea of the paper is to use a GAN-like training to learn a novelty detection approach.[[PDI-NEU], [null]]  In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution.[[MET-POS], [null]]  The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers[[MET-NEU], [null]] . To establish such a behavior, the authors propose early stopping as well as other heuristics.[[MET-NEU], [null]]  \n\nI like the idea of the paper,;[[PDI-POS], [null]]  however, this paper needs a revision in various aspects, which I simply list in the following:;[[OAL-NEU], [null]] \n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \"inlier accepted fraction\" is not well motivated in my opinion.[[RWK-NEG], [CMP-NEG]]  This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data.[[DAT-NEU,MET-NEG], [EMP-NEU]]  The latter is important for the GAN-like training.[[RWK-NEU], [CMP-NEU]] \n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison. [[EXP-NEG], [SUB-NEG,CMP-NEU]] \n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5)[[MET-NEU], [EMP-NEU]] \n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators)[[MET-NEG,ANA-NEG], [EMP-NEG]] \n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?)[[MET-NEU], [EMP-NEU]] \n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ())[[BIB-NEU], [PNF-NEU]] \n* The term \"AI-related task\" sounds a bit too broad[[CNT], [PNF-NEU]] \n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection.[[MET-NEU], [PNF-NEU]] \n* Where is Table 1?[[TNF-NEU], [PNF-NEU]] \n* There are quite a lot of typos.[[OAL-NEG], [CLA-NEG]] \n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating.[[OAL-NEU], [REC-NEU]]  The clarity of the paper has improved;[[OAL-POS], [CLA-POS]]  but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)"[[RWK-NEG,DAT-NEU,EXP-NEG], [SUB-NEG]]