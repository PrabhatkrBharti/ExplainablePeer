 "This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations.[[INT-NEU,RWK-NEU], [null]] \n\nIt is a well-written paper,[[OAL-POS], [CLA-POS]]  however, I am not very convinced by its motivation, the proposed model and the experimental results.[[PDI-NEG,EXP-NEG,MET-NEG,RES-NEG], [EMP-NEG]]  \n\nFirst of all, the improvement is rather limited.[[RES-NEG], [null]]  It is only 0.4 improvement overall on the RACE dataset;[[DAT-NEG,RES-NEG], [SUB-NEG]]  although it outperforms GAR on 7 out of 13 categories;[[MET-POS], [EMP-POS]]  but why is it worse on the other 6 categories?[[DAT-NEG,MET-NEG], [EMP-NEG]]  I don\u2019t see any convincing explanations here.[[ANA-NEG], [EMP-NEG]] \n\nSecondly, in terms of the development of reading comprehension models, I don\u2019t see why we need to care about eliminating the irrelevant options.[[MET-NEG], [EMP-NEG]]  It is hard to generalize to any other RC/QA tasks.[[RWK-NEG], [CMP-NEG]]  If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to. [[RWK-NEG], [CMP-NEG]] The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option.[[MET-NEG], [SUB-NEG,EMP-NEG]]  Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined.[[EXP-NEU], [CNT]]  \n\nSome visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.\n"[[ANA-NEG], [SUB-NEU]]