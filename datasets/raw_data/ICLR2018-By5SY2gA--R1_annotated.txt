 "This paper proposed to use affect lexica to improve word embeddings.[[INT-NEU], [null]]  They extended the training objective functions of Word2vec and Glove with the affect information.[[MET-NEU], [null]]  The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis. [[MET-NEU], [null]] Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove.[[RWK-NEU,EXP-NEU,RES-NEU], [CMP-POS]]  In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups.[[MET-POS,RES-POS,OAL-POS], [EMP-POS]] \n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new.[[PDI-POS], [NOV-NEG]]  Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016.[[RWK-NEU,PDI-NEU,BIB-NEU], [NOV-NEG]] \n-\tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best.[[MET-NEG], [EMP-NEG]]  Do the authors have any intuition why?[[MET-NEU], [null]] \n-\tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me.[[EXP-NEG,TNF-NEG], [EMP-NEG]]   I would like to see to the top k nearest neighbors of each of those words.\n"[[EXP-NEU], [EMP-NEU]]