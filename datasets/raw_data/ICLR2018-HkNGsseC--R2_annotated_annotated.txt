 "The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields.[[RWK-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU]]  To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG]]  The grid tensors are matricized and the ranks of the resultant matrices are compared. [[RWK-NEU,MET-NEU,ANA-NEU], [CMP-NEU]] The paper obtains a lower bound on the rank of the resultant grid tensors[[RWK-NEG,MET-NEG], [EMP-NEG]] , and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs.[[RWK-NEG,EXP-NEG,MET-NEG,ANA-NEG], [CMP-NEG,EMP-NEG]]  Assuming that the result carries over to ConvNets, I find this result to be very interesting.[[RWK-POS,RES-POS], [IMP-POS]]   While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same[[RWK-NEG,ANA-NEG], [EMP-NEG]] . This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts. "[[RWK-POS,MET-POS,ANA-POS], [IMP-POS,CMP-POS]]