 "The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \"standard\" form and then into their correct morphological form,[[INT-NEU,PDI-NEU], [null]] \n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,[[MET-POS], [EMP-POS]] \n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task,[[RES-POS], [EMP-POS]]  and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.[[RES-POS], [EMP-POS]]  \n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences.[[RES-POS,ANA-POS], [EMP-POS]]   \n\nAs an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance.[[TNF-NEG], [PNF-POS]]   \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models."[[MET-NEU], [IMP-NEU]]