 "The authors propose a new network architecture for RL that contains some relevant inductive biases about planning.[[INT-POS], [null]]  This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task.[[RWK-NEU,MET-POS], [EMP-POS]]  The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end).[[RWK-NEU,MET-NEU], [CMP-NEU]]  This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes.[[EXP-NEU,MET-NEU], [null]]  The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax.[[MET-NEU], [null]] \n\nI thought the paper was clear and well-motivated.[[PDI-POS], [CLA-POS]]  The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility.[[MET-POS], [EMP-POS]]  \n\nExperimental results seem promising but I wasn\u2019t fully convinced of its conclusions.[[EXP-POS,RES-POS,ANA-NEU], [EMP-POS]]  In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn\u2019t clear to me that this is the right baseline.[[RWK-NEU], [CMP-NEU]]  Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top.[[MET-NEU], [EMP-NEU]]  Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity?[[MET-NEU], [EMP-NEU]]  Same comment in Atari, but there it\u2019s not really obvious that the proposed architecture is helping.[[RWK-NEU,MET-NEU], [EMP-NEU]]  Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc.[[RWK-NEU,MET-NEU], [EMP-NEU]] \n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it\u2019s not clear from the text and experiments whether it actually was necessary.[[EXP-NEU,ANA-POS], [EMP-NEG]]  Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?[[MET-NEU], [EMP-NEU]]   \n\nDespite some shortcomings in the result section, I believe this is good work and worth communicating as is."[[RES-NEG], [REC-POS]]