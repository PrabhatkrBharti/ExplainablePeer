 "The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks.[[INT-POS,PDI-POS], [null]]  The paper is an extension of Kawaguchi'16.[[RWK-NEU], [NOV-NEU]]  It also provides some sufficient conditions for the non-linear cases.[[PDI-POS], [CNT]]  \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16.[[RWK-NEG,MET-NEG], [SUB-NEG,CMP-NEG]]  I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it.[[MET-NEU,OAL-NEU], [SUB-NEU]]  As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique.[[RWK-NEG,MET-NEG], [CMP-NEG]]  Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum.[[MET-NEU,RES-NEU,FWK-NEU], [SUB-NEU,IMP-NEU,EMP-NEU]]  This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points. "[[OAL-NEU], [CNT]]