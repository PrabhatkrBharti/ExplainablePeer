 "- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations.[[INT-NEU], [null]]  They show that their idea is effective in reducing private information leakage,[[PDI-NEU], [null]]  but this idea alone might not be signifcant enough as a contribution.[[PDI-NEG], [IMP-NEG]]  \n\n- The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction.[[EXT-NEU], [NOV-NEG]]  Indeed judging from the large variance in the accuracy of predicting S in Table 1a-c for single adversaries, I suspect one of the main advantage of the current MARS method comes from variance reduction.[[MET-NEG], [EMP-NEG]]  The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks.[[MET-NEU], [null]]   \n\n- The definition of S, the private information set, is not clear. There is no statement about it in the experiments section, and I assume S is the subject identity.[[DAT-NEG,EXP-NEU], [EMP-NEG]]  But this makes the train-test split described in 4.1 rather odd, since there is no overlap of subjects in the train-test split. We need clarifications on these experimental details.[[EXP-NEG], [EMP-NEG]]  \n\n- Judging from Figure 2 and Table 1, all the methods tested are not effective in hiding the private information S in the learned representation.[[MET-NEG,TNF-NEU], [EMP-NEG]]  Even though the proposed method works better, the prediction accuracies of S are still high.[[MET-NEU], [null]]