 "Summary:\n\nThis paper proposes generative models for point clouds.[[INT-NEU], [null]]  First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.).[[RWK-NEU,PDI-NEU,MET-NEU], [CMP-NEU]]  Then, they train generative models over the auto-encoder's latent space, both using a \"latent-space GAN\" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model.[[MET-NEU], [null]]  To generate point clouds, they sample a latent code and pass it to the decoder.[[MET-NEU], [null]]  They also introduce a \"raw point cloud GAN\" (r-GAN) that, instead of generating a latent code, directly produces a point cloud.\[[MET-NEU], [null]] n\nThey evaluate the methods on several metrics.[[MET-NEU], [null]]  First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset.[[DAT-NEU,MET-NEU,RES-NEU], [null]]  Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance.[[MET-NEU,ANA-NEU], [CNT]]  Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled.[[MET-POS], [EMP-POS]] \n\nPros:\n\n- It is interesting that the latent space models are most successful, including the relatively simple GMM-based model.[[MET-POS], [CMP-POS]]  Is there a reason that these models have not been as successful in other domains?\[[MET-NEU], [IMP-NEU]] n\n- The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs.[[FWK-POS], [IMP-POS,CMP-NEU]]  Due to the simplicity of the method, this paper could be a useful baseline for future work.[[MET-POS,FWK-POS], [IMP-POS,EMP-POS]] \n\n- The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper.[[RES-POS,FWK-POS], [IMP-POS]] \n\nCons:\n\n- How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ?[[RWK-NEG,MET-NEG], [CMP-NEG]] \n\n- The paper simultaneously proposes methods for generating point clouds, and for evaluating them.[[MET-NEG], [EMP-NEG]]  The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs.[[MET-NEU], [EMP-NEU]]  Although the performance on automated metrics is encouraging,[[RES-POS], [IMP-POS]]  it is hard to conclude much about under what circumstances one representation or model is better than another.[[MET-NEU], [CMP-NEG]] \n\n- The technical approach is not particularly novel.[[MET-NEG], [NOV-NEG]]  The auto-encoder performs fairly well,[[MET-POS], [EMP-POS]]  but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance.[[MET-NEU,RES-NEG], [EMP-NEG]]  The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM).[[MET-NEU], [EMP-NEU]] \n\n- While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem.\[[RWK-NEU,MET-NEU], [CMP-NEG]] n\n- The paper could possibly be clearer by integrating more of the \"background\" section into later sections.[[OAL-NEU], [PNF-NEU]]  Some of the GAN figures could also benefit from having captions.[[TNF-NEU], [PNF-NEU]] \n\nOverall, I think that this paper could serve as a useful baseline for generating point clouds,[[FWK-POS], [IMP-POS]]  but I am not sure that the contribution is significant enough for acceptance.\n"[[OAL-NEU], [IMP-NEG,REC-NEU]]