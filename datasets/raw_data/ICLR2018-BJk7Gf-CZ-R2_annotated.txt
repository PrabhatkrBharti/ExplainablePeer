 "Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks.[[INT-NEU,RES-NEU], [null]]  In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion.[[PDI-NEU], [null]]  This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are[[RWK-NEU,OAL-NEU], [null]] .\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach.[[RWK-NEU,MET-NEU,EXT-NEU], [null]] \n\nQuality:\nThe quality is very good.[[OAL-POS], [EMP-POS]]  The paper is technically correct and nontrivial.[[OAL-POS], [EMP-POS]]  All proofs are provided and easy to follow.[[OAL-POS], [PNF-POS]] \n\nClarity:\nThe paper is very clear.[[OAL-POS], [CLA-POS]]  Related work is clearly cited, and the novelty of the paper well explained.[[RWK-POS,BIB-POS,OAL-POS], [CLA-POS,NOV-POS]]  The technical proofs of the paper are in appendices, making the main text very smooth.[[OAL-POS], [PNF-POS]] \n\nOriginality:\nThe originality is weak.[[OAL-NEG], [NOV-NEG]]  It extends a series of recent papers correctly cited.[[BIB-POS], [PNF-POS]]  There is some originality in the proof which differs from recent related papers.[[RWK-POS], [CMP-POS]] \n\nSignificance:\nThe result is not completely surprising,[[RES-NEG], [IMP-NEG]]  but it is significant given the lack of theory and understanding of deep learning.[[RES-NEU], [IMP-NEU]]  Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people."[[RES-NEU], [IMP-NEU]]