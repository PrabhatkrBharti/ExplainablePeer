 "The paper studies the expressive power provided by \"overlap\" in convolution layers of DNNs. [[RWK-NEU,MET-NEU], [EMP-NEU]]  Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling.[[RWK-NEU,MET-NEU], [EMP-NEU]]   Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\[[RWK-NEU,MET-NEU,ANA-NEU], [IMP-NEU,EMP-NEU]] n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width). [[RWK-NEU,RES-POS,ANA-POS], [EMP-POS]]  The key part of the proof is showing a lower bound on the rank for networks with overlap[[RWK-NEG,RES-NEG], [IMP-NEG]] .  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \"generic\" tensor.[[RWK-NEU,EXP-NEU,ANA-NEU], [null]] \n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs, [[RWK-NEU,RES-POS], [EMP-NEU]] which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\[[RWK-NEU,MET-NEU], [CMP-NEU]] n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth)[[RWK-NEG,EXP-NEG,ANA-NEG], [CLA-NEG]] .\n3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank.[[MET-NEG], [EMP-NEG]] \n\nThat said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets[[RWK-NEG,EXP-NEG,RES-NEG,ANA-NEG], [IMP-NEG,EMP-NEG]] , as it gives some intuition about how much overlap might 'suffice'.  \n\nI recommend weak accept.[[OAL-NEG], [REC-NEG]]