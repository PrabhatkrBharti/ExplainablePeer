 "The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm.[[RWK-NEU,PDI-NEU,MET-NEU], [EMP-NEU]]   In particular, the authors propose a new framework to \"sharpen\" the posterior.[[PDI-NEU,MET-NEU], [EMP-NEU]] \n\nIn particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation.[[PDI-POS,MET-POS], [EMP-POS]]   In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why.[[EXP-POS,RES-POS,OAL-NEU], [CLA-NEG,EMP-POS]]   In particular, how does the variational posterior change as a result of the hierarchical prior? [[PDI-NEU], [EMP-NEU]]  It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening. [[PDI-NEU,EXP-NEU,MET-NEU], [CLA-NEG,EMP-NEU]]  Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it. [[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU]]  As a final point, the authors state, \"as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L.[[PDI-POS,EXP-POS,MET-POS], [EMP-POS]] \"  Do the authors mean that the negative log-likelihood will be improved in this case?[[PDI-NEU,EXP-NEU], [EMP-NEU]]   Or the actual optimization?[[PDI-NEU,EXP-NEU], [EMP-NEU]]   Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization.[[PDI-NEU,MET-NEU,OAL-NEG], [CLA-NEG,EMP-NEU]] \n\nThe new evaluation metric proposed in Section 6.1.1 is confusing, and I do not understand what the metric is trying to capture. [[PDI-NEG,EXP-NEG,OAL-NEG], [CLA-NEG]]  This needs significantly more detail and explanation.[[PDI-NEG], [SUB-NEG,EMP-NEG]]   Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs.[[DAT-NEU,EXP-NEU,MET-NEU,OAL-NEG], [CLA-NEG]]   It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them.[[PDI-NEU,OAL-NEG], [CLA-NEG]]   The authors should not compare to the uniform prior as a baseline for entropy.[[PDI-NEG], [CMP-NEG]]   It's much more revealing to compare it to the empirical likelihoods of the words.[[PDI-NEU], [EMP-NEU]]