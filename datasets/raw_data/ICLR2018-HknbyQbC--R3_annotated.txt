 "The paper proposes a way of generating adversarial examples that fool classification systems.[[RWK-NEG], [IMP-NEG]] \nThey formulate it for a blackbox and a semi-blackbox setting (semi being, needed for training their own network, but not to generate new samples).\[[PDI-NEG,MET-NEU,RES-NEG], [IMP-NEG,EMP-NEU]] n\nThe model is a residual gan formulation, where the generator generates an image mask M, and (Input + M) is the adversarial example.\[[RWK-NEU,MET-NEU,RES-NEG], [IMP-NEG,EMP-NEU]] nThe paper is generally easy to understand and clear in their results.[[INT-POS,RES-POS], [CLA-POS,IMP-POS]] \nI am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist[[RWK-NEU,MET-NEU], [EMP-NEU]] . From this paper's literature survey, they dont exist.[[INT-NEG,RWK-NEG], [IMP-NEG]]  \nSo this paper is innovative in two parts:\n- it applies GANs to adversarial example generation\[[INT-NEU,MET-NEU], [EMP-NEU]] n- the method is a simple feed-forward network, so it is very fast to compute\[[RWK-NEU,MET-POS], [EMP-POS]] n\nThe experiments are pretty robust, and they show that their method is better than the proposed baselines[[PDI-POS,EXP-POS,MET-POS], [IMP-POS,EMP-POS]] .\nI am not sure if these are complete baselines or if the baselines need to cover other methods (again, not fully familiar with all literature here).\n"[[EXP-NEU,MET-NEU,EXT-NEU], [null]]