 "The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder.[[PDI-NEU], [null]]  Each node is represented with (dx, dy) along with one-hot representation of three different drawing status.[[PDI-NEU], [null]]  A bi-directional LSTM is used to encode latent space in the training stage.[[MET-NEU], [null]]  Auto-regressive VAE is used for decoding.[[EXP-NEU,MET-NEU], [null]]  \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term.[[DAT-NEU,MET-NEU], [null]]  \n\nPros:\n- Good solution to an interesting problem.[[PDI-POS,RES-POS], [EMP-POS]]  \n- Very interesting dataset to be released.[[DAT-POS], [EMP-POS]] \n- Intensive experiments to validate the performance.[[EXP-POS], [EMP-POS]]  \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy).[[DAT-NEU], [EMP-NEU]]  In the data collection stage, how were the points lists generated from pen strokes? [[DAT-NEU], [EMP-NEU]]  Did each points are sampled from same travelling distance or according to the same time interval? [[DAT-NEU], [EMP-NEU]]  Are there any other potential biases brought because the data collection tools?[[DAT-NEU,MET-NEU], [EMP-NEU]] \n- Is log-likelihood a good loss here?[[RES-NEU], [EMP-NEU]]  Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.[[DAT-NEU], [EMP-NEU]]  How do you deal with this case?[[DAT-NEU], [EMP-NEU]] \n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? "[[DAT-NEU,FWK-NEU], [SUB-NEU,IMP-NEU]]