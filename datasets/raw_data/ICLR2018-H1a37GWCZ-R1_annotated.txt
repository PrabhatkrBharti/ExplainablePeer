 "This paper presents simple but useful ideas for improving sentence embedding by drawing from more context. [[PDI-NEU], [null]] The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other \"governing\" sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc.[[MET-NEU], [EMP-NEU]]  The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context.[[MET-NEU], [EMP-NEU]] \n\nHere are the pros of this paper:\n1) Useful contribution in terms of using broader context for embedding a sentence.[[MET-POS], [EMP-POS]] \n2) Novel and simple \"trick\" for generating OOV words by mapping them to \"local\" variables and generating those variables.[[MET-POS], [NOV-POS]] \n3) Outperforms SkipThought in evals[[EXP-NEU], [EMP-NEU]] .\n\nCons:\n1) Coreference eval: No details are provided for how the data was annotated for the coreference task.[[DAT-NEU,MET-NEG], [EMP-NEG]]  This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference.[[EXP-NEU], [EMP-NEU]]  Also, the authors should make this dataset available for replicability.[[DAT-NEU], [REC-NEU]]  Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes.[[DAT-NEU,MET-NEU], [EMP-NEU]]  Please clarify.\n2) It is not clear to me how the model learns to generate specific OOV variables.[[MET-NEU], [EMP-NEU]]  Can the authors clarify how does the decoder learns to generate these words.[[MET-NEU], [EMP-NEU]] \n\nClarifications:\n1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper?[[EXP-NEU], [EMP-NEU]] \n2) What is the exact heuristic in \"Text Styles\" in section 3.1? Should be stated for replicability."[[EXP-NEU], [CLA-NEU]]