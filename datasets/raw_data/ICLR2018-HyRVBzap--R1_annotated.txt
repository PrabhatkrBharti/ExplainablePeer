 "The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar.[[INT-NEU], [null]]  The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks.[[PDI-NEU,DAT-NEU], [null]]  The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks.[[PDI-NEU,EXP-NEU], [null]]  \n\nThe idea proposed is fairly straight-forward.[[PDI-POS], [EMP-POS]]  Despite being a simple approach, the experimental results are quite promising.[[MET-NEU,RES-POS], [EMP-POS]]   The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.[[ANA-POS], [EMP-POS]]   \n\nAs pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings.[[EXP-NEG,RES-NEG], [EMP-NEG]]  Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.?[[MET-NEU], [EMP-NEU]] \n\nTable 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive. [[MET-NEG,TNF-NEG], [EMP-NEG]] Do the authors have any insight on this?[[MET-NEU], [EMP-NEU]] \n2. Arrow in Figure 3 are not quite readable;\[[TNF-NEG], [CLA-NEG]] n3. The paper is over 11 pages.[[OAL-NEG], [SUB-NEG,PNF-NEG]]  The authors might want to consider shrink it down the recommended length. "[[OAL-NEG], [PNF-NEG]]