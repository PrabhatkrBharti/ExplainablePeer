 "This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches.[[INT-NEU,MET-NEU], [null]]  The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence.[[MET-NEU], [null]]  The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.[[DAT-NEU,RES-NEU], [EMP-NEU]]  It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs.[[MET-POS,RES-POS], [CMP-POS,EMP-POS]]  \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction.[[OAL-POS], [IMP-POS]]  On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning.[[EXP-NEG,RES-NEG,ANA-NEG], [EMP-NEG]]  As such, I am borderline on its acceptance.[[OAL-NEG], [REC-NEG]] \n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \"chrisc\") that has significantly higher EM/F1 scores than the proposed model.[[RWK-NEG,MET-NEG], [CMP-NEG]]  Why is this result not compared to in Table 1?[[RES-NEG,TNF-NEG], [CMP-NEG]]  \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?[[RWK-NEU,BIB-NEU], [CMP-NEU]]  This may allow you to avoid truncation altogether.[[MET-NEU], [EMP-NEU]]  Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient.[[MET-NEU], [EMP-NEU]] \n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions?[[EXP-NEU,MET-NEU], [CMP-NEU]] \n- What is the average number of sentences per document?[[CNT], [CNT]]  It's hard to get an idea of how reasonable the chosen truncation thresholds are without this.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased.[[EXP-NEU], [EMP-NEU]]  Did the authors try truncating after more words (e.g., 10k)?[[EXP-NEU,MET-NEU], [EMP-NEU]] \n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works.[[MET-NEU,RES-NEU], [SUB-NEU,EMP-NEU]] \n- \"Krasner\" misspelled multiple times as \"Kramer\""[[CNT], [CLA-NEG]]