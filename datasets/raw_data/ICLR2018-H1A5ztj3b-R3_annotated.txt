 "This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting.[[INT-NEU], [null]]  It tries to provide an explanation for the phenomenon and a procedure to test when it happens.[[MET-NEU], [null]]  However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.[[OAL-NEG], [APR-NEG]] \n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers.[[RWK-NEU], [null]]  The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models.[[EXP-NEU,MET-NEU], [IMP-NEU]]  Also, the authors do not give a conclusive analysis under what condition it may happen.[[ANA-NEG], [null]] \n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.[[MET-NEU], [EMP-NEU]]  I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations."[[RES-NEU,ANA-NEG], [EMP-NEG]]