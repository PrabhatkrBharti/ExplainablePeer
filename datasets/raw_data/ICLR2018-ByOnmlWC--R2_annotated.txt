 "The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \"strong\" network).[[INT-NEU,MET-NEU], [null]]  The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \"crossover\" network is presumably time-consuming.[[RWK-NEU,EXP-POS], [CMP-POS]] \n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment.[[MET-NEU], [CLA-NEU]]  There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \"evolutionary\" computing.[[PDI-NEU,MET-NEU], [EMP-NEU]]  Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning.[[MET-NEG], [EMP-NEG]]  I would perhaps even call this a distillation network rather than a crossover network.[[MET-NEU], [EMP-NEU]]  In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \"expert\" trajectories can be generated in unlimited quantities.[[RWK-NEU,MET-NEU], [EMP-NEU]]