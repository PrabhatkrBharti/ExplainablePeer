 "This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs).[[INT-NEU,PDI-NEU], [null]]  Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.[[RWK-NEU,MET-NEU], [CMP-NEU]]  The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.[[RES-NEU,ANA-NEU], [EMP-NEU]]  2) Uniform convergence of the empirical risk to population risk.[[RES-NEU,ANA-NEU], [EMP-NEU]]  3) Generalization bound based on stability.[[RES-NEU,ANA-NEU], [EMP-NEU]]  The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations.[[PDI-NEU,MET-NEU], [null]] \n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points.[[RWK-NEU], [CNT]]  Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW.[[RES-NEU], [EMP-NEU]]  Should the risk bound only depends on the dimensions of the matrix W?[[MET-NEU], [EMP-NEU]] \n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations.[[RWK-NEG,RES-NEU], [CMP-NEG]]  Thus, the authors need to refine BM's result for comparison."[[RWK-NEU,RES-NEU], [CMP-NEU]]