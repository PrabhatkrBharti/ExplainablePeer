  The experimental results are similar to previously proposed methods.[[EXP-NEU,MET-NEU], [CMP-NEU]]  \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results.[[EXP-POS,MET-POS,RES-POS], [CLA-POS,EMP-POS]]  However, the method is not properly motivated.[[MET-POS], [EMP-POS]]  As far as I can tell, the paper never answers the questions: Why do we need a guide actor?[[MET-NEU], [null]]  What problem does the guide actor solve?[[MET-NEU], [EMP-NEU]]  \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and[[MET-NEU], [EMP-NEU]]  (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.[[MET-NEU], [EMP-NEU]]  Using second order methods is not an end in itself.[[MET-NEU], [EMP-NEU]]  The experimental results show the authors have found a way to use second order methods without making performance *worse*.[[EXP-POS,RES-POS], [EMP-POS]]  Given the high variability of deep RL, they have not convincingly shown it performs better.[[RES-POS], [EMP-POS]] \n\nThe paper does not discuss the computational cost of the method.[[ANA-NEG], [SUB-NEG]]  How does it compare to other methods?[[MET-NEU], [CMP-NEU]]  My worry is that the method is more complicated and slower than existing methods, without significantly improved performance.[[MET-NEG], [CMP-NEG]] \n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n"[[MET-NEU], [EMP-NEU]]