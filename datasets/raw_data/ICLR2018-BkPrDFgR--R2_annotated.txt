 "Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks.[[INT-NEU], [null]] \n- introduces a simple branch-and-bound approach.[[MET-NEU], [null]] \n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one.[[RWK-NEU,EXP-NEU], [CMP-NEU]] \n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference.[[OAL-POS], [APR-POS]] \n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read.[[OAL-POS], [CLA-POS]] \n\nSoundness: As far as I can tell, the work is sound.[[OAL-POS], [EMP-POS]] \n\nNovelty: This is in my opinion the weakest point of the paper.[[OAL-NEU], [NOV-NEU]]  There isn't really much novelty in the work.[[OAL-NEG], [NOV-NEG]]  The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is).[[MET-NEU], [EMP-NEG]]  The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well).[[EXP-POS,RES-NEU], [NOV-POS,CMP-POS]] \n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods. [[EXP-POS,RES-POS], [EMP-POS]] Unfortunately, there isn't much insight to be gained from them.[[EXP-NEU,RES-NEU], [IMP-NEG]]  I couldn't see any emerging trend/useful recommendations (like \"if your problem looks like X, then use algorithm B\").[[MET-NEU], [EMP-NEG]]  This is unfortunately often the case when dealing with combinatorial search/optimization.[[EXT-NEU], [null]]