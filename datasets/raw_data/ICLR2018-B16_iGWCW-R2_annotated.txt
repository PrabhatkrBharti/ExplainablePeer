 "In conventional boosting methods, one puts a weight on each sample.[[EXT-NEU], [null]]  The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right. [[EXT-NEU], [null]]  Thus the learned weak learner at this round will make different mistakes.[[EXT-NEU], [null]] \nThis idea however is difficult to be applied to deep learning with a large amount of data.[[PDI-NEG], [EMP-NEG]]  This paper instead designed a new boosting method which puts large weights on the category with large error in this round.[[PDI-NEU], [null]]    In other words samples in the same category will have the same weight \n\nError bound is derived.[[PDI-NEU], [null]]   Experiments show its usefulness [[EXP-POS], [EMP-POS]] though experiments are limited\n"[[EXP-NEG], [SUB-NEG]]