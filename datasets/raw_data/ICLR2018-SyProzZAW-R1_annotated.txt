 "Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks.[[EXT-NEU], [null]]  \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c.[[MET-NEU], [null]]  \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables.[[MET-NEU], [null]]  The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2).[[MET-NEU], [null]]  \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation).[[RWK-POS,RES-POS], [CMP-POS,EMP-POS]]  \nOverall, I like the paper.[[OAL-POS], [null]] \n\nPros: new and interesting result, theoretically sound.[[EXP-POS,MET-POS,RES-POS], [EMP-POS]]  \nCons: nothing major.[[OAL-NEU], [EMP-NEU]] \nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?[[EXP-NEU,MET-NEU], [EMP-NEU]]  Could you comment on this problem?[[EXP-NEU,MET-NEU], [EMP-NEU]] \n* Is the assumption that \\sigma has Taylor expansion to order d tight?[[EXP-NEU,MET-NEU], [EMP-NEU]]  (That is, are there counter examples for relaxations of this assumption?)[[EXP-NEU,MET-NEU], [EMP-NEU]]  \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs,[[MET-NEG], [EMP-NEG]]  Could you provide some further comments on this?\n\n"[[MET-NEU], [SUB-NEU]]