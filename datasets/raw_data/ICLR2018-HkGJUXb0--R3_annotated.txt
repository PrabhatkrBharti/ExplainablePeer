 "This paper presents a tensor decomposition method called tensor ring (TR) decomposition.[[INT-NEU,MET-NEU], [null]]  The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors.[[PDI-NEU,EXP-NEU,MET-NEU], [null]]  This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1).[[PDI-NEU,EXP-NEU,MET-NEU], [null]] \n\nTo learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm.[[RWK-NEU,MET-NEU], [EMP-NEU]]  To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable.[[RWK-NEU,MET-NEU], [null]] \n\nThe proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results.[[PDI-NEU,MET-NEU,RES-NEU], [EMP-NEU]] \n\nThis is an interesting work. [[RWK-POS], [IMP-POS]] TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations. [[RWK-NEU], [EMP-NEU]] This seems to be a good direction.[[OAL-POS], [IMP-POS]]  The experimental results are somewhat limited but the overall framework looks appealing.[[EXP-NEG,RES-NEG,OAL-POS], [IMP-POS,EMP-NEG]]