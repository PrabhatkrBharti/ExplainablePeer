 "Quality: The work has too many gaps for the reader to fill in.[[OAL-NEG], [CNT]]  The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.[[PDI-NEU,MET-NEU], [null]]  I am not sure how this is achieved in this work.[[MET-NEG], [EMP-NEG]]  The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.[[MET-NEG], [EMP-NEG]]  Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.[[EXP-NEU], [EMP-NEU]]  How can such a limited technique be generalizing?[[MET-NEU], [EMP-NEU]]  There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\[[RES-NEG], [EMP-NEG]] n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.[[MET-NEG], [CLA-NEG,SUB-NEG]]  It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,[[EXP-POS], [EMP-POS]]  although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.[[MET-NEG], [EMP-NEG]]  From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function.[[MET-NEU], [EMP-NEG]]  If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?[[RWK-NEU], [EMP-NEU]]  This is a key point not discussed in the manuscript.[[RWK-NEG], [CMP-NEG]]  \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data.[[MET-NEG], [EMP-NEG]]  And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output?[[MET-NEG], [EMP-NEG]] \n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster.[[MET-POS], [EMP-POS]]  There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work. [[EXT-NEU], [null]] There is no contribution in the GAN / neural network aspect.[[MET-NEG], [EMP-NEG]]  It is also unclear whether the model generalizes.[[OAL-NEG], [EMP-NEG]]  I don\u2019t think this is a good fit for ICLR.[[OAL-NEG], [APR-NEG]] \n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases.[[MET-NEU], [IMP-NEU]]  The work itself, however, falls short of the goal.[[OAL-NEG], [IMP-NEG]]  At best the generator seems to be working but I fear it is overfitting.[[MET-NEG], [EMP-NEG]]  The contribution for ICLR is rather minimal, unfortunately.[[MET-NEG], [APR-NEG]]