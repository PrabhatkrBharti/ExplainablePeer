 "The authors did extensive tuning of the parameters for several recurrent neural architectures[[INT-POS], [null]] . The results are interesting.[[RES-POS], [null]]  However the corpus the authors choose are quite small,[[OAL-NEG], [SUB-NEG]]  the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn[[MET-NEG,ANA-NEU], [EMP-NEG,SUB-NEG]] .\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens.[[DAT-NEU,EXP-NEU], [SUB-NEU]]  This will use significant resources and is much more difficult,[[MET-NEU], [EMP-POS]]  but it's also really valuable, because it's much more close to real world usage of language models.[[MET-POS], [IMP-POS,EMP-POS]]  And less tuning is needed for these larger datasets.[[DAT-NEU], [EMP-NEU]]  \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. "[[EXP-NEU,ANA-NEU], [REC-POS]]