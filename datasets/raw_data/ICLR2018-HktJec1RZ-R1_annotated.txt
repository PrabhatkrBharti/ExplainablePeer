 "The paper introduces a neural translation model that automatically discovers phrases.[[INT-NEU,PDI-NEU], [null]]   This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.[[PDI-POS], [EMP-POS]]  However, the clarity of the paper could be improved.[[OAL-NEG], [CLA-NEG]] \n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others?[[MET-NEU], [EMP-NEU]] \n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?[[EXP-NEU,MET-NEU], [EMP-NEU]]  In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?"[[EXP-NEU,MET-NEU,TNF-NEU], [EMP-NEU]]