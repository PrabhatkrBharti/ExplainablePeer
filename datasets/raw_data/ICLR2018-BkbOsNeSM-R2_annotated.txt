 "This paper proposes a computationally fast method to train neural networks with normalized weights.[[INT-NEU,MET-POS], [EMP-POS]]  Experiments demonstrate that their method is promising compared to the competitor \u201cNormProp\u201d which explicitly normalizes the weights of neural networks.[[EXP-NEU,MET-NEU], [null]] \n\nPros: \n(1) The paper is easy to follow.[[OAL-POS], [PNF-POS]] \n\n(2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights.[[PDI-POS,TNF-POS], [PNF-POS]] \n\nCons:\n(1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\n(2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp. [[DAT-NEU,EXP-NEU], [null]] It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate.[[EXP-NEU], [SUB-NEU]]  Since the efficiency is one of the main contributions, I suggest authors add this comparison.\n\n[[EXP-NEU], [SUB-NEU]] (3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase.[[MET-NEU,RES-NEU], [null]]  The authors attribute this to the reduction of accumulated rounding error in training process, which is somewhat against the community\u2019s consensus, i.e., float precision is not that important so we can use float32 or even float16 to train/do inference for neural networks.[[MET-NEG], [EMP-NEG]]  I\u2019m curious if this phenomenon still holds if authors use float64 in the experiments.[[MET-NEU,ANA-NEU], [SUB-NEU]] \n\nSome typos:\nFirst line in page 3: \u201cbrining\u201d should be \u201cbringing\u201d\[[CNT], [CLA-NEG]] n\nOverall, I think the current version of the paper is not ready for ICLR conference. [[OAL-NEG], [APR-NEG,REC-NEG]] Authors need more experiments to show their approach\u2019s effectiveness.[[EXP-NEG], [SUB-NEG]]  For example, batching and convolution as mentioned by authors would be more significant. \n"[[EXP-NEG], [SUB-NEG]]