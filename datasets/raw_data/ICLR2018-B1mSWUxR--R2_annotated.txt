 "This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule.[[INT-NEU,PDI-NEU], [null]] \n\nI have a few questions on the motivation and the results.[[MET-NEU,RES-NEU], [null]] \n- In the section \"Open Problems in RAML\", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q.[[EXP-NEU,MET-NEU], [null]]  This is not true.[[EXP-NEG], [EMP-NEG]]  The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q.[[EXP-NEG,MET-NEG], [EMP-NEG]]  It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau).[[MET-NEU], [null]] \n- Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule,[[MET-NEG], [EMP-NEG]]  Can anything be said about this?[[MET-NEG], [EMP-NEG]]  Are the KL divergence small enough to guarantee similar predictive rewards?[[MET-NEG], [EMP-NEG]] \n- In Theorem 2, when does the exponential tail bound assumption hold?[[MET-NEG], [EMP-NEG]] \n- In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML.[[TNF-NEG,MET-NEG], [PNF-NEG]]  Are the differences actually significant?[[MET-NEU], [IMP-NEU]]  Are the differences between SQDML/RAML and ML significant?[[MET-NEU], [IMP-NEU]]  In addition, how should \\tau be chosen in these experiments?\n"[[EXP-NEU,MET-NEU], [EMP-NEG]]