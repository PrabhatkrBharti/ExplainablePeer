 "SUMMARY: This work is about prototype networks for image classification.[[INT-NEU,PDI-NEU], [null]]  The idea is to jointly embed an image and a \"confidence measure\" into a latent space, and to use these embeddings to define prototypes together with confidence estimates.[[PDI-NEU], [null]]  A Gaussian model is used for representing these confidences as covariance matrices.[[PDI-NEU], [null]]  Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype.[[MET-NEU], [null]]  \n\nEVALUATION:\nCLARITY: I found the paper difficult to read.[[OAL-NEG], [CLA-NEG]]  In principle, the idea seems to be clear,[[PDI-POS], [CLA-POS]] but then the description and motivation of the model remains very vague.[[MET-NEG], [CLA-NEG]]  For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)? [[MET-NEU], [EMP-NEU]]  What is the motivation to just average the inverse covariance matrices to compute S_C? [[MET-NEU], [EMP-NEU]]  Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space? [[MET-NEU], [EMP-NEU]]  \nNOVELTY: Honestly, I had difficulties to see which parts of this work could be sufficiently novel. [[OAL-NEG], [NOV-NEG]]  The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting, [[PDI-POS,MET-POS], [EMP-POS]]  but also a time-honored concept. [[MET-NEG], [EMP-NEG]]  The experiments focus very specifically on the omniglot dataset, and it is not entirely clear to me what  should be concluded from the results presented. [[DAT-NEG,EXP-NEG,RES-NEG], [EMP-NEG]]  Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?   \n\n\n" [[RWK-NEU,MET-NEU], [CMP-NEU,EMP-NEU]]