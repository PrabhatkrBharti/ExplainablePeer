 "This paper induces latent dependency syntax in the source side for NMT.[[INT-NEU], [null]]  Experiments are made in En-De and En-Ru.[[EXP-NEU], [null]] \n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).[[RWK-NEU,BIB-NEU], [CMP-NEU]]  In light of this, I see very little novelty in this paper.[[PDI-NEG], [NOV-NEG]]  The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.[[PDI-POS], [NOV-POS]]  Seems thin for a ICLR paper.[[OAL-NEG], [APR-NEG]] \n\nCaption of Fig 1: \"subject/object\" are syntactic functions, not semantic roles.[[TNF-NEU], [CLA-NEG]] \n\nI don't see how the German verb \"orders\" inflects with gender...[[MET-NEU], [EMP-NEU]]  Can you post the gold German sentence?[[MET-NEU], [null]] \n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing.[[MET-NEU], [EMP-NEU]] \n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper.[[RWK-NEU], [NOV-NEG,CMP-NEG]] \n\nWhy is hard attention (sec 3.3) necessary?[[MET-NEU], [EMP-NEU]]  It's not differentiable and requires sampling for training.[[EXP-NEU,MET-NEU], [null]]  This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017).[[RWK-NEU,BIB-NEU], [EMP-NEU]] \n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing.[[EXP-NEG], [EMP-NEG]] \n\nIn table 3, it would be very helpful to display the English source.[[MET-NEU,TNF-NEU], [PNF-POS]] \n\nTable 4 is confusing.[[TNF-NEU], [CLA-NEG]]  The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset?[[DAT-NEG], [null]] \n\nComparison with predicted parses by Spacy are by no means \"gold\" parses...[[RWK-NEU,EXP-NEU], [CMP-NEU]] \n\nMinor comments:\n- Sec 1: \"... optimization techniques like Adam, Attention, ...\" -> Attention is not an optimization technique, but part of a model;[[MET-NEU], [CLA-NEU]] \n- Sec 1: \"abilities not its representation\" -> comma before \"not\".[[MET-NEU], [CLA-NEU]]