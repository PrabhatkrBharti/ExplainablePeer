 "This paper presents a neural architecture for converting natural language queries to SQL statements.[[INT-NEU], [null]]  The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary. [[MET-NEU], [null]] The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization).[[RWK-POS,MET-POS], [CMP-POS,EMP-POS]]  Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model. [[DAT-POS,EXP-POS,MET-POS,RES-POS], [EMP-POS]] Overall, even though the architecture is not very novel,;[[OAL-NEU], [NOV-NEG]]  the paper is well-written and the results are strong;[[RES-POS], [CLA-POS]]  as such, I'd recommend the paper for acceptance.[[OAL-POS], [REC-POS]] \n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?[[MET-NEU], [EMP-NEU]]  Could the output grammar be extended to support joins, for instance? [[MET-NEU], [EMP-NEU]] As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful.[[MET-NEU], [EMP-NEU]]  \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?[[RWK-NEU,EXP-NEU], [EMP-NEU]]  In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?"[[RWK-NEU,MET-NEU], [CMP-NEU]]