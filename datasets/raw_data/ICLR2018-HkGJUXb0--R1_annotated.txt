 "The paper addresses the problem of tensor decomposition which is relevant and interesting.[[PDI-POS], [EMP-POS]]  The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method.[[PDI-NEU,MET-NEU], [EMP-NEU]]  TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices. [[RWK-NEU,EXP-NEU,MET-NEU], [null]] \n\nThe proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order.[[PDI-NEU], [null]]  I think such generalization is interesting but the innovation seems to be very limited.[[RWK-POS], [NOV-NEG]]  \n\nThe paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD.[[PDI-NEU,MET-NEU], [EMP-NEU]]  All of these are well known methods.[[MET-POS,OAL-POS], [IMP-POS,EMP-POS]]  \n\nFinally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images). [[PDI-NEU,EXP-NEU,RES-NEU], [EMP-NEU]] I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods.[[PDI-NEU,EXP-NEU,MET-NEU], [null]] \n\nPlease refer to below for more comments and questions.[[EXT-NEU], [null]] \n\n-- The rating has been updated.[[EXT-NEU], [null]] \n\nPros:\n1. The topic is interesting.[[OAL-POS], [IMP-POS]] \n2. The generalization over TT makes sense.[[RWK-NEU], [null]] \n\nCons:\n1. The writing of the paper could be improved and more clear: the conclusions on inner product and F-norm can be integrated into \"Theorem 5\".[[OAL-NEG], [CLA-NEG]]  And those \"theorems\" in section 4 are just some properties from previous definitions; they are not theorems.[[RWK-NEG], [PNF-NEG,EMP-NEG]]  \n2. The property of TR decomposition is that the tensors can be shifted (circular invariance).[[RWK-NEU], [null]]  This is an interesting property and it seems to be the major strength of TR over TT. [[OAL-POS], [IMP-POS]] I think the paper could be significantly improved by providing more applications of this property in both theory and experiments.[[RWK-NEG,EXP-NEG], [IMP-NEG]] \n3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization.[[RWK-NEU,MET-NEG], [IMP-NEG]]  Any insights or results on the optimization performance vs. the number of latent tensors?[[RWK-NEU,RES-NEU], [null]] \n4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively. [[RWK-NEU,BIB-NEU], [null]] I think this only contains a single round of optimization.[[RWK-NEU], [null]]  Should ALS be applied repeated (each round solves d problems) until convergence?[[RWK-NEU], [EMP-NEU]] \n5. What is the memory consumption for different solvers?[[RWK-NEU], [null]] \n6. SGD also needs to update at least d times for all d latent tensors.[[RWK-NEU,MET-NEG], [EMP-NEG]]  Why is the complexity O(r^3) independent of the parameter d?[[RWK-NEU], [null]] \n7. The ALS is so slow (if looking at the results in section 5.1), which becomes not practical.[[RWK-NEG,RES-NEG,TNF-NEG], [IMP-NEG,PNF-NEG]]  The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers.[[RWK-NEG,RES-NEG], [IMP-NEG,EMP-NEG]] \n8. What does \"iteration\" mean in experimental results such as table 2?[[RWK-NEU,EXP-NEU,RES-NEU,TNF-NEU], [null]]  Different algorithms have different cost for \"each iteration\" so comparing that seems not fair.[[RWK-NEG,MET-NEG], [CLA-NEG,CMP-NEG]]  The results could make more sense by providing total time consumptions and time cost per iteration.[[RES-NEG,OAL-NEG], [IMP-NEG]]  also applies to table 4.[[RWK-NEU,TNF-NEU], [PNF-NEU]] \n9. Why is the \\epsion in table 3 not consistent?[[RWK-NEG,TNF-NEG], [null]]  Why not choose \\epsion = 9e-4 and \\epsilon=2e-15 for tensorization?[[RWK-NEG], [EMP-NEG]] \n10. Also, table 3 could be greatly improved by providing more ablations such as results for (n=16, d=8), (n=4, d=4), etc.[[RWK-NEU,TNF-NEU,OAL-NEU], [EMP-NEU]]  That could help readers to better understand the effect of TR.[[RWK-NEU], [null]] \n11. Section 5.3 could be improved by providing a curve (compression vs. error) instead of just providing a table of sampled operating points.[[RWK-NEG,TNF-NEG], [EMP-NEG]] \n12. The paper mentions the application of image representation but only experiment on 32x32 images.[[RWK-NEG], [SUB-NEG]]  How does the proposed method handle large images?[[PDI-NEU], [null]]  Otherwise, it does not seem to be a practical application.[[RWK-NEG], [IMP-NEG,EMP-NEG]] \n13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images?[[DAT-NEU,EXP-NEU,ANA-NEU,TNF-NEU], [EMP-NEU]] \n\nMinor:\n- Typo: Page 4 Line 7 \"Note that this algorithm use the similar strategy\": use -> uses"[[RWK-NEU], [null]]