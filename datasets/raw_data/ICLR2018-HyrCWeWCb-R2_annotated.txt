 "This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time.[[INT-NEU,MET-NEU], [null]]  The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient.[[MET-NEU], [null]] \n\nThe major weakness of this paper is the unclear presentation.[[OAL-NEG], [PNF-NEG]]  For example, the algorithm is never fully described, though a handful variants are discussed.[[MET-NEG], [SUB-NEG,PNF-NEG]]   How the off-policy version is implemented is missing.[[MET-NEG], [SUB-NEG]]  \n\nIn experiments, why the off-policy version of TRPO is not compared.[[EXP-NEG], [SUB-NEG]]  Comparing the on-policy results, PCL does not show a significant advantage over TRPO.[[RES-NEG], [CMP-NEG]]  Moreover, the curves of TRPO is so unstable, which is a bit uncommon.[[RES-NEG], [EMP-NEG]]  \n\nWhat is the exploration strategy in the experiments?[[EXP-NEU], [EMP-NEG]]  I guess it was softmax probability.[[EXP-NEU], [null]]  However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added.[[MET-NEG], [EMP-NEG]] \n\nAnother issue is the discussion of the entropy regularization in the objective function.[[MET-NEG], [EMP-NEG]]  This regularization, while helping exploration, do changes the original objective.[[MET-NEU], [EMP-NEU]]  When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance.[[RES-NEG], [EMP-NEG]]  Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids."[[EXP-NEG], [SUB-NEU]]