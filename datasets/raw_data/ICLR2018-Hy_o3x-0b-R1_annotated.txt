 "The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs.[[INT-NEU,RWK-NEU,PDI-NEU], [CMP-NEU]]  The numbers in the tables are good but I have several comments on the motivation, originality and experiments.[[EXP-NEU,TNF-POS], [NOV-NEU,EMP-NEU]] \n\nMost parts of the paper provide a detailed review of the literature.[[OAL-NEU], [null]]  However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed.[[RWK-NEG,MET-NEG], [NOV-NEG,EMP-NEG]]  Why should we introduce the convolution stochastic layers?[[MET-NEU], [EMP-NEU]]  Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture?[[EXP-NEU], [EMP-NEU]]   What's the exact challenge of training VAEs addressed by the convolution stochastic layer?[[EXP-NEU], [EMP-NEU]]   Please strengthen the motivation and originality of the paper.[[OAL-NEG], [NOV-NEG]] \n\nThough the results are good,[[RES-POS], [EMP-POS]]  I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?[[MET-NEU,RES-NEU], [EMP-NEU]]   Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images?[[MET-NEU,EXP-NEU,RES-NEU], [SUB-NEU]] \n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \"In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S\u00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1).[[RWK-NEU,RES-NEU,TNF-NEU], [CMP-NEU]]  There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images "there is no stochastic convolutional layer.[[MET-NEG], [CMP-NEG]]    Then is there anything new in FAME on the gray images?[[MET-NEU], [EMP-NEU]]   Furthermore, how could FAME advance the previous state-of-the-art?[[RWK-NEU], [CMP-NEU]]   It seems because of other factors instead of the stochastic convolutional layer.[[MET-NEU], [CNT]]   \n\nThe results on the natural images are not complete.[[RES-NEG,TNF-NEG], [SUB-NEG]]   Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets.[[DAT-NEU,RES-NEG], [SUB-NEU]]  The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table.[[RWK-NEG,DAT-NEG], [CMP-NEG]]  Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange.[[RES-NEG], [EMP-NEG]]  Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer?[[DAT-NEU,MET-NEU,RES-NEU], [EMP-NEG]]  Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset?[[DAT-NEG,MET-NEG,RES-NEG], [EMP-NEG]]  Is it because there is a lot randomness in the stochastic convolutional layer?[[MET-NEU], [EMP-NEU]]  It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results.[[MET-NEU,RES-NEU,ANA-NEU], [SUB-NEU]] \n\nMinor things:\n\nPlease rewrite the sentence \"When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L.[[EXP-NEU], [CLA-NEU,PNF-NEU]]