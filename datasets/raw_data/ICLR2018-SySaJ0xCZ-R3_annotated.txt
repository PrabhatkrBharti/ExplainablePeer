 "This paper proposes a variant of neural architecture search.[[INT-NEU], [null]]   It uses established work on network morphisms as a basis for defining a search space.[[INT-NEU], [null]]   Experiments search for effective CNN architectures for the CIFAR image classification task.[[EXP-NEU], [null]] \n\nPositives:\n\n(1) The approach is straightforward to implement and trains networks in a reasonable amount of time.[[EXP-POS,MET-POS], [EMP-POS]] \n\n(2) An advantage over prior work, this approach integrates architectural evolution with the training procedure. [[RWK-POS,MET-POS], [CMP-POS]]  Networks are incrementally grown; child networks are initialized with learned parameters from their parents.[[EXP-POS,MET-POS], [EMP-POS]]   This eliminates the need to restart training when making an architectural change, and drastically speeds the search.[[EXP-POS,MET-POS], [EMP-POS]] \n\nNegatives:\n\n(1) The state-of-the-art CNN architectures are not mysterious or difficult to find, despite the paper's characterization of them being so.[[RWK-NEG], [CMP-NEG]]   Indeed, ResNet and DenseNet designs are both guided by extremely simple principles: stack a series of convolutional layers, pool occasionally, and use some form of skip-connection throughout.[[MET-NEG], [EMP-NEG]]   The need for architectural search is unclear.[[MET-NEG], [EMP-NEG]] \n\n(2) The proposed search space is boring.[[MET-NEG], [EMP-NEG]]   As described in Section 4, the possibly evolutionary changes are limited to deepening the network, widening the network, and adding a skip connection.[[MET-NEG], [EMP-NEG]]   But these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already available.[[RES-NEU], [CMP-NEU]] \n\n(3) As a consequence of (1) and (2), the result is essentially rigged.[[RES-NEG], [EMP-NEG]]   Since only depth, width, and skip connections are considered, the end network must end up looking like a ResNet or DenseNet, but with some connections pruned.[[MET-NEU], [EMP-NEU]]   There is no way to discover a network outside of the principled design space articulated in point (1) above.[[MET-NEU], [EMP-NEU]]   Indeed, the discovered network diagrams (Figures 4 and 5) fall in this space.[[TNF-NEU], [PNF-NEU]] \n\n(4) Performance is worse than the best hand-designed baselines.[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG]]   One would hope that, even if the search space is limited, the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same space.[[MET-NEU], [EMP-NEU]]   However, the results in Tables 3 and 4 show this not to be the case.[[RES-NEG,TNF-NEG], [EMP-NEG]]   The best human designs outperform the evolved networks.[[EXT-NEU], [null]]   Moreover, the evolved networks are woefully inefficient in terms of parameter count.[[EXT-NEU], [null]] \n\nTogether, these negatives imply the proposed approach is not yet at the point of being useful in practice.[[MET-NEG], [EMP-NEG]]   I think further work is required (perhaps expanding the search space) to resolve the current limitations of automated architecture search.[[MET-NEU,FWK-NEU], [SUB-NEU,IMP-NEU]] \n\nMisc:\n\nTables 3 and 4 would be easier to parse if resources were simply reported in terms of total GPU hours."[[TNF-NEU], [PNF-NEU]]