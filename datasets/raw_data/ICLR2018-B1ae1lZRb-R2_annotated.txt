 "The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network.[[INT-NEU,PDI-NEU], [null]]  Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly.[[PDI-NEU], [CNT]]  The paper shows an interesting result that the distilled low precision network actually performs better than high precision network.[[RWK-POS,MET-POS,RES-POS], [CMP-POS]] \n\nI found the paper interesting[[OAL-POS], [CNT]]  but the contribution seems quite limited.[[OAL-NEG], [SUB-NEG]] \n\nPros:\n1. The paper is well written and easy to read.\n2.[[OAL-POS], [CLA-POS]]  The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally.[[RWK-POS,MET-POS,RES-POS], [SUB-POS,CMP-POS,EMP-POS]] \n\nCons:\n1. The name Apprentice seems a bit confusing with apprenticeship learning.[[OAL-NEG], [PNF-NEG]] \n2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights).[[EXP-NEG,ANA-NEU], [SUB-NEG]] \n3. It is unclear how the proposed method outperforms other methods based on fine-tuning.[[RWK-NEG,MET-NEG], [EMP-NEG]]  It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model."[[RWK-NEG,MET-NEG,FWK-NEU], [CMP-NEG,EMP-NEU]]