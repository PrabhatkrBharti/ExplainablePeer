 "The authors propose a method for performing transfer learning and domain adaptation via a clustering approach.[[MET-NEU], [null]]  The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class.[[INT-NEU], [null]]  Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \"soft labels\" for whether or not pairs of target data belong to the same class.[[DAT-NEU,EXP-NEU], [EMP-POS]]  A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points.[[DAT-NEU,EXP-NEU], [EMP-NEU]]  \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest.[[MET-POS], [NOV-POS]]  The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nThe way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on.[[MET-NEG], [PNF-NEG]]  To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters.[[MET-NEU], [EMP-NEU]]  However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method. [[MET-NEU], [EMP-NEU]] Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model.[[MET-NEU], [EMP-NEU]]  Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear.[[DAT-NEU,MET-NEU], [EMP-NEG]]  Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO.[[MET-NEU], [EMP-NEG]]  Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works.[[MET-NEU], [EMP-NEU]] \n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings.[[EXP-POS], [EMP-POS]]  Strong results in multiple settings indicate that the proposed method is effective and generalizable.[[MET-POS,RES-POS], [EMP-POS]]  Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results.[[RES-NEU,ANA-NEU], [CLA-POS]]  It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works.[[EXP-NEU,MET-NEU], [SUB-NEU]]  I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup.[[MET-NEU], [EMP-NEU]] \n\nAt times, the experimental details are a little unclear.[[EXP-NEG], [EMP-NEG]]  Consistent use of the A, T, and S' dataset abbreviations would help.[[DAT-NEU], [SUB-NEU]]  Also, the results section seems to switch off between calling the method CCN and LCO interchangeably.[[MET-NEU,RES-NEU], [EMP-NEU]]  Finally, a few of the experimental settings differ from their baselines in nontrivial ways.[[RWK-NEU,MET-NEU], [CMP-NEU]]  For the Office experiment, the LCO appears to be trained on ImageNet data.[[DAT-NEU,EXP-NEU], [null]]  While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data.[[DAT-NEU,MET-NEU], [SUB-NEU]]  Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting.[[DAT-NEU,EXP-NEU], [CMP-NEG]]  In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature.[[DAT-NEU,EXP-NEU], [SUB-NEU]] \n\nOverall, the paper seems to have both a novel contribution and strong technical merit.[[MET-POS], [NOV-POS]]  However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained.[[MET-NEG], [PNF-NEG]]  I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission.[[MET-NEU], [PNF-NEU]] \n\n=========================\nUpdate after author rebuttal:\n=========================\nI have read the author's response and have looked at the changes to the manuscript.[[OAL-NEU], [null]]  I am satisfied with the improvements to the paper and have changed my review to 'accept'.[[OAL-POS], [REC-POS]]