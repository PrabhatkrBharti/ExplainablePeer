 "This paper dives deeper into understand reward augmented maximum likelihood training.[[PDI-NEU,EXP-NEU], [null]]  Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule.[[MET-NEG,OAL-NEG], [CLA-NEG,PNF-NEG]]  Please elaborate on this.[[OAL-NEU], [SUB-NEU]] \n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates?[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.[[RWK-NEU], [null]]  However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),[[MET-NEU,ANA-NEU], [SUB-NEU,EMP-NEU]]  Please explain the difference."[[MET-NEU], [CMP-NEU]]