 "  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss.[[INT-NEU,PDI-NEU], [null]] \nThe basic form of SGD selects an example uniformly.[[MET-NEU], [null]]    Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient[[MET-NEU], [null]] .\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015].[[RWK-NEU,MET-NEU], [null]] \n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger.[[RES-NEU], [null]] \n\n  The paper uses LSH structures, computed over the set of examples,[[MET-NEU], [null]] \n to quickly sample examples with large inner products with the current parameter vector \\theta.[[MET-NEU], [null]]    Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity.[[MET-NEU], [null]] \n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings).[[MET-NEU], [null]] \n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited.[[OAL-NEG], [NOV-NEG]]  The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.[[MET-NEG], [EMP-NEG]]   There are theorems,  but they are trivial, straightforward applications of importance sampling.[[MET-NEU], [EMP-NEU]]  \n\n The paper is not well written.[[OAL-NEG], [CLA-NEG]]  The presentation is much more complex that need be.[[OAL-NEG], [PNF-NEG]]  References to classic weighted sampling are[[CNT], [CNT]]  \n\n  The application is limited to certain loss functions for which we can compute LSH structures.[[MET-NEU], [null]]   This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n"[[RES-NEU], [EMP-NEU]]