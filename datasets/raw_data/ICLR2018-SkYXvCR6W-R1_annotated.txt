 "The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row.[[INT-NEU,PDI-NEU], [null]]  The idea is interesting, and overall introduction is clear.[[INT-POS,PDI-POS], [null]] \n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015.[[MET-NEG], [SUB-NEG]]  The results using this particular encoding are not better than any previous work.[[RWK-NEG,RES-NEG], [CMP-NEG,EMP-NEG]] \n\nThe network architecture seems to be arbitrary and unusual.[[MET-NEG], [EMP-NEG]]  It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels.[[MET-NEU], [null]]  The depth of the network is only 5, even with many layers listed in table 5.[[MET-NEG,RES-NEG,TNF-NEG], [EMP-NEG]] \n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features.[[MET-NEG], [EMP-NEG]]  This does not seem to be reasonable.[[MET-NEG], [EMP-NEG]] \n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018."[[OAL-NEG], [APR-NEG,SUB-NEG]]