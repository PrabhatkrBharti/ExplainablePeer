 "The paper proposes a modified approach to RL, where an additional \"episodic memory\" is kept by the agent.[[INT-NEU], [null]]  What this means is that the agent has a reservoir of n \"states\" in which states encountered in the past can be stored.[[DAT-NEU,MET-NEU], [null]]  There are then of course two main questions to address (i) which states should be stored and how [[DAT-NEU,MET-NEU], [null]] (ii) how to make use of the episodic memory when deciding what action to take.[[DAT-NEU,MET-NEU], [null]]  \n\nFor the latter question, the authors propose using a \"query network\" that based on the current state, pulls out one state from the memory according to certain probability distribution.[[MET-NEU], [null]]  This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory.[[MET-NEU], [null]]  Intuitively, one can see why this may be advantageous as one gets some information from the past.[[MET-NEU], [EMP-NEU]]  (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.)[[RWK-NEU,MET-NEU], [EMP-NEU]] \n\nThe first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights.[[MET-POS], [EMP-POS]]  The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose.[[MET-NEU], [null]]  However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past)[[MET-NEU], [null]] . There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.[[MET-NEU], [EMP-NEU]]  \n\nThere is also a toy example created to show that this approach works well compared to the RNN based approaches.[[EXP-NEU], [CMP-NEU]] \n\nPositives:\n\n- An interesting new idea that has potential to be useful in RL[[PDI-POS], [NOV-POS]] \n- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks)[[MET-POS], [EMP-POS]] \n\nNegatives:\n- The math is fudged around quite a bit with approximations that are not always justified[[MET-NEG], [EMP-NEG]] \n- While overall the writing is clear, in some places I feel it could be improved[[OAL-POS], [CLA-NEU]] . I had a very hard time understanding the set-up of the problem in Figure 2.[[TNF-NEG], [PNF-NEG]]  [In general, I also recommend against using figure captions to describe the setup.[[TNF-NEU], [PNF-NEU]] ]\n- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach.[[EXP-NEG], [EMP-NEG]]