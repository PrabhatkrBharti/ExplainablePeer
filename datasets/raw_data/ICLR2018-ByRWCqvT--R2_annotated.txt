 "(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems.[[INT-NEU], [null]]  The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence).[[DAT-NEU,MET-NEU], [null]]  What's similar vs dissimilar is trained with a binary classifier.[[MET-NEU], [null]] \n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning.[[RWK-POS], [SUB-POS]] \n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel.[[MET-POS], [NOV-POS]] \n\n(Cons)\n1. The authors overclaim to be state of the art.[[RWK-NEG], [null]]  For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset.[[DAT-NEG,TNF-NEU], [CMP-NEG]]  I checked the numbers in table 2 and the numbers aren't on par with the recent methods.[[DAT-NEU,TNF-NEG], [CMP-NEG]]  1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16.[[RWK-NEU,BIB-NEU], [CMP-NEU]]  Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2.[[RWK-NEU,EXP-NEU,BIB-NEG], [CMP-NEG]] \n2. There are some typos in the related works section and the inferece procedure isn't clearly explained.[[RWK-NEG], [CLA-NEG]]  Perhaps the authors can clear this up in the text after sec 4.3.[[RWK-NEU], [CLA-NEU]] \n\n(Assessment)\nBorderline. Refer to the Cons section above.[[OAL-NEU], [REC-NEU]]