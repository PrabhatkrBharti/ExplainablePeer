 "This paper adds an interesting twist on top of recent unpaired image translation work.[[INT-POS], [CMP-POS]]  A domain-level translation function is jointly optimized with an instance-level matching objective.[[INT-NEU], [null]]  This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches.[[MET-NEU], [EMP-NEU]]  I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet.[[MET-NEG,RES-NEG], [EMP-NEG]] \n\nMy main criticism is with the experiments and results.[[EXP-NEG,RES-NEG], [null]]  The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets.[[EXP-NEU], [EMP-NEU]]  Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches.[[MET-NEU], [EMP-NEU]]  This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches.[[MET-NEU], [EMP-NEU]]  It feels rather contrived to focus so much on the datasets with exact matches since,;[[DAT-NEU,MET-NEG], [EMP-NEG]]  1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,;[[DAT-NEU], [null]]  2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios);[[DAT-NEU], [null]] 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.[[MET-NEU], [EMP-NEU]]  There is no comparison to any such simple baselines.[[RWK-NEU,MET-NEU], [CMP-NEG]] \n\nI think finding analogies that are not exact matches is much more compelling.[[MET-NEU], [null]]  Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results.[[RES-NEU], [null]]  I\u2019d like to see far more results, and some attempt at a metric.[[RES-NEU], [SUB-NEU]]  One option would be to run user studies where humans judge the quality of the matches.[[EXP-NEU], [EMP-NEU]]  The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results:[[MET-NEG,RES-NEG,TNF-NEU], [EMP-NEG]]  for example, the DiscoGAN results have some artifacts but capture the texture better in row 3.[[EXP-NEU], [EMP-NEU]] \n\nI was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data.[[EXP-NEG,MET-NEG], [EMP-NEG]]  In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful.[[MET-NEG], [EMP-NEU]] \n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches.[[DAT-NEU,MET-NEU], [EMP-NEU]]  For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real.[[DAT-NEU], [SUB-NEU]] \n\nI\u2019d like to see more discussion of why the second stage supervised problem is beneficial.[[MET-NEU], [EMP-NEU]]  Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)?[[MET-NEU], [EMP-NEU]] \n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context.[[INT-NEU], [EMP-NEU]] \n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3.[[RWK-NEU,BIB-NEU], [SUB-NEU]]  Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section.[[MET-NEU], [EMP-NEU]] \n4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when.[[MET-NEU], [EMP-NEG]]  For example, in Eqn. 7, it would be clearer to write out the min over optimization variables.[[EXP-NEU], [EMP-NEG]] \n5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017.[[BIB-NEG], [null]] \n6. Page 7: The following sentence is confusing and should be clarified:[[OAL-NEU], [CLA-NEU]]  \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain.[[RES-NEU,ANA-NEU], [CLA-NEG]] \u201d\n7. Page 7: \u201cThis shows that a good initialization is important for this task.[[ANA-NEU], [CLA-NEG]] \u201d \u2014 Isn\u2019t this more than initialization?[[ANA-NEU], [CLA-NEU]]  Rather, removing the distributional and cycle constraints changes the overall objective being optimized.[[ANA-NEU], [CLA-NEG]] \n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function?[[TNF-NEU], [EMP-NEU]] \n9. Throughout the paper, some citations are missing enclosing parentheses.[[BIB-NEG], [PNF-NEG]]