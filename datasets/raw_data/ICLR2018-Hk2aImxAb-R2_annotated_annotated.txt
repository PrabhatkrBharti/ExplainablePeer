 "This paper presents a method for image classification given test-time computational budgeting constraints.[[INT-NEU], [null]]   Two problems are considered:  \"any-time\" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.[[RWK-NEU,PDI-NEU], [EMP-NEU]]   A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.[[RWK-NEU,MET-NEU], [EMP-NEU]]   In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.[[RWK-NEU,PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU]]   Evaluations are performed on ImageNet and CIFAR-100.[[ANA-NEU], [null]] \n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?[[RWK-NEU,PDI-NEG], [EMP-NEG]]  Also on p.6 I'm not entirely clear on how the \"network reduction\" is performed ---[[RWK-NEG], [CLA-NEG]]  it looks like finer scales are progressively dropped in successive blocks,[[EXT-NEU], [null]]  but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \"lazy evaluation\").[[PDI-NEG,EXP-NEG,MET-NEU,ANA-NEG], [EMP-NEG]]   A picture would help here, showing where the depth-layers are divided between blocks.\[[RWK-NEG], [EMP-NEG]] n\nI was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result: [[PDI-NEG,RES-NEU,ANA-NEU], [CLA-NEG,EMP-NEG]]  It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.[[RWK-NEU,MET-NEU,RES-NEU,ANA-NEU], [EMP-NEU]]   So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).[[RWK-NEG], [null]]   This is fine, but could perhaps be pointed out if that is indeed the case.[[RWK-NEU], [null]] \n\nOverall, this seems like a natural and effective approach, and achieves good results.[[RES-POS,OAL-NEU], [IMP-POS]]