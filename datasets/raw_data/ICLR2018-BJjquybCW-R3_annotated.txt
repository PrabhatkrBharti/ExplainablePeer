 "This paper analyzes the loss function and properties of CNNs with one \"wide\" layer, i.e., a layer with number of neurons greater than the train sample size.[[INT-NEU,PDI-NEU], [null]]  Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums.[[MET-NEU,RES-NEU], [null]]  I like the presentation and writing of this paper.[[OAL-POS], [CLA-POS,PNF-POS]]  However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \"wide\"-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.[[MET-NEG,RES-NEG], [EMP-NEG]]  The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.[[MET-NEU,RES-NEU], [null]]  This is not surprising.[[MET-NEG,RES-NEG], [EMP-NEG]]  It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error. "[[RES-NEU], [EMP-NEU]]