 "This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases.[[INT-NEU,PDI-NEU], [null]]  A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs.[[PDI-NEU], [null]] \n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.[[MET-NEU], [null]]  Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters.[[MET-NEU], [null]]  Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN.[[DAT-NEU,EXP-NEU], [null]] \n\nWhile the idea is interesting and might be a good alternative to standard CNNs,[[PDI-POS], [EMP-POS]]  the paper falls short in terms of providing experimental validation that would demonstrate the latter point.[[MET-NEU], [null]]  It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.[[EXP-NEG], [SUB-NEG]]  They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results.[[DAT-POS,EXP-POS], [EMP-POS]]  The CIFAR-10, STL-10, and SVHN results are disappointing.[[DAT-NEG,RES-NEG], [EMP-NEG]]  CCNNs do not outperform the prior CNN results listed in Table 2,3,4.[[RES-POS], [CMP-POS]]  Moreover, these tables do not even cite more recent higher-performing CNNs.[[TNF-NEG], [SUB-NEG]]  See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper.[[DAT-NEG,RES-NEG], [CMP-NEG]] \n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?[[DAT-NEG,RES-NEG], [CMP-NEG,EMP-NEG]]   To be competitive, deeper CCNNs would likely need to be trained.[[EXP-NEU], [null]]   I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.[[DAT-NEU,RES-NEU], [SUB-NEU]]   Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?[[DAT-NEU,RES-NEU], [EMP-NEU]]   Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.[[DAT-NEG,MET-NEG], [SUB-NEU]]  \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table.[[RES-NEU], [SUB-NEU]]   This would assist in understanding tradeoffs in the design space.[[MET-NEU], [EMP-NEU]]  \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?[[MET-NEU], [CMP-NEU]]   If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?[[MET-NEU], [CMP-NEU]]  \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN? [[MET-NEU], [CMP-NEU]]  Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training.[[EXP-NEG], [EMP-NEG]]   Such dependence for CCNNs appears to be a weakness in comparison."[[MET-NEG], [CMP-NEG]]