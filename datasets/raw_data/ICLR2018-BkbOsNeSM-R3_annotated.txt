 "The paper consider a method for \"weight normalization\" of layers of a neural network. [[INT-NEU,MET-NEU], [null]]  The weight matrix is maintained normalized, which helps accuracy.[[MET-POS,RES-NEU], [EMP-POS]]   However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root).[[MET-NEU], [CNT]] \n\n The paper proposes \"FastNorm\", which is a way to implicitly maintain the normalized weight matrix using much less computation.[[MET-NEU], [null]]   Essentially, a normalization vector is maintained an updated separately.[[MET-NEU], [null]]  \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n[[MET-POS], [EMP-POS]]   Cons:   A very natural and simple solution that is fairly obvious.[[RES-NEG], [CNT]] \n\n          Limited experiments \n\n"[[EXP-NEG], [SUB-NEG]]