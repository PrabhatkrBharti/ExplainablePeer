 "# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP).[[INT-NEU,PDI-NEU], [null]]  The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).[[PDI-NEU], [null]]  In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations.[[MET-NEU], [null]]  This network is trained through supervised learning for the output and reinforcement learning for discrete operations.[[MET-NEU], [null]]  The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline.[[RWK-POS,EXP-POS,RES-POS], [CMP-POS,EMP-POS]] \n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new.[[PDI-POS], [NOV-POS]] \n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description).[[OAL-NEG], [CLA-NEG]]  So, it is hard to understand the core idea of this paper.[[PDI-NEG], [EMP-NEG]] \n- The proposed method and problem setting are not well-justified.[[PDI-NEG,MET-NEG], [EMP-NEG]]  \n- The results are not very convincing.[[RES-NEG], [EMP-NEG]] \n\n# Novelty and Significance\n- The problem considered in this paper is new,[[PDI-POS], [NOV-POS]]  but it is unclear why the problem should be formulated in such a way..[[PDI-NEG], [EMP-NEG]]  To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem.[[PDI-NEU], [EMP-NEU]]  I do not see why this should be formulated as a \"sequential\" decision problem.[[PDI-NEU], [EMP-NEU]]  Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks.[[PDI-NEU], [EMP-NEU]]  This does not require any \"memory\" because all necessary information is available to the network.[[MET-NEU], [EMP-NEU]] \n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications.[[MET-NEU,RES-NEG], [SUB-NEG]]  \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary.[[EXP-NEG], [EMP-NEG]]  The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network.[[PDI-NEU], [EMP-NEU]]  In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective.[[RWK-NEU,MET-NEU], [CMP-NEU]]  It would be good to justify (empirically) the proposed reward function.[[MET-NEU], [SUB-NEU,EMP-NEU]]  \n- The results are not fully-convincing.[[RES-NEG], [EMP-NEG]]  If my understanding is correct, the LTMN is trained to predict the baseline solver's output.[[RWK-NEU,EXP-NEU], [CMP-NEU]]  But, the LTMN significantly outperforms the baseline solver even in the training set.[[RWK-POS,EXP-POS], [CMP-POS,EMP-POS]]  Can you explain why this is possible?[[EXP-NEU], [EMP-NEU]] \n\n# Clarity\n- The problem statement and model description are not described well.[[PDI-NEG], [CLA-NEG,PNF-NEG]]  \n1) Is the network given a sequence of program/solution input?[[MET-NEU], [CNT]]  If yes, is it given during evaluation as well?[[MET-NEU], [EMP-NEU]]  \n2) Many notations are not formally defined.[[OAL-NEG], [PNF-NEG]]  What is the output (o_t) of the network?[[RES-NEU], [EMP-NEU]]   Is it the optimal solution (x_t)?[[RES-NEU], [EMP-NEU]]   \n3) There is no mathematical definition of memory addressing mechanism used in this paper.[[MET-NEG], [SUB-NEG]]  \n- The overall objective function is missing.[[MET-NEG], [SUB-NEG]]  \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"[[BIB-NEU], [null]]