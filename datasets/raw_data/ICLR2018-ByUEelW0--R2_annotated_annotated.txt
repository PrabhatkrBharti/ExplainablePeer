 "The paper proposes an additional transform in the recurrent neural network units.[[INT-NEU], [null]]  The transform allows for explicit rotations and swaps of the hidden cell dimensions.[[MET-NEU], [null]]  The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates.[[PDI-NEU,MET-NEU], [EMP-NEU]] \n\nMy first concern is the motivation.[[PDI-NEU], [null]]  I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates.[[EXP-NEU], [SUB-NEU]]  In the proposed example, it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swaps.[[EXP-NEG], [EMP-NEG]]  Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation.[[EXP-NEU], [EMP-NEU]] \n\nSecondly, the experimental analysis is not very extensive. [[EXP-NEU,ANA-NEU], [null]] The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset.[[DAT-NEU], [SUB-NEU]]  I think a language modeling benchmark and/or a larger scale question answering dataset should be considered.[[DAT-NEU,MET-NEU], [SUB-NEU]] \n\nRegarding the experimental setup, how are the hyper-parameters for the baseline tuned?[[EXP-NEU], [EMP-NEU]]  Have you considered training jointly (across the tasks) as well?[[EXP-NEU], [null]] \n\nAlso, is the setting the same as in Weston et al (2015)?[[RWK-NEU,EXP-NEU], [null]]  While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on.[[RWK-NEU,EXP-NEU], [EMP-NEU]] \n\nFinally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18?[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nSome minor issues:\n- The references are somewhat inconsistent in style: some have urls, others do not; some have missing authors, ending with \"et al\".[[BIB-NEG], [PNF-NEG]] \n- Section 1, second paragraph: senstence\n- Section 3.1, first paragraph: thorugh\n- Section 5: architetures"[[OAL-NEU], [CLA-NEG]]