 "This paper studies the problem of learning one-hidden layer neural networks and is a theory paper.[[INT-NEU], [null]]  A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent.[[MET-NEG], [EMP-NEG]]  This paper establishes an interesting connection between least squares population loss and Hermite polynomials.[[MET-POS], [EMP-POS]]  Following from this connection authors propose a new loss function.[[MET-POS], [NOV-POS]]  Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings.[[MET-POS], [EMP-POS]] \n\nOverall, pretty interesting result and solid contribution.[[RES-POS], [NOV-POS]]  The paper also raises good questions for future works.[[FWK-POS], [null]]  For instance, is designing alternative loss function useful in practice? [[MET-NEU], [EMP-NEU]] In summary, I recommend acceptance.[[OAL-POS], [REC-POS]]  The paper seems rushed to me so authors should polish up the paper and fix typos.[[OAL-NEU], [CLA-NEU]] \n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?[[MET-NEU], [EMP-NEU]]  If so they should clarify this otherwise it confuses the reader a bit.[[MET-NEU], [EMP-NEU]] \n2) What can be said about rate of convergence in terms of network parameters?[[MET-NEU], [EMP-NEU]]  Currently a generic bound is employed which is not very insightful in my opinion.[[MET-NEU], [EMP-NEG]]