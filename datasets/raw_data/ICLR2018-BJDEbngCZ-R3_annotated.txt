 "The paper studies the global convergence for policy gradient methods for linear control problems.[[INT-NEU,PDI-NEU], [null]]  \n(1) The topic of this paper seems to have minimal connection with ICRL.[[INT-NEU,PDI-NEG], [APR-NEG]]  It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully.[[OAL-NEG], [APR-NEG]]  \n\n(2) I am not convinced if the main results are novel.[[RES-NEG], [NOV-NEG]]  The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming.[[MET-NEG], [EMP-NEG]]  The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration. [[MET-NEU], [null]] \n\n(3) The main results of this paper seem technical sound.[[RES-POS], [EMP-POS]]  However, the results seem a bit limited because it does not apply to neural-network function approximator. [[RES-NEG], [SUB-NEG]] It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted.[[PDI-NEG], [EMP-NEG]]  I might have missed something here.[[EXT-NEU], [null]]  I strongly suggest that these results be submitted to a more suitable venue.\n\n"[[RES-NEG], [APR-NEG]]