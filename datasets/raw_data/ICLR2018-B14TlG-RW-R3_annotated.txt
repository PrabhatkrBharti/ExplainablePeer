 "This paper presents a reading comprehension model using convolutions and attention.[[INT-NEU,PDI-NEU], [null]]  This model does not use any recurrent operation but it is not per se simpler than a recurrent model.[[RWK-NEU], [CMP-NEU]]  Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.[[DAT-POS,MET-POS], [EMP-POS]]   On SQuAD dataset, their results show some small improvements using the proposed augmentation technique.[[DAT-NEU,MET-NEU,RES-NEU], [EMP-NEU]]  Their best results, however, do not outperform the best results reported on the leader board.[[RES-NEG], [EMP-NEG]] \n\nOverall, this is an interesting study on SQuAD dataset.[[DAT-POS,OAL-POS], [IMP-POS]]  I would like to see results on more datasets and more discussion on the data augmentation technique.[[DAT-NEU,RES-NEU,ANA-NEU], [SUB-NEU]]  At the moment, the description in section 3 is fuzzy in my opinion. [[CNT], [PNF-NEG]] Interesting information could be:\n- how is the performance of the NMT system?[[DAT-NEU,RES-NEU], [PNF-NEU]]  \n- how many new data points are finally added into the training data set?[[DAT-NEU], [null]] \n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n"[[DAT-NEU], [PNF-NEU]]