 "The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning.[[INT-NEU], [null]] \nThe idea of re-using a pretrained agent has both pros and cons.[[PDI-NEU], [null]]  On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment.[[MET-NEU], [EMP-NEU]]  On the other hand, the usefulness of the learned representation for planning is unclear.[[MET-NEU], [EMP-NEU]]  A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions.[[MET-NEU], [EMP-NEU]] \nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation.[[MET-NEU], [EMP-NEU]] \u2028In addition to that, one could fine-tune the representation during forward model training.[[MET-NEU], [EMP-NEU]]  \nIt would be interesting to see if this can improve the results.[[RES-NEU], [EMP-NEU]] \n\nI personally miss a more technical and detailed exposition of the ideas.[[MET-NEU], [EMP-NEU]]  For example, it is not described anywhere what loss is used for learning the model. [[MET-NEU], [EMP-NEG]] MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.[[MET-NEU], [EMP-NEG]]  \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way.[[MET-NEU], [CLA-NEU,EMP-NEG]] \nI also could find the details on how figure 1 was produced[[TNF-NEG], [EMP-NEG]] . As I understand, MCTS was not used in this experiment.[[EXP-NEG], [EMP-NEU]]  If so, how would one play with just a forward model?[[MET-NEU], [EMP-NEU]] \n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability.[[MET-NEG], [EMP-NEG]]  Is mini-RTS a deterministic environment? [[MET-NEU], [null]] \nWould it be possible to include a non-deterministic baseline in the experimental comparison?[[MET-NEU], [EMP-NEU]] \n\nExperimentally, the results are rather weak compared to pure model-free agents.[[RWK-NEU,EXP-NEU,RES-NEG], [CMP-NEG]]  Somewhat unsatisfying, longer-term prediction results into weaker game play.[[RES-NEG], [EMP-NEG]]  Doesn\u2019t this support the argument about need in stochastic prediction?[[MET-NEU], [EMP-NEU]]  \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance.[[RES-NEG], [CLA-NEG,REC-NEG]]  \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names[[OAL-NEG], [CLA-NEG]] \n* Figure 1 that introduces them contains typos.[[TNF-NEG], [CLA-NEG]]  \n* Formatting of figure 8 needs to be fixed.[[TNF-NEU], [PNF-NEU]]  This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there.[[TNF-NEG], [PNF-NEG]]