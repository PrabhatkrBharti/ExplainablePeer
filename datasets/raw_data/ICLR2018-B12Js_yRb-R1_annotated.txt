 "\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count.[[INT-NEU,PDI-NEU], [null]] \n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals.[[INT-NEU,PDI-NEU], [null]] \n\nClarity:\n- The paper is well written and clarity is good.[[OAL-POS], [CLA-POS]]  Figure 2 & 3 helps the readers understand the core algorithm.[[MET-POS,TNF-POS], [PNF-POS]] \n\nPros:\n- De-duplication modules of inter and intra object edges are interesting.[[MET-POS], [EMP-POS]] \n- The proposed method improves the baseline by 5% on counting questions.[[MET-POS], [EMP-POS]] \n\nCons:\n- The proposed model is pretty hand-crafted.[[MET-NEG], [EMP-NEG]]  I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016).[[RWK-NEG,MET-NEG], [SUB-NEG,EMP-NEG]] \n- One major bottleneck of the model is that the proposals are not jointly finetuned.[[MET-NEU], [EMP-NEU]]  So if the proposals are missing a single object, this cannot really be counted.[[MET-NEU], [EMP-NEU]]  In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals.[[MET-NEU], [EMP-NEU]]  The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is.[[MET-NEG], [SUB-NEG]] \n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain.[[RWK-NEG], [EMP-NEG]] \n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017).[[RWK-NEG,EXP-NEG], [SUB-NEG,CMP-NEG]] \n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).[[MET-NEG], [SUB-NEG]]  This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance.[[MET-NEG], [CMP-NEG,EMP-NEG]] \n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions.[[RWK-NEG,RES-NEG], [CMP-NEG]] \\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL[[RWK-NEG,EXP-NEG,RES-NEG], [SUB-NEG]] \\n\nConclusion:\n- I feel that the motivation is good,[[PDI-POS], [EMP-POS]]  but the proposed model is too hand-crafted.[[MET-NEG], [EMP-NEG]]  Also, key experiments are missing: 1) NMS baseline[[EXP-NEG], [SUB-NEG]]  2) Comparison with VQA counting work  (Chattopadhyay et al., 2017).[[RWK-NEG,EXP-NEG], [SUB-NEG,CMP-NEG]]  Therefore I recommend reject.[[OAL-NEG], [REC-NEG]] \n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017.[[RWK-NEU,BIB-NEU], [SUB-NEU]] \n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016.[[RWK-NEU,BIB-NEU], [SUB-NEU]] \n\nUpdate:\nThank you for the rebuttal.[[EXt-NEU], [CNT]]  The paper is revised and I saw NMS baseline is added.[[OAL-POS,RWK-POS], [SUB-POS]]  I understood the reason not to compare with certain related work.[[RWK-NEU], [CMP-NEU]]  The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy.[[RES-POS], [REC-POS]]  However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6."[[FWK-NEG,OAL-NEG], [IMP-NEG,REC-NEG]]