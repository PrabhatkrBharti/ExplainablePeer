 "After the rebuttal:\n\nI do not think I had a major misunderstanding of the paper.[[EXT-NEU], [CNT]]  I was aware that the features mostly refers to the inputs to softmax.[[EXT-NEU], [CNT]]  In my point 4, I was suggesting that in order to have clustering performance, one might alternatively work on the softmax outputs instead of the inputs.[[MET-NEU], [EMP-NEU]] \n\nMy opinion on this paper remains, and I think the contribution of this paper to machine learning is not very clearer at the current stage.[[OAL-NEG], [APR-NEG]]  It might be the case that the considered scenarios indeed happen in computer vision related problems, but I am not an expert in that regard.[[EXT-NEU,OAL-NEU], [null]] \n\n========================================================================\n\nThis paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance.[[INT-NEU,PDI-NEU], [null]]  The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere.[[MET-NEU], [EMP-NEU]]  Overall, the proposed method is a relatively simple tweak to softmax.[[MET-NEU], [EMP-NEU]]  The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks.[[MET-NEU], [null]] \n\nMy main concern with this paper is the motivation: what are the practical scenarios in which one would want to used proposed method?[[MET-NEG], [EMP-NEG]] \n1. It is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  Euclidean metric (e.g. the problem depicted in Figure 1),  because they are not trained to do so: their purpose is just to predict the correct label.[[MET-NEG], [SUB-NEG,EMP-NEG]]   While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy.[[MET-NEU], [SUB-NEG]] \n\n2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage.[[MET-NEU,TNF-NEU], [EMP-NEU]]  But this does not matter since you already use the labels in the softmax loss.[[MET-NEU], [EMP-NEU]] \n\n3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer.[[MET-NEU], [EMP-NEU]]  In fact, Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy.[[MET-NEG], [EMP-NEG]]  \n\n4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this.[[MET-NEU], [EMP-NEU]]  If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)[[MET-NEU,RES-NEU], [EMP-NEU]] [[MET-NEU], [EMP-NEU]] \n\n5.  The experiments on adversarial robustness and face verification seems more interesting to me,[[EXP-POS], [EMP-POS]]  but the tasks were not carefully explained for someone not familiar with that literature.[[EXP-NEG], [EMP-NEG]]  Perhaps for these tasks, multi-class classification is not the most correct objective, and maybe the proposed regularization can help, but the motivations are not given. \n\n\n\n"[[MET-NEG], [EMP-NEG]]