 "1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter.[[INT-NEU], [null]]  However, all compression methods such as pruning and quantization also have this concern.[[RWK-NEU], [null]]  For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved.[[MET-NEU], [null]]  Therefore, the novelty of the proposed method is somewhat weak.[[MET-NEU], [NOV-NEG]] \n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].[[RWK-NEU], [CMP-NEU]]  However, the paper only did a very simple investigation on related works.[[RWK-NEG], [SUB-NEG,CMP-NEU]] \n[r1] CNNpack: packing convolutional neural networks in the frequency domain.[[RWK-NEU], [null]] \n[r2] LCNN: Lookup-based Convolutional Neural Network.[[RWK-NEU], [null]] \n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks.[[RWK-NEU], [null]] \n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.[[DAT-NEU,EXP-NEU], [SUB-NEG]]  It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet.[[DAT-NEU,EXP-NEU], [SUB-NEU,EMP-NEU]]