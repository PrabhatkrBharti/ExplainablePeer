 "This paper proposes a self-normalizing bipolar extension for the ReLU activation family.[[INT-NEU], [null]]  For every neuron out of two, authors propose to preserve the negative inputs.[[MET-NEU], [null]]  Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU.[[MET-NEU], [null]] \n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN.[[MET-POS], [EMP-POS]] \n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.[[MET-NEU], [null]]   They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions.[[RWK-NEU,EXP-NEU], [CMP-POS]]  They also show that they can train deep residual network architecture on CIFAR without the use of BN.[[MET-NEU], [null]] \n\nQuestion:\n- Which layer mean and variance are reported in Figure 2? [[TNF-NEU], [null]] What is the difference between the left and right plots?[[TNF-NEU], [null]] \n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances.[[ANA-NEU,TNF-NEU], [EMP-NEU]]  It would be nice to report the training loss to see if this is an optimization or a generalization problem.[[EXP-NEU], [EMP-NEU]] \n- How does bipolar activation compare to model train with BN on CIFAR10?[[RWK-NEU,MET-NEU], [CMP-NEU]] \n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?[[RWK-NEU,MET-NEU], [CMP-NEU]] \n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB.[[MET-NEU,ANA-NEU], [EMP-NEU]]  However, BN-LSTM outperforms BELU-RNN on Text8.[[MET-NEU,ANA-NEU], [EMP-NEU]]  Do you know why the trend is not consistent across datasets?[[MET-NEU,ANA-NEU], [EMP-NEU]] \n\n-Clarity/Quality\nThe paper is well written and pleasant to read.[[OAL-POS], [CLA-POS]] \n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel.[[MET-POS], [NOV-POS]] \n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost.[[MET-POS], [IMP-POS]]  In this paper, bipolar activations are used to train very deep stacked RNN. [[MET-NEU], [EMP-NEU]] However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures.[[RWK-NEU,MET-NEG], [EMP-NEG]]  It is not clear what are the advantage of deep stacked RNN in that context."[[MET-NEU], [IMP-NEU]]