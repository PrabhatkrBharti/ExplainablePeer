 "This paper proposes a tensor factorization-type method for learning one hidden-layer neural network.[[INT-NEU], [null]]  The most interesting part is the Hermite polynomial expansion of the activation function.[[MET-NEU], [null]]  Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.[[MET-NEU], [null]]  They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015. [[RWK-NEU,MET-NEU], [null]] At last, they also establish the sample complexity for recovery.[[MET-NEU], [null]] \n\nThe organization and presentation of the paper need some improvement.[[OAL-NEU], [PNF-NEU]]  For example, the authors defer many technical details.[[MET-NEU], [EMP-NEU]]  To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages.[[OAL-NEU], [CLA-NEU]] \n\nThere are also some typos: For example, the dimension of a is inconsistent.[[MET-NEG], [EMP-NEG]]  In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector.[[ABS-NEU], [CLA-NEG]]  On Page 8, P(B) should be a degree-4 polynomial of B.[[CNT], [CLA-NEG]] \n\nThe paper does not contains any experimental results on real data."[[EXP-NEU,RES-NEG], [EMP-NEG]]