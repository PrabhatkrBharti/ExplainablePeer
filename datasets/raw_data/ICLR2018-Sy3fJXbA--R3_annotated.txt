 "The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel.[[INT-NEU,PDI-NEU], [null]]   The input to each model is a sparse sum of the outputs of modules in the previous set.[[INT-NEU,PDI-NEU], [null]]  The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on.[[DAT-POS], [EMP-POS]]   \n\nPros:\n- The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights.[[RWK-POS,MET-POS], [CMP-POS]]   Furthermore, this learning extends to large-scale image datasets.[[DAT-NEU], [null]] \n- There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network.[[MET-POS], [EMP-POS]] \n\nCons:\n- Overall, the approach seems to be an incremental improvement over the previous work ResNeXt.[[RWK-POS,MET-POS], [CMP-POS,EMP-POS]] \n- The datasets used are not very interesting: Cifar is too small, and ImageNet is essentially solved.[[DAT-NEG], [EMP-NEG]]   From the standpoint of the computer vision community, increasing performance on these datasets is no longer a meaningful objective.[[DAT-NEG], [EMP-NEG]]  \n- The modifications add complexity.[[DAT-NEG], [EMP-NEG]]  \n\nThe paper is well written and conceptually simple.[[OAL-POS], [CLA-POS]]   However, I feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate acceptance.   "[[OAL-NEG], [NOV-NEG,REC-NEG]]