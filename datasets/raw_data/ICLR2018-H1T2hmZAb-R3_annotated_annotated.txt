 "Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions.[[INT-NEU], [null]]  Their \"related work section\" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations.[[RWK-NEU], [null]]  However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued.[[MET-NEU], [EMP-NEU]] \n\nTheir contributions are:\n\n1. Formulate complex valued convolution[[MET-NEU], [null]] \n2. Formulate two complex-valued alternatives to ReLU and compare them[[MET-NEU], [CMP-NEU]] \n3. Formulate complex batch normalization as a \"whitening\" operation on complex domain[[MET-NEU], [null]] \n4. Formulate complex analogue of Glorot weight normalization scheme[[MET-NEU], [null]] \n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case.[[MET-NEU], [EMP-NEU]]  For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward.[[MET-NEU], [EMP-NEG]]  However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic.[[RWK-NEU,DAT-NEU,MET-NEU], [EMP-NEU]]  Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance."[[MET-NEU], [null]]