 "This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.[[INT-NEU,PDI-NEU], [null]]   The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies.[[PDI-NEU,MET-NEU], [null]] \n\nTo be honest, I didn't really \"get\" this paper.\n*[[OAL-NEG], [CNT]]  As far I understand, all of the original work policy gradients involved stochastic policies.[[MET-NEU], [EMP-NEU]]   Many are/were Gaussian.\n*[[MET-NEU], [null]]  All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*[[MET-NEU], [null]]  As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution.[[MET-NEU], [EMP-NEU]] \n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas.[[MET-NEU], [EMP-NEU]] \n\nI thought the little 2-mode MOG was a nice example of the premise of the model.[[MET-POS], [EMP-POS]] \n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.[[EXP-NEG], [null]]   Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.[[TNF-NEG], [PNF-NEG]]   Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.[[MET-NEU], [SUB-NEG]]   They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\" [[MET-NEG], [SUB-NEG,EMP-NEG]]  I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published.[[PDI-NEG,OAL-NEG], [APR-NEG,REC-NEG]] \n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n"[[RWK-NEU,BIB-NEG], [PNF-NEG]]