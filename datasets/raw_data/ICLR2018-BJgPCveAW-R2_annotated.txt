 "The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.[[INT-NEU,PDI-NEU], [null]]  Numerical experiments show that such sparse networks can have similar performance to fully connected ones.[[MET-NEU], [null]]  They introduce a concept of \u201cscatter\u201d that correlates with network performance.[[MET-NEU], [null]]  Although  I found the results useful and potentially promising,[[RES-POS], [EMP-POS]]  I did not find much insight in this paper.[[OAL-NEG], [CNT]] \nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.[[MET-NEG], [EMP-NEG]]  Once the signals from different windows are intermixed, how do you even define the windows?[[MET-NEG], [EMP-NEG]]   \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n"[[MET-NEU], [EMP-NEU]]