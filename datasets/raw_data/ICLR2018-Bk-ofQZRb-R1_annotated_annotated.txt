 "Summary: This paper tackles the issue of combining TD learning methods with function approximation.[[INT-NEU], [null]]  The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule.[[MET-NEU], [null]]  Results with linear and non-linear function approximation highlight the attributes of the method.[[MET-NEU,RES-NEU], [null]] \n\nQuality: The quality of the writing, notation, motivation, and results analysis is low[[RES-NEG,ANA-NEG,OAL-NEG], [CLA-NEG]] . I will give a few examples to highlight the point.[[EXT-NEU], [null]]  The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed.[[MET-NEU], [EMP-NEG]]  There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma.[[BIB-NEG], [null]]  This is a very active area of research and the paper needs to justify their approach.[[MET-NEU], [EMP-NEU]]  The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar. [[OAL-NEG,MET-NEG], [EMP-NEG]] The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others.[[MET-NEG,OAL-NEG], [EMP-NEG]] \n\nThe experiments have serious issues.[[EXP-NEG], [null]]  Exp1 seems to indicate that the new method does not converge to the correct solution.[[EXP-NEG], [EMP-NEG]]  The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed.[[EXP-NEG], [EMP-NEG]]  Again exp3 provides little information about the experimental setup.[[EXP-NEG], [EMP-NEG]] \n\nClarity: The clarity of the text is fine, though errors make things difficult sometimes.[[OAL-NEU], [CLA-NEU]]  For example The Bhatnagar 2009 reference should be Maei.[[BIB-NEG], [PNF-NEG]] \n \nOriginality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable. [[MET-NEG], [EMP-NEG]] \n\nSignificance: Because of all the things outlined above, the significance is below the bar for this round.[[OAL-NEG], [IMP-NEG]]