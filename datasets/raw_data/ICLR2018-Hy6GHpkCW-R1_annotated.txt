 "This paper introduces a neural network architecture for generating sketch drawings.[[INT-NEU], [null]]  The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree.[[INT-NEU,PDI-NEU], [null]]  The contribution of this paper of this paper is two-fold.[[OAL-NEU], [CNT]]  I Firstly, the paper introduces a large sketch dataset that future papers can rely on.[[DAT-POS,FWK-POS], [SUB-POS,IMP-POS]]  Secondly, the paper introduces the model for generating sketch drawings.[[MET-NEU], [null]]  I\n\nThe model is inspired by the variational autoencoder.[[MET-NEU], [null]]  I However, the proposed method departs from the theory that justifies the variational autoencoder.[[MET-NEU], [EMP-NEU]]  I believe the following things would be interesting points to discuss / follow up:[[MET-NEU], [SUB-NEU]] \n- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood.[[MET-NEU], [null]]  It seems to have a negative impact for the range of values that are discussed.[[DAT-NEU,RES-NEG], [EMP-NEG]]  However, I would expect there to be an optimum.[[RES-NEU], [EMP-NEU]]  Does the KL term help prevent overfitting at some stage?[[MET-NEU], [EMP-NEU]]  Answering this question may help understand what influence variational inference has on this model.[[MET-NEU,RES-NEU], [EMP-NEU]] \n- The decoder model has randomness injected in it at every stage of the RNN.[[MET-NEU], [EMP-NEU]]  Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing.[[MET-NEU], [EMP-NEU]]  It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term.[[MET-NEG,RES-NEG], [EMP-NEG]]  Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic?[[MET-NEU], [EMP-NEU]]  This definitely would bring it closer to the way the VAE was originally introduced.[[MET-NEU], [EMP-NEU]] \n- The unconditional generative model *only* relies on the \"injected randomness\" for generating drawings, as the initial state is initialised to 0.[[MET-NEU,RES-NEU], [EMP-NEU]]  This also is not in the spirit of the original VAE, where unconditional generation involves sampling from the prior over the latent space.[[RWK-NEG,MET-NEG], [CMP-NEG]] \n\nI believe the design choices made by the authors to be valid in order to get things to work.[[MET-NEU], [EMP-NEU]]  But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better).[[ANA-NEU], [SUB-NEU,EMP-NEU]]  This would help interesting applications inform what is wrong with current theoretical views.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\nOverall, I would argue that this paper is a clear accept."[[OAL-POS], [REC-POS]]