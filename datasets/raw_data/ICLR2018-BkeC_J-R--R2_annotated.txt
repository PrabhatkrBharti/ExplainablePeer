 "This paper proposes to combine reinforcement learning with supervised learning to speed up learning.[[INT-NEU,PDI-NEU], [null]]  Unlike their claim in the paper, the idea of combining supervised and RL is not new.[[PDI-NEG,RWK-NEG], [NOV-NEG,CMP-NEG]]  A good example of this is a supervised actor-critic by Barto (2004).[[PDI-NEU,EXT-NEU], [CMP-NEU]]  I think even alphaGo uses some form of supervision.[[PDI-NEU,EXT-NEU], [CMP-NEU]]  However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel.[[PDI-POS,EXP-POS,MET-POS], [NOV-POS]] \n\nHaving the luxury of some supervised episodes is of course useful.[[MET-POS], [EMP-POS]]  The first step of building a supervised initial model looks straight forward.[[MET-POS], [EMP-POS]]  The next step of the algorithm is less easy to follow, and presentation of the ideas could be much better.[[MET-NEG], [PNF-NEG,EMP-NEG]]  This part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization? Doesn\u2019t this prevent exploration? What are the network parameters (e.g. size of layers) etc.[[MET-NEG], [EMP-NEU]]  I am not sure I could redo the work from the provided information.\n\n[[MET-NEG], [EMP-NEG]] Overall, it is unclear to me what the advantage of the algorithm is over pure supervised learning, and I don\u2019t think a compelling case has been made.[[MET-NEG,RES-NEG], [EMP-NEG]]  Since the influence of the supervision is increased by increasing alpha, it can be expected that results should be better for increasing alpha.[[RES-NEG], [EMP-NEG]]  The results seem to indicate that an intermediate level of alpha is best, though I would even question the statistical significance by looking at the curves in Figure 3.[[MET-NEG,RES-NEU,TNF-NEG], [EMP-NEG]]  Also, what is the epoch number, and why is this 1 for alpha=0?[[MET-NEU], [PNF-NEU]]  If the combination of supervised learning with RL is better, than this should be clearly stated.[[MET-NEU], [PNF-NEG]]  Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer?[[MET-NEU], [SUB-NEG]]  \n\nThe presentation could also be improved with some language edits.[[OAL-NEU], [PNF-NEG]]  Several articles are wrongly placed and even some meaning is unclear.[[OAL-NEU], [PNF-NEG]]  For example, the phrase \u201ccontinuous input sequence\u201d does not make sense; maybe you mean \u201cinput sequence of real valued quantities\u201d.\n\n[[OAL-NEG], [CLA-NEG,PNF-NEG]] In summary, while the paper contains some good ideas, I certainly think it needs more work to make a clear case for this method. \n"[[MET-NEG], [PNF-NEG,EMP-NEG]]