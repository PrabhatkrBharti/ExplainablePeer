 "The paper presents an interesting framework for bAbI QA.[[INT-POS], [null]]   Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences).[[PDI-NEU], [null]]   The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index.[[MET-NEU], [null]]   While the argument makes sense, it is not clear to me why one cannot simply index the original text.[[MET-NEG], [EMP-NEG]]  The additional encode/decode mechanism seems to introduce unnecessary noise.[[MET-NEG], [EMP-NEU]]   The framework does include several components and techniques from latest recent work, which look pretty sophisticated.[[RWK-NEU,MET-NEU], [EMP-NEU]]  However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven.[[DAT-NEU,MET-POS], [SUB-NEG,EMP-NEG]] \n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams[[MET-POS], [EMP-POS]] \n\nCons:\n  1. The overall justification is somewhat unclear[[OAL-NEG], [CLA-NEG]] \n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n"[[MET-NEU], [EMP-NEU]]