 "Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention.[[INT-NEU], [null]]  This text generation task is traditionally done using recurrent neural networks.[[MET-NEU], [null]]  This paper proposes to generate text using GANs.[[MET-NEU], [null]]  GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.[[RWK-NEU], [CMP-NEU]]  This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.[[MET-NEU], [null]]  The whole network is trained on the \"fill-in-the-blank\" task using the sequence-to-sequence architecture for both the generator and the discriminator.[[EXP-NEU,MET-NEU], [null]]  At training time, the generator's encoder computes a context representation using the masked sequence.[[EXP-NEU], [null]]  This context is conditioned upon to generate missing words.[[MET-NEU], [null]]  The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.[[MET-NEU], [EMP-NEU]]  With this approach, one can generate text at test time by setting all inputs to blanks.[[MET-NEU], [EMP-NEU]]  \n\nPros and positive remarks: \n--I liked the idea behind this paper.[[PDI-POS], [null]]  I find it nice how they benefited from context (left context and right context) by solving a \"fill-in-the-blank\" task at training time and translating this into text generation at test time.[[EXP-POS], [EMP-POS]]  \n--The experiments were well carried through and very thorough.[[EXP-POS], [EMP-POS]] \n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.[[MET-POS], [EMP-POS]]  I first thought that performance would be better when the generator's encoder uses the unmasked sequence.[[MET-NEU], [EMP-NEU]]  Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time.[[MET-POS], [EMP-POS]] \n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.[[MET-NEU], [EMP-NEU]]  There is too much pre-training. I find this less elegant. [[MET-NEG], [EMP-NEG]] \n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well?[[RWK-NEU,MET-NEU], [null]] \n            (2) how was the masking done?[[MET-NEU], [null]]  how did you decide on the words to mask? was this at random?[[MET-NEU], [null]] \n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence. [[MET-NEG], [EMP-NEG]] It needs to but \n                  that was not explicit in the paper.[[MET-NEU], [EMP-NEU]] \n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well.[[EXP-NEU], [EMP-NEU]] \n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well.[[DAT-NEU,EXP-NEU], [EMP-NEU]]