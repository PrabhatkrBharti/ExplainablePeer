 "This paper suggests a reparametrization of the transition matrix.[[INT-NEU,MET-NEU], [null]]  The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks.[[MET-NEU], [null]] \n\nThe paper is well-written and authors explain related work adequately.[[OAL-POS], [CLA-POS]]  The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.[[MET-NEU], [EMP-NEU]]  The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.[[PDI-NEU,FWK-NEU], [IMP-NEU]]  \n\nI have two comments on the experiment section:\n\n- Choice of experiments.[[EXP-NEU], [EMP-NEU]]  Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.[[DAT-NEU,EXP-NEU], [CMP-NEU,EMP-NEU]]  For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs.[[DAT-NEU,EXP-NEU], [CMP-NEU,EMP-NEU]]  For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported.[[EXP-NEU], [EMP-NEU]] \n\n- Stopping condition.[[EXP-NEU], [EMP-NEU]] \ The plots suggest that the optimization has stopped earlier for some models.[[RES-NEG], [EMP-NEG]]  Is this because of some stopping condition or because of gradient explosion?[[RES-NEU], [EMP-NEU]]  Is there a way to avoid this?[[MET-NEU], [EMP-NEU]] \n\n- Quality of figures.[[TNF-NEU], [PNF-NEU]]  Figures are very hard to read because of small font.[[TNF-NEG], [PNF-NEG]]  Also, the captions need to describe more details about the figures."[[TNF-NEG], [SUB-NEG,PNF-NEG]]