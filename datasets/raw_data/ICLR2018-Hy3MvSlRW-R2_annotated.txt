 "Summary:\n\nThis paper proposes an adversarial learning framework for machine comprehension task.[[INT-NEU,PDI-NEU], [null]]  Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task.[[PDI-NEU], [null]]  Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N.[[DAT-NEU,RES-NEU], [null]] \n\n\nMy Comments:\n\nThis paper is a direct application of adversarial learning to the task of reading comprehension.[[OAL-NEU], [null]]  It is a reasonable idea and authors indeed show that it works.[[PDI-NEU], [null]] \n\n1. The paper needs a lot of editing.[[OAL-NEG], [PNF-NEG]]  Please check the minor comments.[[OAL-NEG], [PNF-NEG]] \n\n2. Why is the adversary called narrator network?[[MET-NEU], [null]]  It is bit confusing because the job of that network is to obfuscate the passage.[[MET-NEG], [PNF-NEG]] \n\n3. Why do you motivate the learning method using self-play?[[MET-NEU], [EMP-NEG]]  This is just using the idea of adversarial learning (like GAN) and it is not related to self-play.[[MET-NEG], [EMP-NEG]] \n\n4 In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting.[[CNT], [null]]  How is this happening?[[CNT], [null]]  Can you elaborate more?[[CNT], [SUB-NEU]] \n\n5. The learning framework is not explained in a precise way.[[MET-NEG], [EMP-NEG]]  What do you mean by re-initializing and retraining the narrator? Isn\u2019t it costly to reinitialize the network and retrain it for every turn?[[MET-NEU], [EMP-NEU]]  How many such epochs are done?[[MET-NEU], [EMP-NEU]]  You say that test set also contains obfuscated documents.[[MET-NEU], [EMP-NEU]]  Is it only for the validation set?[[MET-NEG], [EMP-NEG]]  Can you please explain if you use obfuscation when you report the final test performance too?[[MET-NEU], [EMP-NEU]]  It would be more clear if you can provide a complete pseudo-code of the learning procedure.[[EXP-NEU,MET-NEU], [EMP-NEU]] \n\n6. How does the narrator choose which word to obfuscate?[[MET-NEU], [EMP-NEU]]  Do you run the narrator model with all possible obfuscations and pick the best choice?[[EXP-NEU], [EMP-NEU]] \n\n7. Why don\u2019t you treat number of hops as a hyper-parameter and choose it based on validation set?[[MET-NEU], [EMP-NEU]]  I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set.[[TNF-NEU,RES-NEU], [EMP-NEU]] \n\n8. In figure 2, how are rounds constructed?[[TNF-NEU], [EMP-NEU]]  Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement?[[EXP-NEU,MET-NEU], [EMP-NEU]]  This will be clear if you provide the pseudo-code for learning.[[MET-NEU], [EMP-NEU]] \n\n9. I do not understand author's\u2019 justification for figure-3.[[TNF-NEG], [EMP-NEG]]  Is it the case that the model learns to attend to last sentences for all the questions?[[MET-NEU], [EMP-NEU]]  Or where it attends varies across examples?\n\n10. Are you willing to release the code for reproducing the results?\n\nMinor comments:\n\nPage 1, \u201cexploit his own decision\u201d should be \u201cexploit its own decision[[MET-NEU], [EMP-NEU]] \u201d\nIn page 2, section 2.1, sentence starting with \u201cIndeed, a too low percentage \u2026\u201d needs to be fixed.[[RES-NEG], [EMP-NEG]] \nPage 3, \u201cforgetting is compensate\u201d should be \u201cforgetting is compensated\u201d.[[CNT], [null]] \nPage 4, \u201cfor one sentences\u201d needs to be fixed.[[CNT], [null]] \nPage 4, \u201cunknow\u201d should be \u201cunknown\u201d.\nPage 4, \u201c??[[CNT], [null]] \u201d needs to be fixed.[[CNT], [null]] \nPage 5, \u201cfor the two first datasets\u201d needs to be fixed.[[DAT-NEG], [EMP-NEG]] \nTable 1, \u201cGMenN2N\u201d should be \u201cGMemN2N\u201d.[[TNF-NEG], [null]]  In caption, is it mean accuracy or maximum accuracy?[[CNT], [null]] \nPage 6, \u201cdataset was achieves\u201d needs to be fixed.[[DAT-NEG], [EMP-NEG]] \nPage 7, \u201cdocument by obfuscated this word\u201d needs to be fixed.\[[CNT], [null]] nPage 7, \u201coverall aspect of the two first readers\u201d needs to be fixed.[[CNT], [null]] \nPage 8, last para, references needs to be fixed.[[BIB-NEG], [null]] \nPage 9, first sentence, please check grammar.[[CNT], [CLA-NEG]] \nSection 6.2, last sentence is irrelevant.\n"[[CNT], [null]]