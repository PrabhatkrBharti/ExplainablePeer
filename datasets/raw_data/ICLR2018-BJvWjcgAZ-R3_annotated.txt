 "This paper proposes a new way of sampling data for updates in deep-Q networks.[[INT-NEU,PDI-NEU], [null]]  The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode.[[PDI-NEU], [null]] \n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques.[[RWK-NEG,OAL-NEG], [SUB-NEG,CMP-NEG]] \n\nThe results presented by this paper shows improvement over the baseline.[[RES-POS], [CMP-POS]]  But the Atari results is still significantly worse than the current SOTA.[[RWK-NEG,RES-NEG], [CMP-NEG]]  Some (theoretical) analysis would be nice.[[ANA-NEG], [SUB-NEG]]  It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case.[[TNF-NEG], [EMP-NEG]] \n\nThere has been a number of highly relevant papers.[[RWK-NEU], [null]]  Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case.[[MET-NEU,TNF-NEU], [EMP-NEU]] \n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method.[[RWK-NEU,MET-NEU], [CMP-NEU]]  Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline.[[RWK-NEG,MET-NEG], [CMP-NEG]]  The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule."[[MET-NEU], [null]]