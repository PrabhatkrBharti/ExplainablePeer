 "The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them.[[PDI-NEU], [null]]  The idea is tested on very small data sets (80 and 50 examples, respectively).[[DAT-NEG], [SUB-NEG]]  The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets.[[RWK-NEG,MET-NEG], [CMP-NEG]] \n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove[[RES-NEG], [EMP-NEG]] . Is the point that GloVe is a bad algorithm?[[MET-NEU], [EMP-NEU]]  That these steps are general?[[MET-NEU], [EMP-NEU]]  If the latter, then the experimental results are far weaker than what I would find convincing.[[EXP-NEG,RES-NEG], [EMP-NEG]]  Why not try on multiple different word embeddings?[[EXP-NEU], [EMP-NEU]]  What happens if you start with random vectors?[[EXP-NEU], [EMP-NEU]]  What happens when you try a bigger data set or a more complex problem?"[[DAT-NEU], [EMP-NEU]]