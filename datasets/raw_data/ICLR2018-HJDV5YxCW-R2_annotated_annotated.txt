 "The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights.[[RWK-NEU,EXP-NEU,RES-NEU,ANA-NEU], [EMP-NEU]] \n\n1.  The paper misses some more recent reference, e.g. [a,b].[[BIB-NEG,OAL-NEG], [SUB-NEG]]  The author should also have a discussion on them.[[OAL-NEG], [SUB-NEG]] \n\n2. Indeed, AlexNet is a good seedbed to test binary methods.[[RWK-NEU,EXP-NEU,MET-NEU], [null]]  However, it is more interesting and important to test on more advanced networks.[[RWK-POS,EXP-POS,MET-POS], [EMP-POS]]  So, I wish to see a section on testing with Resnet and GoogleNet.[[EXT-NEU], [null]] \n\nIndeed, the authors have commented: \"AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures.[[RWK-NEU,EXP-NEU,MET-NEU], [null]] \" So, please show that.[[RWK-NEG], [SUB-NEG]] \n\n3. The paper wants to find a good trade-off on speed and accuracy.[[EXT-NEU], [null]]  The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy?[[RWK-NEU,EXP-NEU,ANA-NEU], [null]] \n\nMy concern is that one-bit system is already complicated to implement.[[EXT-NEU], [null]]  Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?[[RWK-NEU,EXP-NEU,MET-NEU], [null]]  One example is Section 4 in [Courbariaux et al. 2016].[[RWK-NEU,BIB-NEU], [null]] \n\n4. Is trade-off between 1 to 2 bits really important? [[RWK-NEU], [null]] \n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1). [[RWK-NEU,PDI-NEU,MET-NEU,TNF-NEU], [null]] Is such improvement really important?[[EXT-NEU], [null]] \n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b].[[BIB-NEU], [null]]  Extremely low bit neural network: Squeeze the last bit out with ADMM.[[RWK-NEU], [null]]