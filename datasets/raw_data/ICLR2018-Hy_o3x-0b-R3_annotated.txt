 "Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples).[[RWK-NEG,DAT-NEG], [CMP-NEG,REC-NEG]] \n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.[[INT-NEU,MET-NEU], [null]]   The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1], [[MET-NEU], [null]] (2) the latent variable parameters are functions of a CNN, [[MET-NEU], [null]] and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3].[[EXP-NEU,MET-NEU], [null]]   Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported.[[DAT-NEU,RES-NEU], [null]]   Samples are shown for CIFAR 10, MNIST, and OMNIGLOT. [[DAT-NEU,RES-NEU], [null]]        \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off).[[RWK-POS,DAT-POS,EXP-POS,MET-POS], [CMP-POS,EMP-POS]]   This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result.[[DAT-NEU,EXP-NEU], [null]]   I commend the authors for making their code available already via DropBox.[[EXT-POS], [null]]   Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results.[[RES-POS], [EMP-POS]]                  \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper. [[MET-POS], [NOV-POS]]  In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments).[[DAT-NEG,RES-NEG], [EMP-NEG]]   I find the authors\u2019 claim that FAME is performing superior global modeling interesting. [[MET-NEU], [null]]  Is there a way to support this experimentally?[[EXP-NEU], [EMP-NEU]]   Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?[[MET-NEU,RES-NEU], [SUB-NEU]]   \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low,[[OAL-NEG], [NOV-NEG]]  the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable. [[DAT-POS,EXP-POS,MET-POS], [EMP-POS]]  I recommend the paper\u2019s acceptance for this reason.[[OAL-POS], [REC-POS]] \n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n"[[BIB-NEU], [CNT]]