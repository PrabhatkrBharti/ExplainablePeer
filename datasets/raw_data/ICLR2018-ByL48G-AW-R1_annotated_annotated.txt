 "SUMMARY\nThe paper deal with the problem of RL.[[PDI-NEU], [null]]   It proposes a non-parametric approach that maps trajectories to the optimal policy.[[INT-NEU], [null]]   It avoids learning parameterized policies.[[MET-NEU], [null]]   The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it.[[PDI-NEU], [null]] \n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different.[[MET-NEU], [EMP-NEU]] \n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.[[RWK-NEU,MET-NEU], [EMP-NEU]]  \n\nHow do you execute a trajectory?[[MET-NEU], [null]]  Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly.[[MET-NEU], [EMP-NEG]]