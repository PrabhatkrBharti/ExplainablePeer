 "The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML.[[INT-NEU,PDI-NEU], [null]]  Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary.[[MET-NEU], [null]]  The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule.[[MET-NEU], [null]]  (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically.[[INT-NEU,EXP-NEU,MET-NEU], [null]]  (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize.[[DAT-NEU,EXP-NEU,MET-NEU], [null]]  \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric.[[MET-POS,RES-POS], [EMP-POS]]  Given a reward function, one can define the Bayes decision rule.[[MET-NEU], [null]]  The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.[[MET-NEU], [null]]  The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).[[MET-NEU], [null]]  Of course, the moving-out is biased but the replacing is unbiased.[[MET-NEU], [EMP-NEU]]  \n\nThe second contribution is partially valid,[[MET-NEU,RES-NEU], [EMP-NEU]]  although I doubt how much improvement one can get from SQDML.[[MET-NEG,RES-NEG], [EMP-NEG]]  The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).[[EXP-NEU,MET-NEU], [null]]  In fact, this step can result in biased estimation because the replacement is inside the nonlinear function.[[MET-NEG], [EMP-NEG]]  When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example.[[EXP-NEU,MET-NEU], [EMP-NEG]]  However, when x is not repeated frequently, both RAML and SQDML are biased.[[MET-NEU], [EMP-NEU]]  Experiment in section 4.1.2 do not validate significant improvement, either.[[EXP-NEG], [EMP-NEG]] \n\nThe numerical results are relatively weak.[[RES-NEG], [EMP-NEG]]  The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.[[EXP-NEU], [null]]  However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau.[[TNF-NEU,RES-NEU], [EMP-NEU]]  Is there any guidelines to choose \\tau?[[MET-NEU,RES-NEU], [EMP-NEG]]  For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper.[[EXP-NEG,OAL-NEG], [EMP-NEG]]   These results are also lower than the state of the art performance.[[RWK-NEG,RES-NEG], [EMP-NEG]]  \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.[[MET-NEU], [EMP-NEU]]  This is a direct result from definition of conditional probability.[[MET-NEU], [EMP-NEU]]  No Bayesian elements, like prior or likelihood appears here.[[MET-NEU], [EMP-NEU]] \n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator.[[EXP-NEU,MET-NEU], [EMP-NEU]]  Compared with the n-gram replacement used in the paper, which one is better?[[MET-NEU], [CMP-NEU]] \n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.[[MET-NEU], [CMP-NEU]]  This will make the comparison much more clear.[[MET-NEU], [EMP-NEU]] \n(4). What is Theorem 2 trying to convey?[[MET-NEU], [EMP-NEU]]  Although \\tau goes to 0, there is still a gap between Q and Q'.[[MET-NEU], [EMP-NEU]]  This seems to suggest that for small \\tau, Q' is not a good approximation of Q.[[MET-NEG], [EMP-NEG]]  Are the assumptions in Theorem 2 reasonable?[[MET-NEU], [EMP-NEU]]  There are several typos in the proof of Theorem 2.[[MET-NEU], [CLA-NEU]] \n(5). In section 4.2.2, the authors write \"the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\".[[EXP-NEU,MET-NEU], [null]]  Could you explain it in more details?\n\n"[[EXP-NEU,MET-NEU], [PNF-NEU]]