 "The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system.[[INT-NEU,MET-NEU], [null]]  Because the layer-wise decoupling, it can easily be applied for distributed training of the model.[[EXP-NEU], [null]]  The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results.[[EXP-NEU,MET-NEU,RES-NEU], [null]] \n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,[[MET-POS], [EMP-POS]]  it is a shame to see the overall quality of the paper very weak.[[OAL-NEG], [CLA-NEG]]  The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP).[[OAL-NEG], [CLA-NEG]]  The experiments are also very weak.[[EXP-NEG], [EMP-NEG]]  Important information on the experiment settings are missing, e.g., how the model is parallelized.[[EXP-NEG,MET-NEG], [SUB-NEG]] \n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD.[[MET-NEG], [CMP-NEG]]  It needs to be better defined."[[MET-NEG], [EMP-NEG]]