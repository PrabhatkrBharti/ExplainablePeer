 "The authors propose a new defense against security attacks on neural networks.[[INT-NEU], [null]]  The attack model involves a standard l_inf norm constraint.[[MET-NEU], [null]]  Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice.[[MET-POS], [EMP-POS]]  Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks.[[MET-NEU], [EMP-NEU]]  The approach is evaluated for several attacks on MNIST data.[[EXP-NEU,MET-NEU], [null]] \n\nFirst of all, the paper is very well written and structured.[[OAL-POS], [CLA-POS]]  As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic).[[MET-POS], [EMP-POS]]  The certificate is derived with rigorous and sound math.[[MET-POS], [EMP-POS]]  An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown.[[MET-POS], [EMP-POS]]  An innovative training criterion based on that certificate is proposed.[[MET-POS], [null]]  Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks.[[MET-POS], [EMP-POS]] \n\nIn summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation.[[EXP-POS,MET-POS], [EMP-POS]]  For me, it is a clear accept.[[OAL-POS], [REC-POS]]  The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.\n"[[RWK-NEG], [CMP-NEG,EMP-NEG]]