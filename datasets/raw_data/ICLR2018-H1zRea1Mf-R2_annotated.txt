 "The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface[[INT-NEU,PDI-NEU], [null]] . Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time[[PDI-NEU,MET-NEU], [EMP-NEU]] . Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released.[[PDI-NEU,DAT-NEU,EXP-NEU], [EMP-NEU]] \n\nPros:\n- Generating programs with neural networks is an exciting direction[[PDI-NEU,MET-NEU], [IMP-POS]] \n- Novel task of generating UI code from UI screenshots[[PDI-POS], [NOV-POS]] \n- Three new datasets of UI images and corresponding code\[[DAT-POS], [EMP-POS]] n- Paper is clearly written\[[INT-POS], [CLA-POS]] n\nCons:\n- Limited technical novelt[[PDI-NEG], [NOV-NEG]] y\n- Limited experiments[[PDI-NEG,EXP-NEG], [IMP-NEG,EMP-NEG]] \n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.[[PDI-POS,MET-POS], [IMP-POS]]  Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.[[RWK-POS], [NOV-POS,IMP-POS]]  The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task[[INT-POS,RWK-POS,DAT-POS], [NOV-POS,EMP-POS]] .\n\nMy main concern with this paper is a lack of technical novelty.[[INT-NEG], [EMP-NEG]]  The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now.[[MET-NEU], [EMP-NEU]]  Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand.[[MET-NEG], [SUB-NEG]] \n\nThe experiments in this paper are also lacking.[[INT-NEG,EXP-NEG], [EMP-NEG]]  Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments[[INT-POS,DAT-POS,EXP-NEG,MET-POS], [EMP-NEG]] . The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples.[[MET-NEU], [EMP-NEU]]  I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods.[[MET-NEG], [CMP-NEG,EMP-NEG]]  I would have also liked to see more innovation in evaluation.[[MET-NEG], [IMP-NEG]]  Are there metrics other than overall accuracy that could be used to measure performance? [[MET-NEU], [EMP-NEU]] Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?[[PDI-NEU,ANA-NEU], [CMP-NEU]]  In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?[[PDI-NEU,MET-NEG], [EMP-NEU]]  Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?[[DAT-NEU], [IMP-NEU]]  How does performance change with different amounts of training data[[DAT-NEU,RES-NEU], [IMP-NEU]] ? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another[[DAT-NEU], [IMP-NEU]] ? I don\u2019t expect the authors to answer any of these questions in particular;[[INT-NEG,FWK-NEG], [REC-NEG]]  I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset.[[PDI-POS,DAT-POS,EXP-POS], [IMP-POS,EMP-POS]] \n\nOn the whole I appreciate the novelty of the task and dataset,[[INT-POS,RWK-POS,DAT-POS], [NOV-POS]]  but the paper suffers from a lack of technical novelty in the model and limited experimental validation."[[INT-NEG,RWK-NEG,EXP-NEG], [EMP-NEG]]