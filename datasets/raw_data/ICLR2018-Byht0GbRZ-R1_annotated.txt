 "Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences.[[INT-NEU], [null]]  The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.[[MET-NEU], [null]]  Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016).[[RWK-POS,EXP-POS,BIB-NEU], [CMP-POS]] \n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting.[[PDI-POS], [null]]  \n\nWeaknesses:\nThe paper is 8.5 pages long[[OAL-NEG], [PNF-NEG]] .\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017)[[RWK-NEU,MET-NEG], [CMP-NEU]] \n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive.[[MET-NEU], [CMP-NEG]]  Am I missing something about the algorithm?[[MET-NEU], [null]] \n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax.[[DAT-NEU,MET-NEU], [EMP-NEU]]  The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure 2[[DAT-NEU,MET-NEU,TNF-NEG], [CMP-NEG,EMP-NEG]]