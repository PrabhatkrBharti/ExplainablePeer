 "This paper propose an adaptive dropout strategy for class logits.[[INT-NEU,PDI-NEU], [null]]  They learn a distribution q(z | x, y) that randomly throw class logits.[[MET-NEU], [null]] [[INT-NEU,PDI-NEU], [null]]  By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks.[[MET-NEU], [null]]  They learn the dropout distribution by variational inference with concrete relaxation.[[MET-NEU], [null]]  \n\nOverall I think this is a good paper.[[OAL-POS], [CNT]]  The technique sounds, the presentation is clear and I have not seen similar paper elsewhere[[MET-POS,OAL-POS], [PNF-POS,EMP-POS]]  (not 100% sure about the originality of the work though).[[OAL-NEG], [NOV-NEG]]  \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak.[[EXP-NEG], [EMP-NEG]]  Only on CIFAR100 the proposed approach is much better than other approaches.[[RWK-POS,DAT-POS,MET-POS], [CMP-POS,EMP-POS]]  I would like to see the results on more datasets.[[DAT-NEU,RES-NEU], [SUB-NEU]]  Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut."[[RWK-NEU,MET-NEU], [CMP-NEU]]