 "This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor.[[INT-NEU], [null]]  In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems. [[MET-NEU], [null]] The discussion on the deficiencies of the naive LP approach is mostly well done.[[MET-POS], [EMP-POS]]  Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization.[[MET-NEU], [null]]  They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements.[[RWK-NEU,EXP-NEU], [CMP-POS]]  Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.[[RWK-NEU,EXP-NEU], [CMP-NEU]]  \n\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments.[[EXP-NEU,MET-NEU], [IMP-NEU,EMP-NEU]]  Given these clarifications in an author response, I would be willing to increase the score.[[OAL-NEU], [REC-NEU]]  \n\nFor the theory, there are a few steps that need clarification and further clarification on novelty.[[OAL-NEU], [NOV-NEU]]  For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results.[[MET-NEU], [NOV-NEU]]  It looks like Theorem 2 has already been shown in \"Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d.[[RWK-NEU,MET-NEU], [CMP-NEU]]  There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting.[[RWK-NEU,BIB-NEU], [CMP-NEU]]  However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d.[[MET-NEU], [EMP-NEU]]  Is you Theorem 2 somehow an extension? Is Theorem 3 completely new?[[MET-NEU], [EMP-NEU]] \n\nThis is particularly called into question due to the lack of assumptions about the function class for value functions.[[MET-NEG], [EMP-NEG]]  It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function).[[MET-NEU], [EMP-NEU]]  This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*.[[MET-NEU], [null]]  Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small.[[MET-NEU,RES-NEU], [EMP-NEU]]  More about conditions on eta_v would be illuminating. [[MET-NEU], [null]] \n\nThere is also one step in the theorem that I cannot verify.[[MET-NEU], [EMP-NEU]]  On Page 18, how is the squared removed for difference between U and Upi?[[MET-NEU], [EMP-NEU]]  The transition from the second line of the proof to the third line is not clear.[[MET-NEG], [EMP-NEG]]  It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2. [[MET-NEU], [EMP-NEU]] \n\n\nFor the experiments, the following should be addressed.[[EXP-NEU], [null]] \n\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains.[[TNF-NEU], [PNF-NEU]] \n\n2. The central contribution is extending the single step LP to a multi-step formulation.[[MET-NEU], [EMP-NEU]]  It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains.[[MET-NEU], [EMP-NEU]] \n\n3. Increasing k also comes at a computational cost.[[MET-NEU], [EMP-NEU]]  I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO).[[RWK-NEU,MET-NEU], [CMP-NEU]] \n\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization.[[MET-NEU], [EMP-NEU]]  It was also mentioned that increasing the regularization parameter size increases the convergence rate.[[MET-NEU], [EMP-NEU]]  Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization?[[MET-NEU], [EMP-NEU]]  In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned.[[EXP-NEU,RES-NEU], [EMP-NEU]]  Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity?[[EXP-NEU,RES-NEU], [EMP-NEU]]  A bit more discussion on these choices would be helpful.[[MET-NEU], [EMP-NEU]]  \n\nMinor comments:\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint.[[MET-NEU], [CLA-NEU]]