 "The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices.[[INT-NEU,PDI-NEU,MET-NEU], [null]]   They compare this\nbound with a similar recent bound proved by Bartlett, et al.[[RWK-NEU], [CMP-NEU]]   While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions.[[MET-POS], [CMP-POS,EMP-POS]] \n\nThe analysis is elegant.[[ANA-POS], [EMP-POS]]   It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent.[[MET-POS], [CLA-POS]] \n\nVery nice paper.[[OAL-POS], [CNT]] \n\nI guess that networks including convolutional layers are covered by\ntheir analysis.[[ANA-NEU], [EMP-NEU]]   It feels to me that these tend to be sparse,[[ANA-NEU], [EMP-POS]]  but that\ntheir analysis still my provides some additional leverage for such\nlayers.[[ANA-NEU], [SUB-NEU]]   Some explicit discussion of convolutional layers may be\nhelpful.  "[[MET-NEU,ANA-NEU], [SUB-NEU]]