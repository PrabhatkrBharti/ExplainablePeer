 "The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor.[[INT-NEU,PDI-NEU,RES-NEU], [null]]  While the RWA was an interesting idea[[PDI-POS], [null]]  with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods.[[RWK-NEG,MET-NEG,RES-NEG], [CMP-NEG,EMP-NEG]] \n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original.[[MET-POS,OAL-POS], [CLA-POS,EMP-POS]]  On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU[[RWK-NEG,MET-NEG], [CMP-NEG,EMP-NEG]]  - except for MultiCopy where it trains faster,[[MET-POS], [EMP-POS]]  but not to better results and it looks like the difference is between few and very-few training steps anyway.[[RES-NEG], [EMP-NEG]]  The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM.[[RES-NEU,EXT-NEU], [CNT]]  So the results are not strongly convincing, and the paper lacks any mention of newer work on attention.[[RES-NEG], [NOV-NEG]]  This year strong improvements over state-of-the-art have been achieved using attention for translation (\"Attention is All You Need\") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition).[[EXT-NEU], [null]]  To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks."[[MET-NEU,RES-NEU], [CMP-NEU]]