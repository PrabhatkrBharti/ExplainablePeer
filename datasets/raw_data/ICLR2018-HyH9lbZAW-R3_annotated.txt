 "The paper seems to be significant since it integrates PGM inference with deep models.[[OAL-POS], [IMP-POS]]  Specifically, the idea is to use the structure of the PGM to perform efficient inference.[[PDI-NEU], [null]]  A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.[[MET-NEU], [null]]  Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference.[[MET-NEU], [null]] \nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates.[[MET-POS], [EMP-POS]]  As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution.[[RWK-POS,MET-POS], [CMP-POS,EMP-POS]] \n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved.[[RWK-NEU], [CMP-NEU]] \ One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs.[[MET-NEU], [EMP-NEU]] \ Can this be generalized to arbitrary PGM structures? [[MET-NEU], [EMP-NEU]] How about cases when computing Z is intractable?[[MET-NEU], [EMP-NEU]]  Could the proposed approach be adapted to such cases.[[MET-NEU], [EMP-NEU]]  I was not very sure as to why the proposed method is more general than existing approaches.[[RWK-NEU,MET-NEU], [CMP-NEU]] \n\nRegarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets.[[DAT-NEG,EXP-NEU,MET-NEU], [EMP-NEU]]  the approach shows that the proposed methods converge faster than existing methods.[[RWK-NEU,MET-NEU], [CMP-NEU]]  However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting."[[MET-POS], [EMP-POS]]