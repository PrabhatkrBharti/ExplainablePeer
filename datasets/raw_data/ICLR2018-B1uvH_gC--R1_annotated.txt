 "The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks.[[INT-NEU,PDI-NEU], [null]]  The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension.[[PDI-NEU], [null]] \n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000.[[MET-NEG], [EMP-NEG]]  Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example.[[MET-NEG,RES-NEG], [EMP-NEG]]  In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE.[[MET-NEU], [null]]  Please add the Swiss roll example.[[MET-NEU], [SUB-NEU]] \n* Distance preservation appears more and more like a dated DR paradigm.[[MET-NEU], [SUB-NEU]]  Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation.[[MET-NEG], [EMP-NEG]]  Computation accuracy of the geodesic distances in high-dimensional spaces can be poor.[[RES-NEG], [EMP-NEG]]  This could be discussed and some experiments on very HD data should be reported.[[DAT-NEG], [SUB-NEG]] \n* Some key historical references are overlooked, like the SAMMANN.[[BIB-NEG], [null]]  There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago.[[MET-NEG], [EMP-NEG]]  Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity).[[MET-NEG], [CMP-NEG]]  Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too.[[EXP-NEU], [EMP-NEU]] \n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach.[[MET-NEG], [EMP-NEG]]  The motivation in the end of page 3 seems to be computational only.[[MET-NEG], [SUB-NEG]] \n* Section 3 should be further detailed (step 2 in particular).[[MET-NEG], [SUB-NEG]] \n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress.[[DAT-NEG,EXP-NEG], [EMP-NEG]]  The running times are not in favor of the proposed method.[[MET-NEG], [EMP-NEG]]  The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold.[[DAT-NEG], [SUB-NEG]] \n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...').[[RES-NEG], [SUB-NEG]]  What is the take-home message of the paper?"[[OAL-NEU], [IMP-NEU]]