 "This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.[[INT-NEU], [null]]  Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero.[[MET-NEU], [null]]  As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU. [[MET-NEU,RES-NEU], [EMP-NEU]] The DReLU is supposed to remedy the problem of covariate shift better.[[MET-NEU], [null]]  \n\nThe presentation of the paper is clear.[[OAL-POS], [PNF-POS]]  The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed).[[MET-POS], [null]]  Statistical tests are performed for many of the experimental results, which is solid.[[EXP-POS], [EMP-POS]] \n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta? [[MET-NEU], [EMP-NEU]] If it is done by hyperparameter tuning with cross-validation, the training cost may be too high.[[MET-NEU], [null]] \n2) I believe the control experiments are encouraging,[[EXP-POS], [EMP-POS]]  but I do not agree that other techniques like Dropouts are not useful.[[MET-NEG], [EMP-NEG]]  Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.[[MET-POS], [EMP-POS]]  The arguments for skipping this experiments are respectful,[[MET-POS], [EMP-POS]]  but not convincing enough.[[MET-NEU], [EMP-NEU]]   \n3) Batch normalization is popular, especially for the convolutional neural networks.[[MET-NEU], [null]]  However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway.[[MET-NEU], [EMP-NEU]]