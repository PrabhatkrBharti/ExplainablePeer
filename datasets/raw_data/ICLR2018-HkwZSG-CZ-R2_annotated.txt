 "The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language.[[INT-NEU,PDI-NEU], [null]]  As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15).[[INT-NEU,PDI-NEU], [null]]  This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores.[[DAT-NEU,RES-POS], [EMP-POS]] \n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention.[[EXT-NEU], [CNT]]  This makes a lot of the questions asked in this paper extremely relevant to the field.[[EXT-NEU], [CNT]]  However, it is unclear that the rank of the logit matrix is the right quantity to consider.[[MET-NEU], [EMP-NEU]]  For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices.[[MET-NEU], [EMP-NEU]]  Further, the first two \"observations\" in Section 2.2 would be more accurately described as \"intuitions\" of the authors.[[MET-NEU], [EMP-NEU]]  As they write themselves \"there is no evidence showing that semantic meanings are fully linearly correlated.[[MET-NEU], [EMP-NEU]] \" Why then try to link \"meanings\" to basis vectors for the rows of A?[[MET-NEU], [EMP-NEU]] \n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant.[[RWK-POS,MET-POS,RES-POS,ANA-NEU], [CMP-POS,EMP-POS]] \n\nUnfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling.[[DAT-NEG], [SUB-NEG]]  They do constitute a good starting place to test a model,[[MET-POS], [EMP-POS]]  but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting.[[DAT-NEG,MET-NEG,RES-NEU], [SUB-NEG]] \n\nAnother important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2).[[EXP-NEG], [SUB-NEU]]  This could help validate the hypothesis: how does the estimated rank vary with the number of components?[[EXP-NEU,RES-NEU], [EMP-NEU]]  How about the performance and pairwise KL divergence?[[MET-NEU,RES-NEU], [EMP-NEU]]  \n\nThis paper offers a promising direction for language modeling research,[[FWK-POS], [IMP-POS]]  but would require more justification, or at least a more developed experimental section.[[EXP-NEG,ANA-NEG], [SUB-NEG]] \n\nPros:\n- Important starting question[[INT-POS], [PNF-POS]] \n- Thought-provoking approach[[MET-POS], [EMP-POS]] \n- Experimental gains on small data sets[[DAT-NEU,RES-POS,EXP-POS], [EMP-POS]] \n\nCons:\n- The link between the intuition and reality of the gains is not obvious[[RES-NEG], [EMP-NEG]] \n- Experiments limited to small data sets, some obvious questions remain"[[EXP-NEG], [SUB-NEU]]