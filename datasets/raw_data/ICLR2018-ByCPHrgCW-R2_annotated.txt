 "Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping.[[INT-NEU], [null]] \nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain.[[PDI-NEU,MET-NEU], [EMP-NEU]] \nThis \"hybrid\" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time.[[MET-NEU], [EMP-NEU]] \nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference.[[MET-NEU], [EMP-NEU]] \n\nComments:\n1. I recommend the authors to tone down their claims.[[MET-NEU], [CLA-NEU]]  For example, the authors mentioned that \"there has been no complete implementation of established deep learning approaches\" in the abstract, however, the authors did not define what is \"complete\".[[ABS-NEU], [EMP-NEG]]  Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server.[[RWK-NEU,MET-NEU], [EMP-NEU]] \n\nAlso, the claim that \"we show efficient designs\" is very thin to me since there are no experimental comparisons between the proposed method and existing works.[[RWK-NEU,EXP-NEU], [CMP-NEG]]  Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C]. [[RWK-NEU,MET-NEU], [EMP-NEU]] For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE.[[MET-NEU], [EMP-NEU]] \n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.[[RWK-NEU,MET-NEU], [CMP-NEU]]  I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit.[[MET-NEU], [EMP-NEU]]  \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.[[MET-NEU], [EMP-NEU]]  Such optimizations are very common in deep learning field while less known in the field of security.[[EXT-NEU], [null]] \n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy.[[RWK-NEU,BIB-NEU], [null]] \n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.[[RWK-NEU,BIB-NEU], [null]]  \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations.[[RWK-NEU,BIB-NEU], [null]]