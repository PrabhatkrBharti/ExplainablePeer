 "Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation.[[INT-NEU,PDI-NEU], [null]] \nScheme A consists of training a high precision teacher jointly with a low precision student.[[MET-NEU], [EMP-NEU]]  Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode.[[MET-NEU], [EMP-NEU]] \n\nReview:\nThe paper is well written. [[OAL-POS], [CLA-POS]] The experiments are clear and the three different schemes provide good analytical insights.[[EXP-POS,MET-POS,ANA-POS], [EMP-POS]] \nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n[[EXP-NEU,MET-NEU,RES-NEU], [EMP-NEU]] Comments:\nTensorflow citation is missing.\n[[BIB-NEG], [PNF-NEG]] Conclusion is short and a few directions for future research would have been useful."[[FWK-NEU], [IMP-NEU]]