 "The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations.[[PDI-NEU,MET-NEU], [EMP-NEU]]  Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking.[[PDI-NEU,EXP-NEU,MET-NEU,ANA-NEU], [EMP-NEU]] \n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks.[[PDI-NEU,EXP-NEU,MET-NEU], [EMP-NEU]]  On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness.[[PDI-NEG], [SUB-NEG,EMP-NEG]]  For example, how does this compare to random perturbation (say, zero-mean) of the weights?[[PDI-NEU], [CMP-NEU]]  This adds stochasticity as well so why and why not this work? [[RWK-NEU], [EMP-NEU]] The authors do not give any insight in this regard.[[RWK-NEG,EXP-NEG,MET-NEG], [SUB-NEG,EMP-NEG]] \n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner.[[RES-POS,OAL-NEU], [REC-NEU]]  The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else).[[OAL-NEU], [null]]