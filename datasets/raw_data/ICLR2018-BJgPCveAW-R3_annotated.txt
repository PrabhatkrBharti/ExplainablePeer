 "The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters,[[PDI-NEU,MET-NEU], [null]] \n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices.[[PDI-NEU,MET-NEU], [null]] \nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers.[[PDI-NEG], [EMP-NEG]] \n2) The authors reject the technique of 'Deep compression' as being impractical.[[MET-NEG], [EMP-NEG]]  I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network.[[MET-NEG], [EMP-NEG]] \n\np3. What does 'normalized' mean?[[MET-NEU], [EMP-NEU]]  Batch-norm?[[MET-NEU], [EMP-NEU]] \np3. Are you using an L2 weight penalty?[[MET-NEU], [EMP-NEU]]  If not, your fully-connected baseline may be unnecessarily overfitting the training data.[[DAT-NEG,EXP-NEG], [EMP-NEG]] \np3. Table 1. Where do the choice of CL Junction densities come from?[[EXP-NEU,TNF-NEU], [EMP-NEU]]  Did you do a grid search to find the optimal level of sparsity at each level?[[MET-NEU], [EMP-NEU]] \np7-8. I had trouble following the left/right & front/back notation.[[TNF-NEU], [EMP-NEU]] \np8. Figure 7. How did you decide which data points to include in the plots?"[[DAT-NEU,TNF-NEU], [EMP-NEU]]