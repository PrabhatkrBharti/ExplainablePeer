 "This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP.[[INT-NEU], [null]]  The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications.[[EXP-NEU], [EMP-NEU]]  The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed.[[RWK-POS], [CMP-POS]] \n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.[[EXP-POS,MET-POS], [EMP-POS]] \n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR.[[EXP-POS,MET-NEG], [APR-NEG]]  First, there is not much innovation in the model architecture.[[MET-NEG], [EMP-NEG]]  The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level.[[MET-NEU], [null]]  I think this more counts as careful engineering of the SAN model rather than a main innovation.[[MET-NEU], [EMP-NEU]]  Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders.[[EXP-NEU], [null]]  What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller?[[MET-NEU], [null]] \n\n====\n\nI appreciate the answers the authors added and I change the score to 6."[[OAL-POS], [null]]