 "This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length.[[INT-NEU,PDI-NEU], [null]]  Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity.[[EXP-POS], [EMP-POS]]  The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings.[[MET-POS,RES-POS], [EMP-POS]]   There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations.[[EXP-POS], [EMP-POS]]   They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom.[[RWK-POS,RES-POS], [EMP-POS]]  \n\nOverall, I find the paper important for furthering the understanding of fundamental RL algorithms.[[MET-POS,FWK-POS], [CMP-POS]]   However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head.[[RES-NEG], [EMP-NEG]]   May this helped Q_MC have better perceptual capabilities?[[MET-NEU], [EMP-NEU]] \n\nA couple of other questions:\n- I couldn't find any mention of eligibility traces - why?[[MET-NEG], [SUB-NEG]]