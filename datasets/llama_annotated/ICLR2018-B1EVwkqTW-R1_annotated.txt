After reading the rebuttal:\n\nThis paper does have encouraging results.   But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons.   The writing also needs to be improved.   So I think the paper is not ready for publication and my opinion remains.  \n===========================================================\n\nThis paper presents an algorithm for few shot learning.   The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels).   I think the idea of representation learning using a somewhat artificial task makes sense in this setting. \n\nI have several concerns for this submission.  \n1. I am not very familiar with the literature of few shot learning.   I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.   Multilingual Distributed Representations without Word Alignment. ICLR 2014. \nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin.  This approach is useful even when there is only weak supervision to provide the \"similarity/dissimilarity\" information.  I wonder how does this approach compare with the proposed method. \n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.  I do not understand why the compared methods are not consistently used in both experiments.  Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is.  It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy.  It would be nice to understand/visualize what information have been extracted in the representation learning phase.  \n\n3. Relatively minor: The writing of this paper is readable,  but could be improved.  It sometimes uses vague/nonstandard terminology (\"parameterless\") and statement.  The term \"siamese kernel\" is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs.  In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]