  The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. \nThe basic form of SGD selects an example uniformly.    Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient .\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015]. \n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger. \n\n  The paper uses LSH structures, computed over the set of examples, \n to quickly sample examples with large inner products with the current parameter vector \\theta.    Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity. \n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings). \n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited.  The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.   There are theorems,  but they are trivial, straightforward applications of importance sampling.  \n\n The paper is not well written.  The presentation is much more complex that need be.  References to classic weighted sampling are  \n\n  The application is limited to certain loss functions for which we can compute LSH structures.   This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]