This paper presents a method to search neural network architectures at the same time of training.  It does not require training from scratch for each architecture, thus dramatically saves the training time.  The paper can be understood with no problem.   Moderate novelty,  network morphism is not novel,  applying it to architecture search is novel. \n\nPros:\n1. The required time for architecture searching is significantly reduced. \n2. With the same number or less of parameters, this method is able to outperform previous methods, with much less time. \n\nHowever, the method described is restricted in the following aspects. \n\n1. The accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing, this also makes the search greedy , which could be suboptimal.  In addition, the algorithm in this paper selects the best performing network at each step, which also hampers the discover of the optimal model.\ n\n2. Strong human prior, network morphism IV is more general than skip connection, for example, a two column structure belongs to type IV.  However, in the implementation, it is restricted to skip connection by addition.  This choice could be motivated from the success of residual networks.  This limits the method from discovering meaningful structures.  For example, it is difficult to discover residual network denovo.  This is a common problem of architecture searching methods compared to handcrafted structures. \n\n3. The comparison with Zoph & Le is not fair because their controller is a meta-network and the training happens only once.  For example, the RNNCell discovered can be fixed and used in other tasks, and the RNN controller for CNN architecture search could potentially be applied to other tasks too (though not reported).\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]