The paper is written well and clear.    The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist.   \n\nPros: the intuition and geometry is rather clearly presented.   \n\nCons: \nReferences to \"CaffeNet\"  and \"LeNet\" (even though the latter is well-known) are missing.   In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained.  \n\nIt is not clear why the authors have decided to use out-dated 5-layer \"LeNet\"  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures).  It would be nice to see how the behavior and boundaries look in these cases.   \n\nThe conclusion is speculative:\n\"Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it\nis key to suppress this subspace of shared positive directions, which can possibly be done through\nregularization of the objective function.  This will be the subject of future works. \" \n\nIt is clear that regularization should play a significant role in shaping the decision boundaries.  Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used.  All these factors should play a very significant role in the experimental validation of their hypothesis. \n\nNotes: I did not check the proofs of the theorems in detail. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]