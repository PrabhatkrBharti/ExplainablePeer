This paper presents a method for image classification given test-time computational budgeting constraints.   Two problems are considered:  \"any-time\" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.   A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.   In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.   Evaluations are performed on ImageNet and CIFAR-100. \n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the \"network reduction\" is performed ---  it looks like finer scales are progressively dropped in successive blocks,  but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \"lazy evaluation\").   A picture would help here, showing where the depth-layers are divided between blocks.\ n\nI was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:   It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.   So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).   This is fine, but could perhaps be pointed out if that is indeed the case. \n\nOverall, this seems like a natural and effective approach, and achieves good results[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]