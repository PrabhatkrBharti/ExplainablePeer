\nSummary:\n A method for creation of semantical adversary examples in suggested.  The \u2018semantic\u2019 property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter).  The generator is trained with a WGAN optimization.  Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space.  The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences.  It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected. \nPage 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated.  This should be explicitly said and the parameter value should be stated.\nPage 4: \n-\tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers.  This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described. \n-\tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier.  In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier).  In the churce-vs. tower case, a  relatively weak MLP classifier was used.  It would be more instructive to see the results for a better, convolutional classifier. \nPage 5:\n-\tthe description of the various networks used for text generation is insufficient for understanding:\no\tThe AREA is described in two sentences.  It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task\no\t \u2018inverter\u2019 here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces. \no\t It is not clear what the \u2018four-layers strided CNN\u2019 is: its structure, its role in the system. How is it optimized? \no\tIn general: a block diagram showing the relation between all the system\u2019s components may be useful, plus the details about the structure and optimization of the various modules.  It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough.  Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.\no\tSNLI data should be described: content, size, the task it is used for \n\n\nPro:\n-\tA novel idea of producing natural adversary examples with a GAN \n-\tThe generated examples are in some cases useful for interpretation and network understandin g \n-\tThe method enables creation of adversarial examples for block box classifiers \nCons\n-\tThe idea implementation is basic . Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested \n-\tThe generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier).  Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]