This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting.  They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory.  \n\nThe main novelty of this work are 1-balancing mechanism for the replay memory.  2-Using multiple models for short and long term memory.  The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before.  As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting.  \n\nThe paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model.  It is not clear to me as why is the case, if anything it should be the opposite.  Humans and animal are not given task ID and it's always clear distinction between task in real world. \n\nDeep Generative Replay section and description of DGDMN are written poorly and is very incomprehensible.  It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism.  \n\nUsing the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting. \n\nBaselines: why is Shin et al. (2017) not included as one of the baselines?  As it is the closet method to this paper it is essential to be compared against. \n\nI disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA.   Overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capacity. \n\n12 pages is very long, 8 pages was the suggested page limit. It\u2019s understandable if the page limit is extend by one page, but 4 pages is over stretching[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]