This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule. \n\nI have a few questions on the motivation and the results. \n- In the section \"Open Problems in RAML\", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q.  This is not true.  The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q.  It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau). \n- Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule,  Can anything be said about this?  Are the KL divergence small enough to guarantee similar predictive rewards? \n- In Theorem 2, when does the exponential tail bound assumption hold? \n- In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML.  Are the differences actually significant?  Are the differences between SQDML/RAML and ML significant?  In addition, how should \\tau be chosen in these experiments?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]