The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.   However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words.  However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution.  Does the argument hold for general noise distributions ?  With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof.  \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising.  The paper is interesting but lacks strong empirical results.  It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]