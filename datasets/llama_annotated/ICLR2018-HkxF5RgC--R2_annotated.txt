This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip.  The paper describes the use additional mechanisms for synchronization and memory loading.  \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).   With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads.  Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise.  \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip.  One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those. \n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs.  However, Narang et. al., 2017 already explore this idea, although briefly.  Furthermore, the gains from the sparsity appear rather limited over real applications.  I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).  Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial.  However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]