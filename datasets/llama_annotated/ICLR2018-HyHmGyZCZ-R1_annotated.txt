The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them.  The idea is tested on very small data sets (80 and 50 examples, respectively).  The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets. \n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove . Is the point that GloVe is a bad algorithm?  That these steps are general?  If the latter, then the experimental results are far weaker than what I would find convincing.  Why not try on multiple different word embeddings?  What happens if you start with random vectors?  What happens when you try a bigger data set or a more complex problem?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]