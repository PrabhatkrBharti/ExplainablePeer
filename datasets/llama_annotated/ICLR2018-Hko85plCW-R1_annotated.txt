This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention.  The paper is very well written and easy to follow . The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger. \n\nThe first set of questions is about the monotonic attention.  Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?  For example, how far does using the expected context vector deviate from marginalizing the monotonic attention?  The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention . How does the greedy step affect training and decoding?  It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding . These questions should really be answered in [1] . Since the authors are extending their work and since these issues might cause training difficulties,  it might be useful to look into these design choices .\n\nThe second question is about the window size $w$ . Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper,  why not attend to the entire segment, i.e., from the current boundary to the previous boundary ?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.  (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.)  How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW ?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition . If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention?  The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information.  Would the special cases lead to worse performance and if so why is there a difference?\ n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]