This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.  Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero.  As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU.  The DReLU is supposed to remedy the problem of covariate shift better.  \n\nThe presentation of the paper is clear.  The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed).  Statistical tests are performed for many of the experimental results, which is solid. \n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta?  If it is done by hyperparameter tuning with cross-validation, the training cost may be too high. \n2) I believe the control experiments are encouraging,  but I do not agree that other techniques like Dropouts are not useful.  Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.  The arguments for skipping this experiments are respectful,  but not convincing enough.   \n3) Batch normalization is popular, especially for the convolutional neural networks.  However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]