The submission describes an empirical study regarding the training performance\nof GANs; more specifically, it aims to present empirical evidence that the\ntheory of divergence minimization is more a tool to understand the outcome of\ntraining (i.e. Nash equillibrium) than a necessary condition to be enforce\nduring training itself .\n\nThe work focuses on studying \"non-saturating\" GANs, using the modified generator\nobjective function proposed by Goodfellow et al. in their seminal GAN paper, and\naims to show increased capabilities of this variant, compared to the \"standard\"\nminimax formulation.  Since most theory around divergence minimization is based\non the unmodified loss function for generator G, the experiments carried out in\nthe submission might yield somewhat surprising results compared the theory. \n\nIf I may summarize the key takeaways from Sections 5.4 and 6, they are:\n- GAN training remains difficult and good results are not guaranteed (2nd bullet\n  point) ;\n- Gradient penalties work in all settings, but why is not completely clear; \n- NS-GANs + GPs seems to be best sample-generating combination, and faster than\n  WGAN-GP .\n- Some of the used metrics can detect mode collapse. \n\nThe submission's (counter-)claims are served by example (cf. Figure 2, or Figure\n3 description, last sentence), and mostly relate to statements made in the WGAN\npaper (Arjovsky et al., 2017). \n\nAs a purely empirical study, it poses more new and open questions on GAN\noptimization than it is able to answer; providing theoretical answers is\ndeferred to future studies.  This is not necessarily a bad thing, since the\nextensive experiments (both \"toy\" and \"real\") are well-designed, convincing and\ncomprehensible . Novel combinations of GAN formulations (non-saturating with\ngradient penalties) are evaluated to disentangle the effects of formulation\nchanges. \n\nOverall, this work is providing useful experimental insights, clearly motivating\nfurther study.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]