This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU).  Experiments show that ISRLU is promising compared to competitors like ReLU and ELU. \n\nPros:\n(1) The paper is clearly written. \n\n(2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter \\alpha (although only fixed value is used in the experiments) to control the negative saturation zone.  \n\nCons:\n(1) Authors claim that ISRLU is faster than ELU, while still achieves ELU\u2019s performance. However, they only show the reduction of computation complexity for convolution, and speed comparison between ReLU, ISRLU and ELU on high-end CPU . As far as I know, even though modern CNNs have reduced convolution\u2019s computation complexity,  the computation cost of activation function is still only a very small part (less than 1%) in the overall running time of training/inference . \n\n(2) Authors only experimented with two very simple CNN architectures and with three different nonlinear activation functions, i.e., ISRLU/ELU/ReLU and showed their accuracies on MNIST . They did not provide the comparison of running time which I believe is important here as the efficiency is emphasized a lot throughout the paper .\n\n(3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing . Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results. \n\nOverall, I think the current version of the paper is not ready for ICLR conference.  As I suggested above, authors need more experiments to show the effectiveness of their approach.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]