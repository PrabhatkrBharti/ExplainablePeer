Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS).  The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information.  They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%). \n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory. \n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available. \n\nCons:\n- The model-based approach is disappointing compared to the model-free approach. \n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct,;  though I found some of the figures and definitions confusing, specifically:\n\n- The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.).  I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly. \n- In Figure 3b, it is not clear to me what the difference between the red and blue curves is. \n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI. \n- The caption in Figure 8 is malformatted. \n- In Figure 7, the baseline of \\hat{h_t}=h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if \\hat{h_t}=h_t) to see how much performance suffers as a result of model error. \n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain.  However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features.  The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below). \n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS.  The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms.  The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]