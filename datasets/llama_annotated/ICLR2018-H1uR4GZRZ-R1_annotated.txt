This paper investigates a new approach to prevent a given classifier from adversarial examples.  The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks.  Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training. \n\nThe proposed algorithm is clearly described.  However there are issues in the presentation. \n\nIn section 2-3, the problem setting is not suitably introduced. \nIn particular one sentence that can be misleading:\n\u201cGiven a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient\u2026\u201d\nYou should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient.  The practical way in which the adversarial examples are generated is not known to the player.  An adversary could choose any policy.  The only thing the player knows is the best adversarial policy. \n\nIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient.  Does it come from a high variance?  If it is the case, you should explain that the optimal policy of the adversary is approximated by \u201cfast gradient sign method\u201d.  \n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples.  SAP does not perform as well as adversarial training, but SAP could be used with a trained network.  \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training.  The presentation could be improved[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]