The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph.   Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems. \n\nPros: \n-Authors provide some empirical evidence for the benefits of using their technique. \n-Authors are fairly upfront about how, overall, it seems their technique isn't doing *too* much--null results are still results, and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much. \n\nCons: \n-The grammar in the paper is pretty bad.   It could use a couple more passes with an editor. \n-For a, more or less, entirely empirical paper, the choices of experiments are...somewhat befuddling.   Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good. \n-While I mentioned it as a pro, it also seems to be that this technique simply doesn't buy you very much as a practitioner.   If this is true--that learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments. \n\nUltimately, I will have to suggest rejection, unless the authors considerably beef up their manuscript with more experiments, more details, and improve the grammar considerably[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]