This paper proposes a tensor factorization-type method for learning one hidden-layer neural network.  The most interesting part is the Hermite polynomial expansion of the activation function.  Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.  They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015.  At last, they also establish the sample complexity for recovery. \n\nThe organization and presentation of the paper need some improvement.  For example, the authors defer many technical details.  To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages. \n\nThere are also some typos: For example, the dimension of a is inconsistent.  In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector.  On Page 8, P(B) should be a degree-4 polynomial of B. \n\nThe paper does not contains any experimental results on real data.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEU],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEU]]