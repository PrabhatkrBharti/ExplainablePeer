Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples). \n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.   The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1],  (2) the latent variable parameters are functions of a CNN,  and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3].   Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported.   Samples are shown for CIFAR 10, MNIST, and OMNIGLOT.         \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off).   This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result.   I commend the authors for making their code available already via DropBox.   Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results.                  \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper.   In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments).   I find the authors\u2019 claim that FAME is performing superior global modeling interesting.   Is there a way to support this experimentally?   Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?   \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low,  the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable.   I recommend the paper\u2019s acceptance for this reason. \n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]