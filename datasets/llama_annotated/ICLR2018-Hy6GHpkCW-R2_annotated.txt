The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder.  Each node is represented with (dx, dy) along with one-hot representation of three different drawing status.  A bi-directional LSTM is used to encode latent space in the training stage.  Auto-regressive VAE is used for decoding.  \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term.  \n\nPros:\n- Good solution to an interesting problem.  \n- Very interesting dataset to be released. \n- Intensive experiments to validate the performance.  \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy).  In the data collection stage, how were the points lists generated from pen strokes?   Did each points are sampled from same travelling distance or according to the same time interval?   Are there any other potential biases brought because the data collection tools? \n- Is log-likelihood a good loss here?  Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.  How do you deal with this case? \n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.? [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]