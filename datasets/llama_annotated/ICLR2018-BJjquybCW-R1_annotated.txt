This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks.  I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \"wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples. \n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data. \n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum.  These solutions achieve zero squared-loss. \n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1).  Intuitively, (1) is an easy result.  Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent.  The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small. \n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima;  (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure;  (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero.  If an optimization algorithm falls onto these solutions, it will be hard to escape. \n\nOverall, the paper presents several incremental improvement over existing theories.  However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]