After reading the authors's rebuttal I increased my score from a 7 to a 6.   I do think the paper would benefit from experimental results , but agree with the authors that the theoretical results are non-trivial and interesting on their own merit. \n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l} )\n\nThe work is inspired by previous results for feed forward nets and CNNs . However, what is unique to RNNs is their ability to model long term dependencies across time.  \n\nTo analyze this specific property, the authors propose a concept called \"start-end rank\" that essentially models the richness of the dependency between two disjoint subsets of inputs . Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points.  Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E) .\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E).  If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution) . A higher rank would correspond to more dependence across time . \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank,  since I think it much makes it clearer to explain . Right now the authors explain separation rank first and then discuss tensors / matricization). \n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs. \n\nI overall like the paper's theoretical results,  but I have the following complaints:\n\n(1)  I have the same question as the other reviewer.  Why is Theorem 1 not a function of L?   Do the papers that prove similar theorems about ConvNets able to handle general L ? What makes this more challenging?  I feel if comparing L=2 vs L=3 is hard, the authors should be more up front about that in the introduction/abstract .\n\n(2) I think it would have been stronger if the authors would have provided some empirical results validating their claims. \n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]