This paper proposes \"spectral normalization\" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function.  The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.  Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods. \n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.  The experimental results seem solid and seem to support the authors' claims.  I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.  Like the anonymous commenter, I also initially thought that the proposed \"spectral normalization \" is basically the same as \"spectral norm regularization\", but given the authors' feedback on this I think the differences should be made more explicit in the paper. \n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication. \n\nSmall Nits: \n\nSection 4: \"In order to evaluate the efficacy of our experiment\": I think you mean \"approach\". \n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \"As we prophesied ...\", and in the paragraph below \n * \"... is a tad slower ...\".[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]