The problem of interest is to train deep neural network models with few labelled training samples.  The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data.  The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching.  The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.  The teacher also supplies an uncertainty estimate to each predicted label.  How about the heuristic function?  This is used for learning initial feature representation of the student model.  Crucially, the teacher model will also rely on these learned features.  Labelled data and unlabelled data are therefore lie in the same dimensional space.  \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.  Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points.   Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data.   On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited.   As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations).   It will be informative to provide results with a single GP model.   \n2)\tFrom modifying learning rates to weighting samples.   Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty.   Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]