This paper applies the boosting trick to deep learning.  The idea is quite straightforward, and the paper is relatively easy to follow.  The proposed algorithm is validated on several image classification datasets. \n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper.  The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against. \n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise. \n3. In Eq. (3), \\tilde{D} is not defined. \n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$.   \n5. How many layers is the DenseNet-BC used in this paper?  Why the error rate reported here is higher than that in the original paper? \nTypo: \nIn Session 3 Line 7, there is a missing reference. \nIn Session 3 Line 10, \u201c1,00 object classes\u201d should be \u201c100 object classes\u201d. \nIn Line 3 of the paragraph below Equation 5, \u201cclasse\u201d should be \u201cclass\u201d.\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]