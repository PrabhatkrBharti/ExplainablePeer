The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer.  The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task .\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model. \n- The paper is clearly written. \n\nCons:\n- I am not sure what is novel in the proposed model.  While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network.  Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N.  Please tell me if I am missing something, but I am not sure of the contribution of the paper.  Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful. \n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-NEG],[ETH-NEG]]