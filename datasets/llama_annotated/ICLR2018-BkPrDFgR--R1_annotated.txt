The paper studies methods for verifying neural nets through their piecewise\nlinear structure.  The authors survey different methods from the literature,\npropose a novel one, and evaluate them on a set of benchmarks. \n\nA major drawback of the evaluation of the different approaches is that\neverything was used with its default parameters.  It is very unlikely that these\ndefaults are optimal across the different benchmarks.  To get a better impression\nof what approaches perform well, their parameters should be tuned to the\nparticular benchmark.  This may significantly change the conclusions drawn from\nthe experiments. \n\nFigures 4-7 are hard to interpret and do not convey a clear message.  There is no\nclear trend in many of them and a lot of noise.  It may be better to relate the\nstructure of the network to other measures of the hardness of a problem, e.g.\nthe phase transition.  Again parameter tuning would potentially change all of\nthese figures significantly, as would e.g. a change in hardware.  Given the kind\nof general trend the authors seem to want to show here, I feel that a more\ntheoretic measure of problem hardness would be more appropriate here. \n\nThe authors say of the proposed TwinStream dataset that it \"may not be\nrepresentative of real use-cases\". It seems odd to propose something that is\nentirely artificial. \n\nThe description of the empirical setup could be more detailed.  Are the\nproperties that are being verified different properties, or the same property on\ndifferent networks? \n\nThe tables look ugly.  It seems that the header \"data set\" should be \"approach\"\nor something similar. \n\nIn summary, I feel that while there are some issues with the paper, it presents\ninteresting results and can be accepted.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]