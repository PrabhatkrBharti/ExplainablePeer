This paper proposes a fast way to learn convolutional features that later can be used with any classifier.  The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.  \nIn the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets. \n\nPros:\nThe paper compares different classifiers on three datasets. \n\nCons:\n- Considering an adaptive schedule of the learning decay is common practice in modern machine learning.  Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch. \n- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers. \n- There are many spelling errors \n- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore.  It is well known that CNN features are much better if enough data is available.\n [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]