The authors describe a mechanism for defending against adversarial learning attacks on classifiers.  They first consider the dynamics generated by the following procedure.  They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating.   \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent.  They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach.  \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples).  In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps.  They conclude with empirical observations about the performance of this algorithm. \n\nThe paper is well-written and easy to follow.  However, I found the empirical results to be a little underwhelming.  Sens-FGSM outperforms the adversarial training defenses tuned for the \u201cwrong\u201d iteration, but it does not appear to perform particularly well with error rates well above 20%.  How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)?  Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM?  It is my understanding that Sens-FGSM is not trained to a particular iteration of the \u201ccat-and-mouse\u201d game.  Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81?  With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.),;  thus I would liked to see more thorough experiments here as well.  For example, it\u2019s unlikely that the defender would ever know the attack network utilized by an attacker.  How robust is the defense against samples generated by a different attack network?  The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying.  Any defense can be *evaluated* against samples generated by any attacker strategy.  Is it the case that the defenses fall flat against samples generated by different architectures?  \n\n\nMinor Comments:\nSection 3.1, First Line. \u201df(ul(g(x),y))\u201d appears to be a mistake[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]