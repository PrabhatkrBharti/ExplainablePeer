This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL.  Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better.   The results is interesting and novel.   This paper should be accepted.  \n\nThe presentation of the paper can be improved a bit.   Much of the notation introduced in section 3.1 is not used later on.   There seems to be a bit of a disconnect before and after section 3.3.  The algorithm in deep RL could be explained a bit better. \n\nThere are some papers that could be connected.  Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments. \n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals. \n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train.  An experiment illustrating this effect could be illuminating. \n\nCan the proposed approach help when we have recurrent value functions?  Since recurrence does not guarantee that all information needed is captured. \n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al. \n\nPotential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]