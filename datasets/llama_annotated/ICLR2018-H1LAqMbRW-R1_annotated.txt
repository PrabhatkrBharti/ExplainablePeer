The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning. \nThe idea of re-using a pretrained agent has both pros and cons.  On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment.  On the other hand, the usefulness of the learned representation for planning is unclear.  A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions. \nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation. \u2028In addition to that, one could fine-tune the representation during forward model training.  \nIt would be interesting to see if this can improve the results. \n\nI personally miss a more technical and detailed exposition of the ideas.  For example, it is not described anywhere what loss is used for learning the model.  MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.  \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way. \nI also could find the details on how figure 1 was produced . As I understand, MCTS was not used in this experiment.  If so, how would one play with just a forward model? \n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability.  Is mini-RTS a deterministic environment?  \nWould it be possible to include a non-deterministic baseline in the experimental comparison? \n\nExperimentally, the results are rather weak compared to pure model-free agents.  Somewhat unsatisfying, longer-term prediction results into weaker game play.  Doesn\u2019t this support the argument about need in stochastic prediction?  \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance.  \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names \n* Figure 1 that introduces them contains typos.  \n* Formatting of figure 8 needs to be fixed.  This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEG]]