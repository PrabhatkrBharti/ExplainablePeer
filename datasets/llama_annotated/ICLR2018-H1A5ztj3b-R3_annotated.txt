This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting.  It tries to provide an explanation for the phenomenon and a procedure to test when it happens.  However, I don't find the paper of high significance or the proposed method solid for publication at ICLR. \n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers.  The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models.  Also, the authors do not give a conclusive analysis under what condition it may happen. \n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.  I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.[[CLA-POS],[JUS-POS],[DEP-NEG],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEG]]