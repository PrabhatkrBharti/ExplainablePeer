The idea of the paper is to use a GAN-like training to learn a novelty detection approach.  In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution.  The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers . To establish such a behavior, the authors propose early stopping as well as other heuristics.  \n\nI like the idea of the paper,;  however, this paper needs a revision in various aspects, which I simply list in the following:; \n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \"inlier accepted fraction\" is not well motivated in my opinion.  This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data.  The latter is important for the GAN-like training. \n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison.  \n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5) \n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators) \n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?) \n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ()) \n* The term \"AI-related task\" sounds a bit too broad \n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection. \n* Where is Table 1? \n* There are quite a lot of typos. \n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating.  The clarity of the paper has improved;  but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]