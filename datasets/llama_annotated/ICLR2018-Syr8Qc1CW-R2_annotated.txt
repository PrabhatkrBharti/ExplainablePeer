Summary:\nThis paper investigated the problem of attribute-conditioned image generation using generative adversarial networks.  More specifically, the paper proposed to generate images from attribute and latent code as high-level representation.  To learn the mapping from image to high-level representations, an auxiliary encoder was introduced.  The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss.  To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline.   Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.  \n\n== Novelty and Significance ==\nMulti-attribute image generation is an interesting task but has been explored to some extent.  The integration of generative adversarial networks with auto-encoding loss is not really a novel contribution.  \n-- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016. \n\n== Technical Quality == \nFirst, it is not clear how was the proposed annihilating operation used in the experiments (there is no explanation in the experimental section).  Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution).  I would appreciate the authors to elaborate this a bit. \n\nSecond, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network).  The proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribution. \n\nThird, it seems that the proposed multi-attribute generation pipeline works for binary attribute only.  However, such assumption limits the generality of the work.  Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes  and (2) not specific to face images. \n-- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015.\n-- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016. \n-- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.Yan et al., In ECCV 2016. \n\nAdditionally, considering the generation quality, the CelebA samples in the paper are not the state-of-the-art.  I suspect the proposed method only works in a more constrained setting (such as Multi-PIE where the images are all well aligned). \n\nOverall, I feel that the submitted version is not ready for publication in the current form.\n"[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]