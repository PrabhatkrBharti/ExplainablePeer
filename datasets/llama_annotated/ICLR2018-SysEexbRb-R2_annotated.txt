This paper studies the critical points of shallow and deep linear networks.  The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima.  Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices.  I have not checked the proofs in detail but the general strategy seems sound.  While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.  The authors also study the analytic form of critical points of a single-hidden layer ReLU network.  However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear. \n\n\nDetailed comments:\n\n- I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear.  This paper is mostly about linear networks.  While a result has been stated for single-hidden ReLU networks.  In my view this particular result is an immediate corollary of the result for linear networks.  As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear.  I would suggest rewording title/abstract/intro \n\n- Theorem 1 is neat, well done! \n\n- Page 4 p_i\u2019s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly).  \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc? \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful.  What insights do you gain by knowing these theorems etc.  Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one). \n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications?  Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound? \n\n- Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful.  e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]