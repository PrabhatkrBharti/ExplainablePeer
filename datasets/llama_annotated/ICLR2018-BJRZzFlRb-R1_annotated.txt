This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices.  The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks. \n\nOverall, the paper is well-written.  The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation. \n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer.  For example, the model does not guarantee to be able to convert resulting \"indices\" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector.  As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task. \nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well. \nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer.  There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies. \n\nFurthermore, I would like to see two additional analysis.  First, if we trained the proposed model with starting from \"zero\" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]