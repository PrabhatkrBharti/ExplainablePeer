The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy).  \n\nThe paper is well written, clear, organized and easy to follow. \n\nGiven that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons.  They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting.  \n\nAlthough this work and its results are very useful for practitioners,  it lacks in two aspects.  First, it only considers a single task for which GANs are very popular.  Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions.  Some of the conclusions could be further clarified with additional experiments (e.g., Sec 3.6 \u2018while the reason that RMS also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the ImageNet dataset\u2019).\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]