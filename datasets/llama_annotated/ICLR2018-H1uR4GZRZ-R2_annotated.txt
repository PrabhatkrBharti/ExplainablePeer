This paper propose a simple method for guarding trained models against adversarial attacks.  The method is to prune the network\u2019s activations at each layer and renormalize the outputs.  It\u2019s a simple method that can be applied post-training and seems to be effective.\ n\nThe paper is well written and easily to follow.  Method description is clear.  The analyses are interesting and done well.  I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods. \n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images.  Would be good to know if the authors have any intuition why is that the case. \n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text. \n\nI would be interested to know more about the intuition behind the proposed method.  It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method. \n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial. \n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014.  Would be good to see the performance against other forms of adversarial attacks as well if they exist[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]