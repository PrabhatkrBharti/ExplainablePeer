The paper consider a method for \"weight normalization\" of layers of a neural network.   The weight matrix is maintained normalized, which helps accuracy.   However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root). \n\n The paper proposes \"FastNorm\", which is a way to implicitly maintain the normalized weight matrix using much less computation.   Essentially, a normalization vector is maintained an updated separately.  \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n   Cons:   A very natural and simple solution that is fairly obvious. \n\n          Limited experiments \n\n[[CLA-NEG],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]