This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner.  The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers.  Using this system, the authors are able to harness much more compute to learn very high quality policies in little time.  The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow.  \n\nIt\u2019s hard to take issue with a paper that has such overwhelmingly convincing experimental results.  However, there are a couple additional experiments that would be quite nice:\n\u2022\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X).  \n\u2022\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time.  For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames? \n\nPros:\n\u2022\tWell written and clear .\n\u2022\tVery impressive results .\n\u2022\tIt\u2019s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm. \n\nCons:\n\u2022\tHard to replicate experiments without the deep computational pockets of DeepMind.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]