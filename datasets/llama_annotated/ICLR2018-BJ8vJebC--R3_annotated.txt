This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust. \n\nOverall the paper is a clearly written, well described report of several experiments.  It shows convincingly that standard NMT models completely break down on both natural \"noise\" and various types of input perturbations.  It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.  The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations. \n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.  The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps)  but the impact may be.  I wonder if you could put this in the context of \"training with input noise\", which has been studied in Neural Network for a while (at least since the 1990s).  I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise.  Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet. \n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \"noisy\" training set and adding that to clean data?  Or introducing noise \"on-line\" as part of the training?  If fixed, what sizes were tried?  More information on the experimental design would help. \n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \"Rand\" line for German, or the Swap/Mid/Rand lines for Czech.  It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance. \n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way. \n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]