Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs . The key idea is to maintain a chart to take into account all possible spans.  The paper also introduces an attention method over chart cells.  The experimental results show that the propped model outperforms tree-lstm using external parsers. \n\nComment: I kinda like the idea of using chart, and the attention over chart cells.  The paper is very well written. \n- My only concern about the novelty of the paper is that the idea of using CYK chart-based mechanism is already explored in Le and Zuidema (2015). \n- Le and Zudema use pooling and this paper uses weighted sum.  Any differences in terms of theory and experiment? \n- I like the new attention over chart cells.  But I was surprised that the authors didn\u2019t use it in the second experiment (reverse dictionary). \n- In table 2, it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm (0.3%) is \u201cgood enough\u201d.  In which cases the former did correctly but the latter didn\u2019t? \n- In table 3, what if we use the right-branching tree-lstm with attention? \n- In table 4, why do Hill et al lstm and bow perform much better than the others?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]