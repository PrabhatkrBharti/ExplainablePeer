The authors present a method to enable robust generation of adversarial visual\ninputs for image classification. \n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation. \n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution. \n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains. \n\nThe paper is clear to follow and the objective employed appears to be sound.  I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples. \n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each \n   experiment?  Is it a PCFG?  Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?)  And on a related note, how were the number of sampled \n   transformations chosen? \n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input. \n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective.  I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required. \n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\ n   what effect the monte-carlo approximation of the objective has on things. \n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment. \n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from.  There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one. \n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models?  Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason?  Similar questions for the 5\n   models in the 3D rendering experiment. \n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \"over a variety of viewpoints\".  Were these\n   viewpoints chosen randomly? \n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves.  For now, I think the\nsubmission is good for a weak accept  \u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]