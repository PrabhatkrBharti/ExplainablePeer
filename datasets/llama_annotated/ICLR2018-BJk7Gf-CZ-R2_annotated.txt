Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks.  In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion.  This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are .\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach. \n\nQuality:\nThe quality is very good.  The paper is technically correct and nontrivial.  All proofs are provided and easy to follow. \n\nClarity:\nThe paper is very clear.  Related work is clearly cited, and the novelty of the paper well explained.  The technical proofs of the paper are in appendices, making the main text very smooth. \n\nOriginality:\nThe originality is weak.  It extends a series of recent papers correctly cited.  There is some originality in the proof which differs from recent related papers. \n\nSignificance:\nThe result is not completely surprising,  but it is significant given the lack of theory and understanding of deep learning.  Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]