The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick.  The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.  \n\nThis paper is well-written and easy to follow.   The motivation is clear and the idea is simple and effective.\ n\nIt would be better to provide deeper analysis in Subsection 6.1.  The current analysis is too simple.  It may be interesting to explain the meanings of individual components.  Does each component is related to a certain topic?  Is it meaningful to perform ADD or SUBSTRACT on the leaned code?  \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]