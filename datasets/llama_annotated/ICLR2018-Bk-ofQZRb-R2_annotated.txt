This paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next state\u2019s value.  The constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next state\u2019s value.  It is shown empirically that the constrained update does not diverge on Baird\u2019s counter example and improves performance in a grid world domain and cart pole over DQN. \n\nThis paper is reasonably readable.  The derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potential. \n\nThe key issue is that the justification for the constrained gradients is lacking.  What is the effect, in terms of convergence, in modifying the gradient in this way?  It seems highly problematic to simply remove a whole part of the gradient, to reduce effect on the next state.  For example, if we are minimizing the changes our update will make to the value of the next state, what would happen if the next state is equivalent to the current state (or equivalent in our feature space)?  In general, when we project our update to be orthogonal to the maximal change of the next states value, how do we know it is a valid direction in which to update?  \n\nI would have liked some analysis of the convergence results for TD learning with this constraint, or some better intuition in how this effects learning.  At the very least a mention of how the convergence proof would follow other common proofs in RL.  This is particularly important, since GTD provides convergent TD updates under nonlinear function approximation; the role for a heuristic constrained TD algorithm given convergent alternatives is not clear.  \n \nFor the experiments, other baselines should be included, particularly just regular Q-learning.  The primary motivation comes from the use of a separate target network in DQN, which seems to be needed in Atari (though I am not aware of any clear result that demonstrates why, rather just from informal discussions).  Since you are not running experiments on Atari here, it is invalid to simply assume that such a second network is needed.  A baseline of regular Q-learning should be included for these simpler domains.  \n\nThe results in Baird\u2019s counter example are discouraging for the new constraints.  Because we already have algorithms which better solve this domain, why is your method advantageous?  The point of showing your algorithm not solve Baird\u2019s counter example is unclear. \n\nThere are also quite a few correctness errors in the paper, and the polish of the plots and language needs work, as outlined below.  \n\nThere are several mistakes in the notation and background section.  \n1. \u201cIf we consider TD-learning using function approximation, the loss that is minimized is the squared TD error. \u201c This is not true; rather, TD minimizes the mean-squared project Bellman error.  Further, L_TD is strangely defined: why a squared norm, for a scalar value?  \n2. The definition of v and delta_TD w.r.t. to v seems unnecessary, since you only use Q.  As an additional (somewhat unimportant) point, the TD-error is usually defined as the negative of what you have.  \n3. In the function approximation case the value function and q functions parameterized by \\theta are only approximations of the expected return. \n4. Defining the loss w.r.t. the state, and taking the derivative of the state w.r.t. to theta is a bit odd.  Likely what you meant is the q function, at state s_t?  Also, are ignoring the gradient of the value at the next step?  If so, this further means that this is not a true gradient.   \n\nThere is a lot of white space around the plots, which could be used for larger more clear figures.  The lack of labels on the plots makes them hard to understand at a glance, and the overlapping lines make finding certain algorithm\u2019s performance much more difficult.  I would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plots. \n\nExamples of odd language choices:\n\t-\t\u201cThe idea also does not immediately scale to nonlinear function approximation.  Bhatnagar et al. (2009) propose a solution by projecting the error on the tangent plane to the function at the point at which it is evaluated.  \u201c - The paper you give exactly solves for the nonlinear function approximation case.  What do you mean does not scale to nonlinear function approximation?  Also Maei is the first author on this paper. \n\t-\t\u201cThough they do not point out this insight as we have\u201d - This seems to be a bit overreaching. \n- \u201cthe gradient at s_{t+1} that will change the value the most\u201d  - This is too colloquial.  I think you simply mean the gradient of the value function, for the given s_t, but its not clear.[[CLA-NEG],[JUS-NEG],[DEP-NEG],[FAI-NEG],[CON-NEG],[ENG-NEG],[ACC-NEG],[CST-NEG],[NOV-NEG],[ETH-NEG]]