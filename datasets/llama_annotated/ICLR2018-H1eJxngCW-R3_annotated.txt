1) This paper proposes a new dataset for Reading Comprehension (RC).  Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly.  \n\nOverall, I think this dataset is very useful for RC.  The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs. \n\n2) I have the questions as follows:\ni) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots.  However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC.  Is there any mistake in the description of the abstract? \n\nii) What is the standard for creating the questions?  I noticed that the time and location information was used to generate questions sometime, but sometimes these kinds of questions are ignored. \n\niii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents?  \n\niv) What is the average length of the answers in both ParaphraseRC and SelfRC?  I found that the answers are usually very short, which is more like factoid QA.  It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge.  \n\nv) During NLP pre-processing (section 4), how do you prune the irrelevant documents?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-NEG],[ETH-NEG]]