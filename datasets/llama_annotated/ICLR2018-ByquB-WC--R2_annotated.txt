This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN).  RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017).  RMN reduces the complexity to linear time for the bAbi dataset.  RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning.  RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time. \n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning.  However, how widespread is this problem across other models or are you simply addressing a point problem for RN?  For example, Entnet is able to reason as the input is fed in and the decoding costs are low.  Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply.  \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP).  It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.  For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion.  More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights.  Showing results with multiple hops (1,2,..) would be useful here. \n\nMore details are needed about Figure 3.  Is this on bAbi as well?  How did you generate these stories with so many sentences?  Another clarification is the bAbi performance over Entnet which claims to solve all tasks.  Your results show 4 failed tasks, is this your reproduction of Entnet? \n\nFinally, what are the savings from reducing this time complexity?  Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops. \n\nOverall, this paper feels like a small improvement over RN.  Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.  One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]