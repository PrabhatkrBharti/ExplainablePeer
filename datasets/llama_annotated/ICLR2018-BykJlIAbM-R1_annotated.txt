This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences.  The idea of using similar sentence pairs as cluster-to-cluster translation is interesting.  \nThe experimental results seem promising, but the presentation can be improved.  Some parts of the paper are hard to read. \n \n \nMajor\n1. What is the model/baseline in Tables 3, 4, 5? \n2. What is the intuition in adding target cluster entropy in Eq. 3? \n3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from?  Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster? \n4. In section 4.2, the name cluster is a bit confusing with the one in section 3.1. What's the relationship? The symbols C(Y*) and C(X*) are not used afterward. \n5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by \"efficient\"? \n6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs? \n7. What's the reward used in the experiments ?\n8. In the Monte-Carlo sampling, how many pairs are sampled?  \n \nMinor \n1. In Table 1, where is sigma defined? \n2. The notation D for a dataset in Section 3.3 is confusing with D in system D. \n3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified. \n4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y). \n5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10. \n6. Some recent results on WMT DE-EN are missing, such ashttps://arxiv.org/abs/1706.03762.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]