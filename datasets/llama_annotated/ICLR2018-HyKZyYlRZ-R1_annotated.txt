The paper presents a multi-task, multi-domain model based on deep neural networks.  The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition.  The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic.  The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task. \n\nThe paper is well written and easy to follow. \n\nThe contributions of the paper are novel and significant.  The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring.  The experiments clearly show the viability of the approach and give interesting insights.  This is surely an important step towards more general deep learning models.  \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database.  Moreover, some databases could be used for different tasks, such as WSJ or ImageNet. \n\n* The training procedure of the model is not explained in the paper.  What is the cost function and what is the strategy to train on multiple tasks ?  The paper should at least outline the strategy. \n\n* The experiments are sufficient to demonstrate the viability of the approach,  but the experimental setup is not clear.  Specifically, there is an issue about the speech recognition part of the experiment.  It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ?  The metrics used in Table 1 are also not clear, they should be explained in the text.  Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used.  Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets). \n\n* Using raw waveforms as audio modality is very interesting,  but this approach is not standard for speech recognition,  some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26\u201330. \nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal.  In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE. \nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals.  Learning the Speech Front-end With Raw Waveform CLDNNs.  Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015. \n\nRevised Review:\nThe main idea of the paper is very interesting and the work presented is impressive.  However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks.  The experiments are interesting,  except for the WSJ speech task, which is almost meaningless.  Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition. \nI thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition.  A simpler speech task such as Keyword Spotting could also be investigated.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]