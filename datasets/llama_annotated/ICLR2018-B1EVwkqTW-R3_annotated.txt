Summary: \nThe paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an SVM model.  The SVM is trained for the final classification task at hand using the last layer features of the deep network.  The motivation behind all this is to learn the input features to the SVM as opposed to hand-crafting them, and use the generalization ability of the SVM to do well on tasks which have only a handful of training examples.  The authors apply their technique to two datasets, namely, the Omniglot dataset and the TIMIT dataset and show that their model does a reasonable job in these two tasks.  \n\nWhile the paper is reasonably clearly written and easy to read  I have a number of objections to it.  \n\nFirst, I did not see any novel idea presented in this paper.  Lots of people have tried pre-training a neural network on auxiliary task(s) and using the features from it as input to the final SVM classifier.  People have also specifically tried to train a siamese network and use its features as input to the SVM.  These works go way back to the years 2005 - 2007, when deep learning was not called deep learning.  Unless I have missed something completely, I did not see any novel idea proposed in this paper.  \n\nSecond, the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach.  For example, the authors compare their model against rather weak baselines.  I would have liked the experiments to be more thorough, with comparison to the state of the art models for the two datasets. \n"[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]