This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017).  Differently from (Yogatama et al, 2017), this paper doesn\u2019t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way.  \n\nOverall, I think it is really an interesting direction and the proposed method sounds reasonable.  However, I am concerned about the following points:  \n\n- The improvements are really limited on both the SNLI and the Reverse Dictionary tasks.  (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it\u2019d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold.  Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures. \n\n- The computational complexity of this model shouldn\u2019t be neglected.  If I understand it correctly, the model needs to compute O(N^3) LSTM compositions.  This should be at least discussed in the paper.  And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).\ n\n- I am wondering about the effects of the temperature parameter t. Is that important for training? \n\nMinor:\n- What is the difference between LSTM and left-branching LSTM? \n- I am not sure if the attention overt chart is a highlight of the paper or not.  If so, better move that part to the models section instead of mention it briefly in the experiments section.  Also, if any visualization (over the chart) can be provided, that\u2019d be helpful to understand what is going on. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]