The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network.  The basic problem is empirical risk minimization with a incremental penalty for each non zero weight . To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables.  The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights.  The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training.  \n\nThe results presented in the paper are convincing . They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time.  They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models.  The hard concrete distribution is a small but nice contribution on its own. \n\nMy only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832).  Although the focus is apparently different, these methods are clearly closely related . A discussion of this relationship seems really important.\ n\nSpecific comments/questions:\n- The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this.  For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation.  This would give a stronger sense of the kind of wins that are possible in this framework .\n- Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations.  Extra evaluations of different relaxations would be appreciated.  At the very least a comparison to concrete would be nice. \n- In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]