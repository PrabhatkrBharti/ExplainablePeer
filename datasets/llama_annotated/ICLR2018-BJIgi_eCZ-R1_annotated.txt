The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \"fully-aware\" of all levels of abstraction, e.g. word-level, phrase-level, etc.  In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD.  They also propose an attention mechanism that works better than others (Symmetric + ReLU). \n\nStrengths:\n- The paper is well-written and clear. \n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field. \n- The multi-level attention is novel and indeed seems to work, with convincing ablations. \n- Nice engineering achievement, reaching the top of the leaderboard (in early October). \n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances.  Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA). \n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard.  Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution. \n\n\nMinor:\n- Probably figure 4 can be drawn better.  Not easy to understand nor concrete. \n- Section 3.2 GRU citation should be Cho et al. [2]. \n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.  Could you perform ablation without contextualized embedding (CoVe)? \n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.  EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]