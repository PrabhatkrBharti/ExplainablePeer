The authors has addressed my concerns, so I raised my rating.  \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. \n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens.  The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus.  \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning.  Results on this data might be more convincing. \n\nThe results of MOS is very good,  but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15.  This will make it less usable, I think it's necessary to provide the training time comparison. \n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER.  \n\n[1] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom.  \"On the state of the art of evaluation in neural language models. \" arXiv preprint arXiv:1707.05589 (2017). \n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342 \n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]