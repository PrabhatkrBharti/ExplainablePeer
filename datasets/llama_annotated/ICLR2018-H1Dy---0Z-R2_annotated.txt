A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory.  Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines. \n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial).  I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA).  Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so! \n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance) \nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus! \n\nMinor points:\n- Figure 1 does not seem to be referenced in the text  \n- \u00ab In principle, Q-learning variants are off-policy methods \u00bb => not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying \u00ab furthermore we are using a multi-step return \u00bb is too vague) \n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics. \n- In caption of Fig. 5 25K should be 250K \n- In appendix A why duplicate memory data instead of just using a smaller memory size? \n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme? \n- Why store rewards and gamma\u2019s at each time step in memory instead of just the total discounted reward? \n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix \n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEG]]