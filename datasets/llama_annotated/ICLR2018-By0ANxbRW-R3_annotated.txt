1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter.  However, all compression methods such as pruning and quantization also have this concern.  For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved.  Therefore, the novelty of the proposed method is somewhat weak. \n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].  However, the paper only did a very simple investigation on related works. \n[r1] CNNpack: packing convolutional neural networks in the frequency domain. \n[r2] LCNN: Lookup-based Convolutional Neural Network. \n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks. \n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.  It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]