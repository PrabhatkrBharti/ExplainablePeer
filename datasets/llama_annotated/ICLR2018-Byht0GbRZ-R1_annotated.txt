Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences.  The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.  Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016). \n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting.  \n\nWeaknesses:\nThe paper is 8.5 pages long .\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017) \n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive.  Am I missing something about the algorithm? \n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax.  The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]