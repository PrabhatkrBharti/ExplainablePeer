This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments. \n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm.   They extend the scheme to allow the use of different scaling parameters and to m-bit quantization.  Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods.  \n\nThe experiments are complete and the writing is good. \n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b) . Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]