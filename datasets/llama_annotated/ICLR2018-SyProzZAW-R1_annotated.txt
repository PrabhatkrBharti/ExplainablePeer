Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks.  \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c.  \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables.  The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2).  \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation).  \nOverall, I like the paper. \n\nPros: new and interesting result, theoretically sound.  \nCons: nothing major. \nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?  Could you comment on this problem? \n* Is the assumption that \\sigma has Taylor expansion to order d tight?  (That is, are there counter examples for relaxations of this assumption?)  \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs,  Could you provide some further comments on this?\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]