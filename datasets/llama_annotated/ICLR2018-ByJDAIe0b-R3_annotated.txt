This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks.  The essential problems here are how to identify which states should be stored and how to retrieve memory during action prediction.  The proposed architecture could identify the \u2018key\u2019 states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory.  The weight assigning (write) network is optimized for maximize the expected rewards.  This article focuses on the calculation of gradient for write network, and provides some mathematical clues for that. \n\nThis article compares their proposed architecture with RNN (GRU with 10 hidden unit) in few toy tasks.  They demonstrate that proposed model could work better and rational of write network could be observed.  However, it seems that hyper-parameters for RNN haven\u2019t been tuned enough.  It is because the toy task author demonstrates is actually quite similar to copy tasks, that previous state should be remembered.  To my knowledge, copy task could be solved easily for super long sequence through RNN model.  Therefore, empirically, it is really hard to justify whether this proposed method could work better.  Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps.  \n\nAccording to that, the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEG]]