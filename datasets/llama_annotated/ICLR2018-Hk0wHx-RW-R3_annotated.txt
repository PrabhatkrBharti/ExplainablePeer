[====================================REVISION ======================================================]\nOk so the paper underwent major remodel, which significantly improved the clarity.  I do agree now on Figure 5, which tips the scale for me to a weak accept.  \n[====================================END OF REVISION ================================================]\n\nThis paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning.  Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib.  They then go on to explore the sparsity of the latent space \n\nMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me.  On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does) \n\nMinor comments\nPage 1: forcing an compact -> forcing a compact\n\u201cand and\u201d =>and\nSection 2: mention that I is mutual information, it is not obvious for everyone\n\nFigure 3: circles/triangles are too small, hard to see \nFigure 5: not really convincing.  B does not appear much more structured than a, to me it looks like a simple transformation of a.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]