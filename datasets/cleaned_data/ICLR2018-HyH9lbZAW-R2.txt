 "This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents. \nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components.  The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms.  Overall, I think that the paper proposes some\ninteresting ideas,  however, in its current form I do not think that the novelty\nof the contributions are clearly presented and that they are not thoroughly\nevaluated in the experiments. \n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components.  However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm.  As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution  and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters.  These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017), \nbut I think the authors may be overselling these ideas as a brand new algorithm. \nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better. \n\nAnother concern that I have is that there are a lot of conditiona-conjugacy\nassumptions baked into the algorithm that the authors only mention at the end\nof the presentation of their algorithm.  Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation.  Though one could do\nthis, the authors do not adequately show that one should, or that one can do this\nwithout suffering a lot of error in the posterior approximation.  I think that\nwithout an experiment the small section on non-conjugacy should be removed. \n\nFinally, I found the experimental evaluation to not thoroughly demonstrate the\nadvantages and disadvantages of the proposed algorithm.  The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels.  The experiments do not however demonstrate why the algorithm is\nperforming better.  For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient?  It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm. \n\nI also have some minor comments on the paper:\n- There are a lot of typos. \n- The first two sentences of the abstract do not really contribute anything to the paper.  What is a powerful model?  What is a powerful algorithm? \n- DNN was used in Section 2 without being defined. \n- Using p() as an approximate distribution in Section 3 is confusing notation \n  because p() was used for the distributions in the model.