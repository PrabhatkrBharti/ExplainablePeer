 "This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators).  The authors provide both theoretical and experimental validation of their idea.  \n\nPros:\n+ The paper is generally very clearly written.  The motivation, notation, and method are clear. \n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases.  \n+ The approach is a novel twist on an existing method for learning from noisy data.  \n\nCons: \n- All experiments use simulated workers; this is probably common but still not very convincing. \n- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. \"To re (label), or not to re (label).\" HCOMP 2014. \n- The authors should have compared their approach to the \"base\" approach of Natarajan et al.  \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these. \n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture. \n\nQuestions: \n- How would the model need to change to account for example difficulty?  \n- Why are Joulin 2016, Krause 2016 not relevant? \n- Best to clarify what the weights in the weighted sum of Natarajan are.  \n- \"large training error on wrongly labeled examples\" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels?  Where does this ground truth come from? \n- Not clear what \"Ensure\" means in the algorithm description. \n- In Sec. 4.4, why is it important that the samples are fresh?