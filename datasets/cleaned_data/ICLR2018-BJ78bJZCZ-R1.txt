 "The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor.  While the RWA was an interesting idea  with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods. \n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original.  On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU  - except for MultiCopy where it trains faster,  but not to better results and it looks like the difference is between few and very-few training steps anyway.  The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM.  So the results are not strongly convincing, and the paper lacks any mention of newer work on attention.  This year strong improvements over state-of-the-art have been achieved using attention for translation (\"Attention is All You Need\") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition).  To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks."