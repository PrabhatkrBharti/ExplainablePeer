 "Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output. \n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\ n\nCons\n- Sections 3.1 and 3.2 are hard to understand.  There seem to be inconsistencies\n  in the notation.  For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1].  The usage is inconsistent. \n(2) It is not clear how \"y^b_n can be expressed as \\sum_{k=1}^K z_{nk}f_k(x_n)\"\nin general.  This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity.  However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit \n(\"The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\ ")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \"center\" ?  I could not understand in what sense is\nthis the center, and of what ? It seems that the max value has been subtracted\nfrom all the logits into a softmax (which is a fairly standard operation). \n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class.  This does not address the issue of understanding why the\nnetwork makes a certain prediction for a particular input.  Therefore this\napproach will be of limited use.\ n\n- The paper should include more analysis of how this method helps interpret the\n  actions of the neural net, once the core units have been identified. \nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked. \nHowever, there is not enough analysis on showing whether and how the identified\nhidden units help \"interpret\" the model.\ n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail.  However, the writing still needs to be polished. \n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\ n\nOriginality\nThis approach to finding important hidden units is novel .\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks.  However, it only identifies hidden units that are important for\na class, not what are important for any particular input.   Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers.  However, the experiments only focus on identifying important hidden\nunits and fall short of actually providing an interpretation using these hidden\nunits."