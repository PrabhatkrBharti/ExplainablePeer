 "Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field.  This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.  The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set.  By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT.  The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting.  Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin.  Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop. \n\nThe proposed algorithm is new and writing is clear.  However, the paper is not flawless.  The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning.  To validate such theoretical result, a non-deep-learning model should be adopted.  The ERM for active learning has been investigated in the literature, such as \"Querying discriminative and representative samples for batch mode active learning\" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper.  Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough.  The authors should provide more competing algorithms in batch mode active learning."