 "In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner.  The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus.  \n\nOverall, the descriptions of the proposed model (Section 3 - Section 5) are hard to follow.  This is not because the proposed model is technically difficult to understand.  On the contrary, the model is heuristic, and simple, but the descriptions are unclear.  Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text).  However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used.  Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations.  However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions.  Moreover, the motivations behind the proposed methods for different components are missing.   Also, a lot of types make the descriptions more difficult to follow, e.g., \"may not helpful or even harmful\", '\"Figure 3\", \"we show this Section 6\", \"large size a vocabulary\", etc. \n\nAnother major concern is that the technical contributions of the proposed model is quite limited.  The only technical contributions are (4) and the way to construct the co-occurrence information A.  However, such contributions are quite minor, and technically heuristic.  Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering.  In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network. \n\nRegarding experiments, one straight-forward baseline is missing.  As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain.  For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight.