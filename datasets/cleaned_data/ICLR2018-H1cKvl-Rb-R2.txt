 "The authors propose a new exploration algorithm for Deep RL . They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style. \n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper.\ n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper).  Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise?  \n-How does the distribution of Q values look like during different phases of learning? \n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2. \n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)? \n\nMinor things:\n-Missing propto in Eq 7 ?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n"