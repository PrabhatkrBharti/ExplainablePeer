 "This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based).  The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform.  This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline.  The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset. \n\n(significance) This is a promising idea.  This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation.  Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard. \nUnfortunately, I do not believe the paper is ready for publication as it stands, as it suffers from lack of clarity and the experimentation is limited in scope. \n\n(clarity) While Section 3.3 clearly defines the explicit form of the algorithm (where data and labels are essentially pre-processed via a copula transform), details regarding the \u201cimplicit form\u201d are very scarce.  From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder $f_\\beta(x)$ which gaussianizes the input (thus emulating the explicit transform) ?  Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ?  There are also many missing details in the experimental section: how were the number of \u201cactive\u201d components selected ?  Which versions of the algorithm (explicit/implicit) were used for which experiments ?  I believe explicit was used for Section 4.1, and implicit for 4.2 but again this needs to be spelled out more clearly.  I would also like to see a discussion (and perhaps experimental comparison) to standard preprocessing techniques, such as PCA-whitening. \n\n(quality) The experiments are interesting and seem well executed.  Unfortunately, I do not think their scope (single synthetic, plus a single UCI dataset) is sufficient.  While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset.  How does this method perform for more realistic data, even e.g. MNIST ?  I think it is crucial to highlight that the deficiencies of DIB matter in practice, and are not simply a theoretical consideration.  Similarly, the representation analyzed in Figure 7 is promising,but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper.  I would have also liked to see a more direct and systemic validation of the claims made in the paper.  For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x.  A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion. \n\nPros:\n* Theoretically well motivated\n* Promising results on synthetic task\n* Potential for impact\n Cons:\n* Paper suffers from lack of clarity (method and experimental section) \n* Lack of ablative / introspective experiments\n* Weak empirical results (small or toy datasets only)."