 "This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization.  However, the results do not convince this reviewer to switch to using 'post-training'. \n\nLearning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago.  However, freezing the layers and continue to train the last layer is of a minor novelty.  The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline.  c.f. Eq 3 and Eq 4.\nTherefore, it's unclear whether the gain in generalization is due to an additional \\lambda term or from the post-training training itself. \n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise. \n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments. \n\nFor table 1, please use decimal points instead of commas.\n"