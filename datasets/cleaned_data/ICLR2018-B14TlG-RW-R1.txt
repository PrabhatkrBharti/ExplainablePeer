 "Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention.  The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference.  Authors also propose a paraphrasing based data augmentation method which helps in improving the performance.  Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference. \n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written. \n\n1. Why don\u2019t you report your model performance without data augmentation in Table 1?  Is it because it does not achieve SOTA?  The proposed data augmentation is a general one and it can be used to improve the performance of other models as well.  So it does not make sense to compare your model + data augmentation against other models without data augmentation.  I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models. \n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n 3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset.  That will show the generality of the proposed architecture.  Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision. \n\n4. Are you willing to release your code to reproduce the results?\ n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\ n2. In the first contribution bullet point, \u201cthat exclusive built upon\u201d should be \u201cthat is exclusively built upon\u201d.\n"