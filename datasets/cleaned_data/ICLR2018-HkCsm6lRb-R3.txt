 "The paper proposes a method for generating images from attributes.  The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples , and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples.  During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa.  The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution.  The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e.  if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes.  Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided,  it outperforms when some of the attributes are missing (Figure 4a).  It also outperforms existing methods in terms of coverage and compositionality. \n\nMajor comments:\n\nThe paper is well written, and summarizes its contribution succinctly. \n\nI did not fully understand the 'retrofitting' idea.  If I understood correctly, the authors first train \\theta and \\phi and then fix \\theta to train \\phi_x and \\phi_y.  If that is true, then is \\calL(\\theta, \\phi, \\phi_x, \\phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing \\theta? Please clarify? \n\nMinor comments:\n\n- 'in order of increasing abstraction', does the order of gender-> smiling or not -> hair color matter? Or, is male, *, blackhair a valid option?\n\n-  what are the image sizes for the CelebA dataset\n\n- page 5: double the\n\n - Which multi-label classifier is used to classify images in attributes?"