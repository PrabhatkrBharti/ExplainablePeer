 "Summary\nThe paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated updates.  The architecture consists of a \u2018slow\u2019 network which provides weight updates for the \u2018fast\u2019 network which outputs the predictions of the system.  The experiments show that the architecture outperforms a couple of related models on an associative retrieval problem. \n\nQuality\nThe authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in Danihelka et al. (2016).  The difference with the original task seems to be that the network is also trained to predict a \u2018blank\u2019 symbol which indicates that no prediction has been made.  While this task is artificial, it does make sense in the context of what the authors want to show.  The fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models, adds to the quality of the experiment.  It is somewhat unfortunate that the paper doesn\u2019t give more detail about the precise hyper-parameters involved and that there is no comparison with the associative LSTM from Danihelka et al.  Did these hyper-parameters also include the sizes of the models?  Otherwise it\u2019s not very clear to me why the numbers of parameters are so much higher for the baseline models.  While I think that this experiment is well done, it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of tasks.  It is commendable that the authors also discuss the memory requirements and increased wall clock time of the model. \n\nClarity\nI found the paper hard to read at times and it is often not very clear what the most important differences are between the proposed methods and earlier ones in the literature.  I\u2019m not saying those differences aren\u2019t there, but the paper simply didn\u2019t emphasize them very well and I had to reread the paper from Ba et al. (2016) to get the full picture.   \n\nOriginality/Significance\nWhile the architecture is new, it is based on a combination of previous ideas about fast weights, hypernetworks and activation gating and I\u2019d say that the novelty of the approach is average.  The architecture does seem to work well on the associative retrieval task, but it is not clear yet if this will also be true for other types of tasks.  Until that has been shown, the impact of this paper seems somewhat limited to me. \n\nPros\nExperiments seem well done. \nGood baselines. \nGood results. \n\nCons\nHard to extract the most important changes from the text. \nOnly a single synthetic task is reported.\n\n