 "This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input.  The model achieves good results on bAbI compared to memory networks and the relation network model.  From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question).  This average is then fed to a softmax layer for answer prediction.  I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks.  Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)?  The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments.  I'd encourage the authors to do a more detailed experimental study with more tasks,;  but I can't recommend this paper's acceptance in its current form. \n\nother questions / comments:\n- \"we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation? \n- writing could be cleaned up for spelling / grammar (e.g., \"last 70 stories\" instead of \"last 70 sentences\"), currently the paper is very hard to read and it took me a while to understand the model"