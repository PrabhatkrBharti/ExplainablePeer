 "The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced.  The paper proposes a Bayesian framework to address it with a Gaussian mixture model. \n\nOverall the math looks reasonable.  I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math.  Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1).  This has been used in multi-class classification before.  For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment).\ n\nNote that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes.  This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue. \n\nI am quite a bit concerned about the experimentation protocol as well.  The datasets are relatively smaller scale, and datasets such as MNIST and CIFAR are known to overfit.  As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST).  Regardless, the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but I am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method."