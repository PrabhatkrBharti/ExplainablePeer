 "The paper proposes and evaluates a method to make neural networks for image recognition color invariant. \n\nThe contribution of the paper is: \n - some proposed methods to extract a color-invariant representation \n - an experimental evaluation of the methods on the cifar 10 dataset \n - a new dataset \"crashed cars \"\n - evaluation of the best method from the cifar10 experiments on the new dataset \n\nPros: \n - the crashed cars dataset is interesting.  The authors have definitely found an interesting untapped source of interesting images. \n\n\nCons: \n- The authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network.  \n- The paper is incomplete without the appendices.  In fact the paper is referring to specific figures in the appendix in the main text. \n - the authors define color invariance as a being invariant to which specific color an object in an image does have, e.g. whether a car is red or green, but they don't think about color invariance in the broader context - color changes because of lighting, shades, .....  Also, the proposed methods aim to preserve the \"colorfullness\" of a color.  This is also problematic, because while the proposed method works for a car that is green or a car that is red, it will fail for a car that is black (or white) - because in both cases the \"colorfulness\" is not relevant.  Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black.  \n- the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for \"ord\" the input is a WxHx1 matrix, but for \"ord\" and \"cifar\" the input is a \"WxHx3\" matrix.  This probably leads to more parameters in the convolutions.  \n- the results in the  figure 4: it's very unlikely that the differences reported are actually significant.  It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best.  The proposed method does not lead to any relevant improvement. \nFigure 6/7: are very hard to read. I am still not sure what exactly they are trying to say. \n\nMinor comments: \n - section 1: \"called for is network\" -> called for is a network \n - section 1.1: And and -> And \n - section 1.1: Appendix -> Appendix C\n - section 2: Their exists many -> There exist many\n - section 2: these transformation -> these transformations\n - section 2: what does \"the wallpaper groups\" refer to? \n - section 2: are a groups -> are groups\n - section 3.2: reference to a non-existing figure\n - section 3.2/Training: 2499999 iterations = steps? \n - section 3.2/Training: longer as suggested -> longer than suggested