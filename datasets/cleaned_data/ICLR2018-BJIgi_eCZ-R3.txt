 "(Score before author revision: 4)\n(Score after author revision: 7)\n\nI think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly.  I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference). \n\nTherefore I would now support accepting this paper. \n\n------------(Original review below) -----------------------\n\nThe authors present an enhancement to the attention mechanism called \"multi-level fusion\" that they then incorporate into a reading comprehension system.  It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores. \n\ni.e. the authors form a vector \"HoW\" (called history of the word), that is defined as a concatenation of several vectors:\n\nHoW_i = [g_i, c_i, h_i^l, h_i^h]\n\nwhere g_i = glove embeddings, c_i = COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word. \n\nThe attention score is then a function of these concatenated vectors i.e. \\alpha_{ij} = \\exp(S(HoW_i^C, HoW_j^Q)) \n\nResults on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match).  The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers. \n\nThe authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best.  \n\nComments:\n\n-I feel overall the contribution is not very novel.   The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset (e.g. some combination of attention between question/context and LSTMs over question/context).  The only novelty is these \"HoW\" inputs to the extra attention mechanism that takes a richer word representation into account. \n\n-I feel the model is seems overly complicated for the small gain (i.e. 75.7->76.0 Exact Match), especially on a relatively exhausted dataset (SQuAD) that is known to have lots of pecularities (see anonymous comment below).  It is possible the gains just come from having more parameters. \n\n-The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. \\alpha_{ij}^l, \\alpha_{ij}^h, \\alpha_{ij}^u) it will learn to attend to \"different regions for different level\".  However, there is nothing enforcing this and the gains just probably come from having more parameters/complexity."