 "The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training. \n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way \n\nThe paper is fairly clear and these extensions are reasonable .  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact .  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.  So, making improvements to help solve grid-worlds better is not so motivating   The work on dynamic environments was an interesting step:  it would have been interesting to see how the \"models\" learned for the dynamic environments differed from those for static environments.