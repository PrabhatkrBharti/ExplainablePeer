 "The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results.  The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function. \n\nA second part of the paper looks at whether explanations are global or local.  The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case.  The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets.  Results further outline the resemblance between the compared methods. \n\nIn the appendix, the last step of the proof below Eq. 7 is unclear.  As far as I can see, the variable g_i^LRP wasn\u2019t defined, and the use of Eq. 5 to achieve this last could be better explained.  There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers."