 "Summary: This paper proposes a different approach to deep multi-task learning using \u201csoft ordering. \u201d  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across.  Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks.  The idea of \u201csoft ordering\u201d enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular.  \n\nThe methods reviewed prior work which the authors refer to as \u201cparallel order\u201d, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn\u2019t be the case.  They authors then extend the approach to \u201cpermuted order\u201d and finally present their proposed \u201csoft ordering\u201d approach.  The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance.  \n\nThe \u201csoft ordering\u201d approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task.  The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to \u201cparallel ordering\u201d and \u201cpermuted ordering\u201d and show the performance gain. \n\nPositives: \n- The paper is clearly written and easy to follow. \n- The idea is novel and impactful if its evaluated properly and consistently.  \n- The authors did a great job summarizing prior work and motivating their approach. \n\nNegatives: \n- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation).  CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection.  \u2028\n(TODO) I would recommend that the authors test their approach on such setting. \n- Figure 6 is a bit confusing, the authors do not explain why the \u201cPermuted Order\u201d performs worse than \u201cParallel Order\u201d.  Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task.  \n\u2028(TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader. \n- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)?  How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL. \u2028\n(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare. \n\nNotation and Typos:\n- Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify. \n1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order. \n\u20282- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense. \n- Figure 4 is very small and not easy to read the text.  Does single task mean average performance over the tasks?  \n- In eq.(3) Choosing \\sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol. \n\u2028Conclusion: I would suggest that the authors address the concerns mentioned above.  Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication.