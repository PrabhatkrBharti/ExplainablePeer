 "This paper proposes an interesting variational posterior approximation for the weights of an RNN.  The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN.  \n\npros:\n--I liked the posterior sharpening idea.  It was well motivated from a computational cost perspective hence the use of a hierarchical prior.  \n--I liked the uncertainty analysis.  There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights.  These works can benefit from the uncertainty analysis scheme introduced in this paper. \n--The experiments were well carried through. \n\ncons:\n--Change the title! the title is too vague.  \"Bayesian recurrent neural networks\" already exist and is rather vague for what is being described in this paper. \n--There were a lot of unanswered questions:\n (1) how does sharpening lead to lower variance?  This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section \n(2) how is the level of uncertainty related to performance?  It would have been insightful to see effect of \\sigma_0 on the performance rather than report the best result.  \n(3) what was the actual computational cost for the BBB RNN and the baselines? \n--There were very minor typos and some unclear connotations.  For example there is no such thing as a \"variational Bayes model\". \n\nI am willing to adjust my rating when the questions and remarks above get addressed."