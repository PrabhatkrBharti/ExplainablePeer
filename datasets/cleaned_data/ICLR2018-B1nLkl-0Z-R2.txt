 "The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.   It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.   Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG. \n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.   The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.   The inclusion of proof summaries in the main text would strengthen this aspect of the paper. \n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.   In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact. \n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance."