 "The paper proposes a \u2019Cross View training\u2019 approach to semi-supervised learning.  In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model.  The auxiliary softmax layers take different views of the input for prediction. \n\nPros:\n1. A simple approach to encourage better representations learned from unlabeled examples.  \n\n2. Experiments are comprehensive. \n\nCons:\n\n0. The whole paper just presented strategies and empirical results.  There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? Why?   \n\n1. The addition of auxiliary layers improves Sequence Tagging results marginally.  \n\n2. The claim of cross-view for sequence tagging setting is problematic.  Because the task is per-position tagging, those added signals are essentially not part of the examples, but the signals of its neighbors.  \n\n3. Adding n^2 linear layers for image classification essentially makes the model much larger.  It is unfair to compare to the baseline models with much fewer parameters.  \n\n4. The \"CVT, no noise\" should be compared to \"CVT, random noise\", then to \"CVT, adversarial noise\". The current results show that the improvements are mostly from VAT, instead of CVT. \n\n\n"