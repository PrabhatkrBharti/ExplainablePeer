 "The paper is well motivated and written.  However, there are several issues. \n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems.  Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks.  The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel . Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)?  \n1b. Why should we keep increasing the regularization constant beyond a limit?  Is this for compressing the networks (for which there are alternate procedures), or anything else.  In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers?  Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)?  \n2. The proposed experiments are not very conclusive.  Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks.  Secondly, more datasets including imagenet needs to be tested.  Unless these two are done, we cannot assertively say that the proposal seems to do interesting things.  Thirdly, it is not clear what Figure 5 means in terms of goodness of learning.  And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other).