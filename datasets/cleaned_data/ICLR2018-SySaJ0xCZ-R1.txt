 "This paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources.  The high-level idea is similar to the evolution method of [Real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch. \n\nCompared to other papers on neural architecture search, the required computational resource is impressively small: close to state-of-the-art result in one day on a single GPU.  However, it is not clear to me what contribute to the massive improvement of speed.  Is it due to the network morphing that preserve equality?  Is it due to a good initial network structure?  Is it due to the well designed mutation operations?  Is it due to the simple hill climbing procedure (basically evolution that only preserve the elite)?  Is it due to a well crafted search space that is potentially easier? \n\nThe experiments in this paper does not provide enough evidence to tease apart the possible causes of this dramatic reduction on computational resources.  And the comparisons to other papers seems not fair since they all operate on different search space.  \n\nIn summary, getting net2net to work for architecture search is interesting.  And I love the results.  These are very impressive numbers for neural architecture search.  However, I am not convinced that the improve is resulted from a better algorithm.  I would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resources."