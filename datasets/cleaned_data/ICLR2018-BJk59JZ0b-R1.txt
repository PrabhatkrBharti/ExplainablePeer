 "\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL.   They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic).  They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best). \n\nThe paper has a technical focus. \n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches \n\n- Compared to many existing techniques, on 9 tasks \n\ncons:\n\n- no mention of time costs, except that for more samples, S > 1, for\n taylor approximation, it can be very expensive. \n\n- one would expect more information to strictly improve performance,\n  but the results are a bit mixed (perhaps due to convergence to local optima and both actor and critic being learned at same time, \n  or the Gaussian assumptions, etc.). \n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation). \n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)? \n\n- How were the tasks selected?  Are they all the continuous control tasks available in open ai?\n\n\n \n\n\n\n\n"