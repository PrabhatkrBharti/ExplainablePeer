 "This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis.  The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter.  \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search . Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\ n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong.  However, I find this result to be out of the context with the main theme of the paper.  \n\nI find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity.  Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments .\n\nWhen it comes to the experiments only one real-world experiment is present . It is hard to conclude which method is better based on a single real-world experiment . Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time.  I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations.  If this is the case, some configurations could easily require more time to evaluate than the others.  It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots. \n\nIt is not clear what is the stopping criterion for each of the methods used in the experiments . One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned.  It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods.  \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n \nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\n