 "The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations.  Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance.  The approach of using BN after non-linearity is termed \"standardization layer\" (https://arxiv.org/pdf/1301.4083.pdf).  I encourage the authors to validate their claims against simple approach of using BN after non-linearity.  "