 "The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise.  It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \"structured noise\", and (3) Samples from out-of-dataset classes.  The experiments show robustness to these types of noise.  \n\nReview: \nThe claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise.  This is supported by the literature on \"data cleaning\" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.   While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy. \n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).  The paper did not get a chance to test these differences, and I can only raise a few hypotheses.  First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle.  This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.    Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise.   Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general.   \n\nWithout such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers.   \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search.   For example, for Conv4, how many channels at each layer?   how was the net initialized?  which hyper parameters were tuned and with which values?  were hyper parameters tuned on a separate validation set?  How was the train/val/test split done, etc.  These details are useful for judging technical correctness. \n-- Figure 8 failed to show for me.  \n-- Figure 9,10, need to specify which noise model was used.\n\n\n\n\n\n"