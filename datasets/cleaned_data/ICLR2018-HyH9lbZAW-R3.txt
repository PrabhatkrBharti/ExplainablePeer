 "The paper seems to be significant since it integrates PGM inference with deep models.  Specifically, the idea is to use the structure of the PGM to perform efficient inference.  A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.  Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference. \nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates.  As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution. \n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved. \ One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs. \ Can this be generalized to arbitrary PGM structures?  How about cases when computing Z is intractable?  Could the proposed approach be adapted to such cases.  I was not very sure as to why the proposed method is more general than existing approaches. \n\nRegarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets.  the approach shows that the proposed methods converge faster than existing methods.  However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting."