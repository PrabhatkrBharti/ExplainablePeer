 "\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.   They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL.   The domain is video games.     All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).  \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL \n\n- a number of approaches developed for the basic idea \n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games) \n\n- paper is overall well written/clear \n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling) \nCouldn't comparisons be made, in some way, to other multitask work? \n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks  ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!). \n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling). \n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section). \n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space).  Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper. \n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features.  Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting. \n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch? \n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ?  Could go either way since the network\n   has to allocate resources to learn other games too.   But is there a pattern? \n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5. \n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?  Why not explore both\n  possibilities?  what are the pros/cons? (section 3)\n\n\n\n\n"