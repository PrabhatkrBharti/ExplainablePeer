 "Summary:\nThe contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics. \nThe paper first presents the mathematical form of the proposed activation function (ISRLU), and then shows the similarities to ELU graphically.  It then argues that speeding up the activation function may be important since the convolution operations in CNNs are becoming heavily optimized and may form a lesser fraction of the overall computation.  The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions.  The possibility of computing a faster approximation of ISRLU is also mentioned .\nPreliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU .\n\nQuality and significance:\nThe paper proposes an interesting direction for optimizing the computational cost of training and inference using neural networks . However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice. \nThe paper does not present clear benchmarks showing a) what is the fraction of CPU cycles spent in evaluating the activation function in any reasonably practical neural network,  b) and what is the percentage of cycles saved by employing the ISRLU. \nThe presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.\nThe effect of using the faster approximation on performance also remains to be investigated. \n\nClarity:\nThe content of the paper is unclear in certain areas. \n- It is not clear what Table 2 is showing.  What is \"performance\" measured in? In general the Table captions need to be clearer and more descriptive . The acronym pkeep in later Tables should be clarified. \n- Why is the final Cross-Entropy Loss so high even though the accuracy is >99% for the MNIST experiments ? It looks like the loss at initialization was reported instead?"