 "The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML.  Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary.  The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule.  (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically.  (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize.  \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric.  Given a reward function, one can define the Bayes decision rule.  The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.  The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).  Of course, the moving-out is biased but the replacing is unbiased.  \n\nThe second contribution is partially valid,  although I doubt how much improvement one can get from SQDML.  The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).  In fact, this step can result in biased estimation because the replacement is inside the nonlinear function.  When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example.  However, when x is not repeated frequently, both RAML and SQDML are biased.  Experiment in section 4.1.2 do not validate significant improvement, either. \n\nThe numerical results are relatively weak.  The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.  However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau.  Is there any guidelines to choose \\tau?  For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper.   These results are also lower than the state of the art performance.  \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.  This is a direct result from definition of conditional probability.  No Bayesian elements, like prior or likelihood appears here. \n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator.  Compared with the n-gram replacement used in the paper, which one is better? \n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.  This will make the comparison much more clear. \n(4). What is Theorem 2 trying to convey?  Although \\tau goes to 0, there is still a gap between Q and Q'.  This seems to suggest that for small \\tau, Q' is not a good approximation of Q.  Are the assumptions in Theorem 2 reasonable?  There are several typos in the proof of Theorem 2. \n(5). In section 4.2.2, the authors write \"the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\".  Could you explain it in more details?\n\n"