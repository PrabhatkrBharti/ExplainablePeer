 "The main contribution of the paper is a method that provides visual explanations of classification decisions.  The proposed method uses \n - a generator trained in a GAN setup\n - an autoencoder to obtain a latent space representation\n - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it).  \nThe method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided. \n\n\n - The paper proposes a (I believe) novel method to obtain visual explanations.  The results are visually compelling  although most results are shown on a medical dataset - which I feel is very hard for most readers to follow.  The MNIST explanations help a lot.   It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader. \n\n - The paper shows that the results are plausible using a neat trick.  The authors train their system with the testdata included which leads to very different visualizations.  It would be great if this analysis could be performed for MNIST as well. \n\n\nFrom the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction.\nAlso, there is the work by Hendricks et al on Generating Visual Explanations.  This should probably also be referenced. \n\nminor comments: \n- some figures with just two parts are labeled \"from left to right\" - it would be better to just write left: ... right: ...\ n- figure 2: do these images correspond to each other?  If yes, it would be good to show them pairwise.\n- figure 5: please explain why the saliency map is relevant.  This looks very noisy and non-interesting.\n\n"