 "The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset.  Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) .  This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results. \n\n+ Quality:\n- Simple method leading to great results on ImageNet! \n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work.  One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes. \n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs. \n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID) \n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\ n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs. \n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger. \n\nOverall, I really enjoy reading this paper and recommend for acceptance!