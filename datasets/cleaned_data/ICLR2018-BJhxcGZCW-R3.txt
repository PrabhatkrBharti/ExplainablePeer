 "In the medical context, this paper describes the classic problem of \"knowledge base completion\" from structured data only (no text).   The authors argue for the advantages of a generative VAE approach (but without being convincing).   They do not cite the extensive literature on KB completion.   They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods. \n\nThe authors seem unaware of a large literature on \"knowledge base completion.\"  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015],  \n\nThe paper claims that operating on pre-structured data only (without using text) is an advantage.   I don't find the argument convincing.   There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. \"universal schema\" [Riedel et al, 2014]. \n\nThe paper claims that \"discriminative approaches\" need to iterate over all possible entity pairs to make predictions.   In their generative approach they say they find outputs by \"nearest neighbor search.\"   But the same efficient search is possible in many of the classic \"discriminatively-trained\" KB completion models also. \n\nIt is admirable that the authors use an interesting (and to my knowledge novel) data set.   But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995.   The method is evaluated only against their own VAE-based alternatives.   It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.\n"