 "After reading the rebuttal:\n\nThe authors addressed some of my theoretical questions.  I think the paper is borderline, leaning towards accept. \n\nI do want to note my other concerns:\n\nI suspect the theoretical results obtained here are somewhat restricted to the least-squares, autoencoder loss.   \n\nAnd note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better.  The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs.  So it is not clear if the proposed algorithm is better for training the classifier, which may be of more interest. \n\n=============================================================\n\nThis paper presents an algorithm for training deep neural networks.  Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers.  \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders.  Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation.  As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015.  This itself can be a small, nice contribution. \n\nWhat concerns me is the extension to multiple layers.  Some questions are not clear from section 3.4:\n1.  Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it?  A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result. \n\n2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence.  Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time.  As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime.  In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations). \n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD.  Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n"