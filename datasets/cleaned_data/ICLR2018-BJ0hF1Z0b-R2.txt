 "\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee.  The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed. \n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent \n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point. \n\nCorrectness: The paper is technically correct. \n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline. \n2. Clipping is supposed to help with the exploding gradients problem.  Do you have an idea why a low threshold hurts the performances?  Is it because it reduces the amplitude of the updates (and thus simply slows down the training)? \n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)? \n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee. \n2. Strong experimental setup that analyses in details the proposed extensions. \n3. Experiments performed on public datasets. \n\nCons\n-------\n\nNone \n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \"is given in Figure 1\" -> \"is given in Algorithm 1\"\n\n Note\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say."