 "This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy.  The writing is clear, concise and easy to follow. \n\nAn important argument in favour of using complex-valued networks is said to be the propagation of phase information.  However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat.  CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise . It definitely does not preserve phase, like modReLU would. \n\nThis makes me wonder whether the \"complex numbers\" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in \"Compressing neural networks with the hashing trick\" by Chen et al.).  Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this. \n\nThe image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach.  The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated.  It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague.  Once again, a more in-depth analysis of this phase behavior would be very welcome. \n\nI'm on the fence about this work: I like the ideas and they are explained well,  but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). \n\n\nComments:\n\n- The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work.  Maybe some subsection titles would help make it feel a bit more cohesive. \n\n- page 3: \"(cite a couple of them)\" should be replaced by some actual references :) \n\n- Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer?  It would be nice to discuss computational cost as well. \n\n\nREVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments.  In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network. \n\nRegarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this.  With this in mind, it is of course completely fine that the results are not better than for real-valued networks.\n"