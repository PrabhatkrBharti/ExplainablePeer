 "Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention.  This text generation task is traditionally done using recurrent neural networks.  This paper proposes to generate text using GANs.  GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.  This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.  The whole network is trained on the \"fill-in-the-blank\" task using the sequence-to-sequence architecture for both the generator and the discriminator.  At training time, the generator's encoder computes a context representation using the masked sequence.  This context is conditioned upon to generate missing words.  The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.  With this approach, one can generate text at test time by setting all inputs to blanks.  \n\nPros and positive remarks: \n--I liked the idea behind this paper.  I find it nice how they benefited from context (left context and right context) by solving a \"fill-in-the-blank\" task at training time and translating this into text generation at test time.  \n--The experiments were well carried through and very thorough. \n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.  I first thought that performance would be better when the generator's encoder uses the unmasked sequence.  Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time. \n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.  There is too much pre-training. I find this less elegant.  \n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well? \n            (2) how was the masking done?  how did you decide on the words to mask? was this at random? \n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence.  It needs to but \n                  that was not explicit in the paper. \n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well. \n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well.