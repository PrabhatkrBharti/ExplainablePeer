 "This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (\"n-grams\") which can be queried efficiently.  The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks.  I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA.  However, I have some concerns about the specific implementation and model discussed here.  How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language?  Addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage.  The paper is missing a clear analysis of NGM's limitations...  the examples of knowledge storage from bAbI in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple.  In its current form, I am borderline but leaning towards rejecting this paper. \n\nOther questions:\n- is \"n-gram\" really the most appropriate term to use for the symbolic representation?  N-grams are by definition contiguous sequences... The authors may want to consider alternatives. \n- why focus only on extractive QA?  The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach.  Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA? \n- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper.  How long does the encoding time take with 10 million sentences? \n- Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017?\n"