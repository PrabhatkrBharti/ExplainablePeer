 "The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator.  The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \"spectrally normalized\" objective. \n\nI think the methodology presented in this paper is neat and the experimental results are encouraging.  However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato).  For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2.  I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients.  Please clarify this. \n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator.  Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?"