 "The description of the proposed method is very unclear.  From the paper it is very difficult to make out exactly what architecture is proposed.  I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior?  Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training.  I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?  That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns? \n\n* The word \"layers\" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder.  E.g. \"The top stochastic layer z_L in FAME is a fully-connected dense layer\".  No, z_L is a vector of latent variables.  Are you saying the encoder produces it using a fully-connected layer? \n* Section 2.2 starts talking about \"deterministic layers h\".  Are these part of the encoder or decoder?  What is meant by \"number of layers connecting the stochastic latent variables\"? \n* Section 2.3: What is meant by \"reconstruction data\"? \n\nIf my understanding of the method is correct, the novelty is limited.  Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.  The reported likelihood results are very impressive though, and would be reason for acceptance if correct.  However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood.  There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound.  Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \"eps\": this is not guaranteed to give the right result.  Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y).  Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain)."