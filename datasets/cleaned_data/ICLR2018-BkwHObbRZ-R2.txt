 "This paper studies the problem of learning one-hidden layer neural networks and is a theory paper.  A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent.  This paper establishes an interesting connection between least squares population loss and Hermite polynomials.  Following from this connection authors propose a new loss function.  Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings. \n\nOverall, pretty interesting result and solid contribution.  The paper also raises good questions for future works.  For instance, is designing alternative loss function useful in practice?  In summary, I recommend acceptance.  The paper seems rushed to me so authors should polish up the paper and fix typos. \n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?  If so they should clarify this otherwise it confuses the reader a bit. \n2) What can be said about rate of convergence in terms of network parameters?  Currently a generic bound is employed which is not very insightful in my opinion.