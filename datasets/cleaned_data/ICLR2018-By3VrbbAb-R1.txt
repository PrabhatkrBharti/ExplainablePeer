 "This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU.   Regarding the latter methods: what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish, and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be made.   The final threshold for 'successful' speedups feels somewhat arbitrary -- why 16ms in particular?   In any case, these methods are useful to document, but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the paper.   \n\nWhile the idea of integrating the spelling error probability into the search for completions is a sound one, the specific details of the model being pursued feel very ad hoc, which diminishes the ultimate impact of these results.   Specifically, estimating the log probability to be proportional to the number of edits in the Levenshtein distance is really not the right thing to do at all.   Under such an approach, the unedited string receives probability one, which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive normalization.   Even under the assumption that a normalized edit probability is not particularly critical (an issue that was not raised at all in the paper, let alone assessed), the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural language.   For example, the probability p_1 of 'pkoe' versus p_2 of 'zoze' as likely versions of 'poke' (as, say, the prefix of pokemon, as in your example) should be such that p_1 >>> p_2, not equal as they are in your model.   Probabilistic models of string distance have been common since Ristad and Yianlios in the late 90s, and there are proper probabilistic models that would work with your same dynamic programming algorithm, as well as improved models with some modest state splitting.   And even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model.   It may very well end up that your very simple model does as well as a well estimated model, but that is something to establish in your paper, not assume.   That such shortcomings are not noted in the paper is troublesome, particularly for a conference like ICLR that is focused on learned models, which this is not.   As the primary contribution of the paper is this method for combining correction with completion, this shortcoming in the paper is pretty serious. \n\nSome other comments:\n\nYour presentation of completion cost versus edit cost separation in section 3.3 is not particularly clear, partly since the methods are discussed prior to this point as extension of (possibly corrected) prefixes.   In fact, it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation, or, frankly, the way in which the experimental results are described.   \n\nThe notation that you use is a bit sloppy and not everything is introduced in a clear way.   For example, the s_0:m notation is introduced before indicating that s_i would be the symbol in the i_th position (which you use in section 3.3).   Also, you claim that s_0 is the empty string, but isn't it more correct to model this symbol as the beginning of string symbol?   If not, what is the difference between s_0:m and s_1:m?   If s_0 is start of string, the s_0:m is of length m+1 not length m. \n\nYou spend too much time on common, well-known information, such as the LSTM equations.   (you don't need them, but also why number if you never refer to them later? )  Also the dynamic programming for Levenshtein is foundational, not required to present that algorithm in detail, unless there is something specific that you need to point out there (which your section 3.3 modification really doesn't require to make that point). \n\nIs there a specific use scenario for the prefix splitting, other than for the evaluation of unseen prefixes?   This doesn't strike me as the most effective way to try to assess the seen/unseen distinction, since, as I understand the procedure, you will end up with very common prefixes alongside less common prefixes in your validation set, which doesn't really correspond to true 'unseen' scenarios.   I think another way of teasing apart such results would be recommended. \n\nYou never explicitly mention what your training loss is in section 5.1. \n\nOverall, while this is an interesting and important problem, and the engineering details are interesting and reasonably well-motivated, the main contribution of the paper is based on a pretty flawed approach to modeling correction probability, which would limit the ultimate applicability of the methods.