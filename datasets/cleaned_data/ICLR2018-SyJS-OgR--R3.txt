 "\n\nThis paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating.   The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation.   This interpretation plays a role in the proposed training method by informing how the \u201cstep sizes\u201d in the Euler discretization should change when doubling the depth of the network.   The punchline of the paper is that the authors are able to achieve similar performance as \u201cfull ResNet training\u201d but with significantly reduced training time. \n\nOverall, the proposed method is novel \u2014 even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant.   Moreover the paper is clearly written.   Experimental results are decent \u2014 there are clear speedups to be had based on the authors' experiments.   However it is unclear if these gains in training speed are significant enough for people to flock to using this (more complicated) method of training. \n\nI only have a few small questions/comments:\n* A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size.   This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint. \n* One thing I\u2019m unclear on is how convergence was assessed\u2026 my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn\u2019t this also depend on the depth in some way?  \n* Would the speedups be more dramatic for a larger dataset like Imagenet? \n* Finally, not being very familiar with multigrid methods from the numerical methods literature \u2014 I would have liked to hear about whether there are deeper connections to these methods.\n\n\n"