 "Although the problem addressed in the paper seems interesting,  but there lacks of evidence to support some of the arguments that the authors make.  And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference.  Detailed critiques are as following:1. The idea proposed by the authors seems too quite simple.  It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM. \n2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5.  It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change. \n3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that.  And PCA as a non-random projection would a nice baseline to compare against. \n4. The paper seems to misuse the term \u201cFalse positive rate\u201d as the y label of figure 3(d/e/f).  The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive.  So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a).  From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified.  The paper should come up with a better term for that evaluation. \n5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d).  Need more experiment on more dimensionality to prove that. \n6. In the appendix B results part, sometimes the word \u2019S_attack\u2019 is typed wrong. And the values in  \u201cdistorted/distorted\u201d columns in Table 5 do not match up with the ones in Figure 3(c)."