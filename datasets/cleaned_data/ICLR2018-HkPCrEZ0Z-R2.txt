 "The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick. This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5.  They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters). \n\nMy main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?).  In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps.  This could have been done via empirical work, for instance:\n- Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1. \n- Show the effect of the reparameterization trick on estimator variance.\n- Compare the bias and variance of TRPO estimates vs the proposed method.