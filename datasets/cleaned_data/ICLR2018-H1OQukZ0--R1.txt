 "Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning.   The covered framework is limited to regularization parameters.  These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error).  The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters.  The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system.  Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach. \n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow.  Nevertheless the paper is quite well written. \n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016) . As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors. \n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017).  The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed. \n- The goal of the paper is to address automatically the learning of regularization parameters.  Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed.  It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious.  This fact weakens the scope of the online hyper-parameter optimization approach. \n- It may be helpful to indicate the standard deviations of the experimental results.