 "This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP.  The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications.  The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed. \n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance. \n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR.  First, there is not much innovation in the model architecture.  The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level.  I think this more counts as careful engineering of the SAN model rather than a main innovation.  Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders.  What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller? \n\n====\n\nI appreciate the answers the authors added and I change the score to 6."