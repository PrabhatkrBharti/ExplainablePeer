 "This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks.  The authors claimed to obtain efficiency improvement and better numerical stability. \n\nThis is a short paper that contains five pages.  The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices.  In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training.  In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network.  In terms of numerical stability, though experimental results were reported, there is no theoretical analysis.  The experiments are quite limited.\n"