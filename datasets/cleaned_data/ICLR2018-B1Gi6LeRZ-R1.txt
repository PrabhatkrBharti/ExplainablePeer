 "Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes.  Previous approaches to training data mixing are (1) from random classes, or (2) from the same class.  The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network.  Results look like significant improvements over standard learning setups. \n\nDetailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks.  In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power.  The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters. \n\nMinor Asks:\n(1) Clarification on how the error rates are defined.  Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label). \n(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes.  Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales.  I.e. are we talking about a space of 10 total classes or 10000 total classes?  How does num required epochs get impacted as we increase this class space? \n(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels. \n(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation...  what happens if you just randomly pick one of the 10 crops for prediction?"