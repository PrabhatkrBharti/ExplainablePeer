 "This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer.  As a result, a gradient descent algorithm converges to the unique solution.  This works address a conjecture proposed by Tian (2017). \n\nWhile it is clearly written, my main concern is whether this model is significant enough.  The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis,  but it makes the model considerably simpler than any practical setting.\n\n"