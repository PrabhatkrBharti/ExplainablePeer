This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks.  \nPros: Good empirical results. \nCons:  There is not much technical contribution.  The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved.  \n\n1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2).  \n\n2. Label embedding learning has been investigated in many previous works.  The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification.  This assumption is not very natural \u2014 though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels.   The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works.  \n\n3. The proposed model is not well explained.   \n(1) By using the objective in eq.(14), how to learn the embeddings E?  \n(2) The authors state \u201cIn back propagation, the gradient from z2 is kept from propagating to h\u201d.  This makes the learning process quite arbitrary under the objective in eq.(14).  \n(3) The label embeddings are not directly used for the classification (H(y, z\u2019_1)), but rather as auxiliary part of the objective.  How to decide the test labels[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]