The paper proposes a variance reduction technique for policy gradient methods.  The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines. \n\n\nThe writing and organization of the paper is very well done.  It is easy to follow, and succinct while being comprehensive.  The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively.  There is only one mostly minor issues with the algorithm development and the experiments need to be more polished.  \n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0.  This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions).  However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0.  Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied. \n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms.  The paper would be a much stronger contribution, if the experiments could be improved.  \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc. \n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper.  While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline.  What was the goal here? \n- In general, the graphs are difficult to read; fonts should be improved and the graphs polished.  \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline? \n- It'd be great if Plot (a) and (b) in Figure 5 are swapped. \n\nOverall I think the idea proposed in the paper is beneficial.  Better discussing the strong theoretical assumption should be incorporated.  Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner.  Particularly with some clarity on the experiments, I would be willing to increase the score.  \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained?  This should be explicitly shown, at least in the appendix.  \n2. The listed site for videos and additional results is not active. \n3. Some typos\n- Section 2 - 1st para - last line: \"These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.\". \n- Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i}).  \n- Section 4.2 - \\hat{Q} is just Q in many places[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]