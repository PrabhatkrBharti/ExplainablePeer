The paper is clearly written, with a good coverage of previous relevant literature.  \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced. \nTherefore, the paper must show that this new method performs better in some way compared with previous methods.  They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT. \n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN \n\n2. Clearly written\ n\n3. Quite good results on several relevant tasks. \n\nCons:\n1. Technical novelty is somewhat limited \n\n2. Experiments do not evaluate run time, memory use, computational complexity, or stability.  Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]