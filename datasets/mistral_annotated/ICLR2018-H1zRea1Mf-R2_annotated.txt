The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface . Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time . Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released. \n\nPros:\n- Generating programs with neural networks is an exciting direction \n- Novel task of generating UI code from UI screenshots \n- Three new datasets of UI images and corresponding code\ n- Paper is clearly written\ n\nCons:\n- Limited technical novelt y\n- Limited experiments \n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.  Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.  The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task .\n\nMy main concern with this paper is a lack of technical novelty.  The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now.  Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand. \n\nThe experiments in this paper are also lacking.  Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments . The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples.  I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods.  I would have also liked to see more innovation in evaluation.  Are there metrics other than overall accuracy that could be used to measure performance?  Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?  In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?  Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?  How does performance change with different amounts of training data ? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another ? I don\u2019t expect the authors to answer any of these questions in particular;  I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset. \n\nOn the whole I appreciate the novelty of the task and dataset,  but the paper suffers from a lack of technical novelty in the model and limited experimental validation.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]