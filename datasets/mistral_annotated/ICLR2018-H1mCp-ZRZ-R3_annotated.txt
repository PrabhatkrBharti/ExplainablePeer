The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein\u2019s identity and control functionals.  The method relates closely to prior work on action-dependent baselines,  but explores in particular on-policy fitting and a few other design choices that empirically improve the performance.  \n\nA criticism of the paper is that it does not require Stein\u2019s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate.  The derivation through Stein\u2019s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick. \n\nThe empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning.  However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:\n\n-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action.  A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.\n\n-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice.  Similar comparison should be done with off-policy fitting in Q-Prop. \n\nI wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations.  \n\nThe paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop.  As discussed above, it is encouraged to elaborate other potential causes that led to performance differences.  The experimental results are presented well for a range of Mujoco tasks.  \n\nPros:\n\n-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time\n\n-Good empirical evaluation \n\nCons:\n\n-]The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein\u2019s identity etc.  and does not inherit novel insights due to this derivation.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]