This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.   The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies. \n\nTo be honest, I didn't really \"get\" this paper.\n*  As far I understand, all of the original work policy gradients involved stochastic policies.   Many are/were Gaussian.\n*  All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*  As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution. \n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas. \n\nI thought the little 2-mode MOG was a nice example of the premise of the model. \n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.   Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.   Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.   They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \"Very well, run your algorithm 10x longer and see where you end up!\"   I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published. \n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-POS],[ETH-POS]]