The paper studies the expressive power provided by \"overlap\" in convolution layers of DNNs.   Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling.   Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\ n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width).   The key part of the proof is showing a lower bound on the rank for networks with overlap .  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \"generic\" tensor. \n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs,  which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\ n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth) .\n3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank. \n\nThat said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets , as it gives some intuition about how much overlap might 'suffice'.  \n\nI recommend weak accept[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]