The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of \u201chard\u201d functions that are not easily representable by shallower networks.   It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss.   I think these contributions warrant publishing the paper at ICLR 2018.   The paper is also well written, a bit dense in places, but overall well organized and easy to follow.  \n\nA key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer.   It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer.   I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors\u2019 comments in case I\u2019m missing some simplification. \n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL.   For example, adding PWL to Theorems and Corollaries in Section 3.1 will help.    Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]