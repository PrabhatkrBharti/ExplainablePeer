In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization.  Their technique can be applied on top of existing GANs and can address issues such as mode collapse.  The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN.  Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse.  This is a strong contribution \n\nIn Table 2 the difference between inception scores for DCGAN and this approach seems significant to ignore.  The authors should explain more possibly. \nThere is a typo in Page 2 \u2013 For all these varaints, -variants.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]