NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset?  Fair comparison of the data is a serious concern.  I have listed this review as a good for publication due to the novelty of ideas presented,  but the accusation of misrepresentation below is a serious one and I would like to know the author's response. \n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose \u201cstochastic lazy attributes\u201d, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid.  Using Bayesian Optimization, search over this space can yield decodings with targeted properties. \n\nMany of the ideas presented are novel.  The results presented are state-of-the art.  As noted in the paper, the generation of syntactically and semantically valid data is still an open problem.  This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning. \n\n*Remarks on methodology*\n\nBy initializing a decoding by \u201cguessing\u201d a value, the decoder will focus on high-probability starting regions of the space of possible structures.  It is not clear to me immediately how this will affect the output distribution.  Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space.  Does this sacrifice exploration for exploitation in some quantifiable way?  Some exploration of this issue or commentary would be valuable.  \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background.  In my opinion, this section could benefit from a little more expansion and conceptual definition. \n\nThe first 3 sections of the paper are very clearly written,  but the remainder has many typos and grammatical errors (often word omission).  The draft could use a few more passes before publication.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]