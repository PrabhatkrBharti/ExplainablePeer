Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping. \nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain. \nThis \"hybrid\" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time. \nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference. \n\nComments:\n1. I recommend the authors to tone down their claims.  For example, the authors mentioned that \"there has been no complete implementation of established deep learning approaches\" in the abstract, however, the authors did not define what is \"complete\".  Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server. \n\nAlso, the claim that \"we show efficient designs\" is very thin to me since there are no experimental comparisons between the proposed method and existing works.  Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C].  For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE. \n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.  I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit.  \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.  Such optimizations are very common in deep learning field while less known in the field of security. \n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. \n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.  \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]