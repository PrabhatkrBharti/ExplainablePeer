This paper presents a method for matrix factorization using DNNs.  The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7).  From the methodology point of view, such extensions are relatively straightforward.  As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically . The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features. \n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer.  In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem).  I think this paper does not connect very well with that literature.  When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework.  I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods,  or if the success of this paper is mostly due to clever processing of text features using DNNs. \n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors.  \n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \"inference\" several times without definition \n    -- \"if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \".  Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples? \n    -- \"Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\" . \n3. the experimental setup seems quite unusual to me: \"since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\".  This seems very problematic if most of the entries are not observed.  Why cannot you use the typical evaluation procedure for collaborative filtering,  where you hide some known entries during model training, and evaluate on these entries during test? [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]