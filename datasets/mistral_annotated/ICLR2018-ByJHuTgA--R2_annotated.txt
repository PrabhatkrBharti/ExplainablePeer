The authors did extensive tuning of the parameters for several recurrent neural architectures . The results are interesting.  However the corpus the authors choose are quite small,  the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn .\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens.  This will use significant resources and is much more difficult,  but it's also really valuable, because it's much more close to real world usage of language models.  And less tuning is needed for these larger datasets.  \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]