This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input.  For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better. \nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved. \n\nBasically the motivation and method is good, the drawback of this paper is\nits narrow scope and lack of necessary explanations.  Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \"general English\".  Why is this necessary?  Why not\n  just using better enough basic English and the text of the target user? \n\n- To achieve the goal above, huge data (not the \"portion of the general English\") should be communicated over the network.  Is this really worth doing?  If only\n  \"the portion of\" general English must be communicated, why is it validated? \n\n- For measuring performance, authors employ keystroke saving rate.  For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation.  Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important. \n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models.  Are there any\n  comparison with these previous efforts? \n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so.  I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts. \n\nMinor:\n- t of $G_t$ in page 2 is not defined so far.\n- What is \"gr\" in Section 2.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]