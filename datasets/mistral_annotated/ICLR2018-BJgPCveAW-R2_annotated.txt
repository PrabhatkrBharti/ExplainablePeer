The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.  Numerical experiments show that such sparse networks can have similar performance to fully connected ones.  They introduce a concept of \u201cscatter\u201d that correlates with network performance.  Although  I found the results useful and potentially promising,  I did not find much insight in this paper. \nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.  Once the signals from different windows are intermixed, how do you even define the windows?   \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]