ORIGINALITY & SIGNIFICANCE\n\nThe authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network.  The authors point out that the original \"value iteration network\u201d (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network.   It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps.  Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems.  The work is therefore original and significant. \n\nThe algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability.  The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster.   The M in MVProp in particular seems to be very useful in scaling up to the large grids.  The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time. \n\n\nQUALITY\n\nThe symbol d_{rew} is never defined \u2014 what does \u201cnew\u201d stand for?  It appears to be the number of latent convolutional filters or channels generated by the state embedding network.  \n\nSection 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }. \nBased on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer.  \n\nIn equation 1, it wasn\u2019t obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation? \nThe h( \\Phi( x ), v^{k-1} ) sort of makes sense \u2026  value is only calculated with respect to only the observation of the maze obstacles but the policy \\pi is calculated with respect to the joint  observation and agent state.  \n\nThe expression \n\n   h_{aid} ( \\phi(0), v )   =   <  Wa,   [ \\phi(o) ; v ]   >   +   b\n\nmakes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions).  I gather the tensor Wa = R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding \\phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time?  \n\nI didn\u2019t understand the r^in, r&out representation in section 4.1. These are given by the domain? \n\nI did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision. \n\nA bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful. \n\n\nPOST REVISION COMMENTS:\n\n- I didn't reread the whole thing -  just used the diff tool.   \n- It looks like the typos in the equations got fixed\n- The new phrase \"enables to learn to plan\" seems pretty awkward[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-NEG],[CST-POS],[NOV-POS],[ETH-NEG]]