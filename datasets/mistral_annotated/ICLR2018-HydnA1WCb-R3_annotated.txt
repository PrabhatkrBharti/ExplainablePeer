SUMMARY: This work is about prototype networks for image classification.  The idea is to jointly embed an image and a \"confidence measure\" into a latent space, and to use these embeddings to define prototypes together with confidence estimates.  A Gaussian model is used for representing these confidences as covariance matrices.  Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype.  \n\nEVALUATION:\nCLARITY: I found the paper difficult to read.  In principle, the idea seems to be clear, but then the description and motivation of the model remains very vague.  For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)?   What is the motivation to just average the inverse covariance matrices to compute S_C?   Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space?   \nNOVELTY: Honestly, I had difficulties to see which parts of this work could be sufficiently novel.   The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting,   but also a time-honored concept.   The experiments focus very specifically on the omniglot dataset, and it is not entirely clear to me what  should be concluded from the results presented.   Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?   \n\n\n"[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]