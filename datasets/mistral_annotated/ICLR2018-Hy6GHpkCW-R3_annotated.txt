The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches. \n\n+ new and large dataset \n+ novel algorithm \n+ well written \n- no evaluation of dataset \n- virtually no evaluation of algorithm \n- no baselines or comparison \n\nThe paper is well written, and easy to follow.  The presented algorithm sketch-rnn seems novel and significantly different from prior work. \nIn addition, the authors collected the largest sketch dataset, I know of.  This is exciting as it could significantly push the state of the art in sketch understanding and generation.  \n\nUnfortunately the evaluation falls short.  If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short. \nFor ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop.  Remove the variational component, and phrasing it simply as an auto-encoder.  Table 1 is good,  but not sufficient.  Training loss alone likely does not capture the quality of a sketch. \nA comparison the Graves 2013 is absolutely required, more comparisons are desired. \nFinally, it would be nice to see where the algorithm falls short, and where there is room for improvement. \n\nIf the authors wish to push their dataset, it would help to first evaluate the quality of the dataset.  For example, how well do humans classify these sketches?  How diverse are the sketches?  Are there any obvious modes?  Does the discretization into strokes matter? \nAdditionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on?  Are there any good automated metrics, and how well do they correspond to human judgement? \n\nIn summary, I'm both excited about the dataset and new architecture,  but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset.  I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]