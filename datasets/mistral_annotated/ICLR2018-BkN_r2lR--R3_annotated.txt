This paper adds an interesting twist on top of recent unpaired image translation work.  A domain-level translation function is jointly optimized with an instance-level matching objective.  This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches.  I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet. \n\nMy main criticism is with the experiments and results.  The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets.  Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches.  This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches.  It feels rather contrived to focus so much on the datasets with exact matches since,;  1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,;  2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios); 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.  There is no comparison to any such simple baselines. \n\nI think finding analogies that are not exact matches is much more compelling.  Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results.  I\u2019d like to see far more results, and some attempt at a metric.  One option would be to run user studies where humans judge the quality of the matches.  The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results:  for example, the DiscoGAN results have some artifacts but capture the texture better in row 3. \n\nI was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data.  In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful. \n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches.  For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real. \n\nI\u2019d like to see more discussion of why the second stage supervised problem is beneficial.  Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)? \n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context. \n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3.  Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section. \n4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when.  For example, in Eqn. 7, it would be clearer to write out the min over optimization variables. \n5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017. \n6. Page 7: The following sentence is confusing and should be clarified:  \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain. \u201d\n7. Page 7: \u201cThis shows that a good initialization is important for this task. \u201d \u2014 Isn\u2019t this more than initialization?  Rather, removing the distributional and cycle constraints changes the overall objective being optimized. \n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function? \n9. Throughout the paper, some citations are missing enclosing parentheses[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]