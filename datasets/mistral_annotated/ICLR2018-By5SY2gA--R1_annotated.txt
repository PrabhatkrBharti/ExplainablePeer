This paper proposed to use affect lexica to improve word embeddings.  They extended the training objective functions of Word2vec and Glove with the affect information.  The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis.  Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove.  In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups. \n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new.  Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016. \n-\tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best.  Do the authors have any intuition why? \n-\tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me.   I would like to see to the top k nearest neighbors of each of those words.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]