The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization.  When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning.  The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \"strong\" penalty parameter.  In their experimental results, the phase transition is not observed anymore with their protocol.  This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol. \n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked.  The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem.  I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions.  I think that the problem would be better handled that way than with the proposed strategy,;  which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases. \n\nTo summarize, I believe that the paper addresses an important point,;  but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere. \n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9)  \n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper  \n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]