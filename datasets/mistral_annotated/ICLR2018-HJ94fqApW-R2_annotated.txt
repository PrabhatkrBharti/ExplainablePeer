This paper is well written and it was easy to follow . The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers . This is achieved by forcing the output of some channels being constant during training.  This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter . \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks . The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet. \n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc.  Could the authors explain their choices?  How sensible is the algorithm to these hyperparameters?\ nIt would be nice to see empirically how much of computation the proposed approach takes during training.  How much longer does it takes to train the model with the ISTA based constraint ?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]