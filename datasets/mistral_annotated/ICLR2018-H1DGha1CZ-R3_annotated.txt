This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU. \n\nComments:\n\n1. Using expectation to explain why DReLU works well is not sufficient and convincing.  Although DReLU\u2019s expectation is smaller than expectation of ReLU, but it doesn\u2019t explain why DReLU is better than very leaky ReLU, ELU etc. \n2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc. \n3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious.  I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU.  \n\nOverall, I don\u2019t think this paper meet ICLR\u2019s novelty standard, although the authors present some good numbers, but they are not convincing.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]