Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks.  The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points. \n\nThe paper is well organized and well written.  Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points.  This technique generalizes existing work under full relaxation of assumptions.  In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices;  In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices.  Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work. \n\nThe results obtained from the analytical forms of the critical points are interesting,  but one problem is that how to obtain the proper solution of equation (3)?  In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12).  It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]