This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item.  The main framework is based on the Dawid-Skene model.  By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels.  The paper discusses the influence of the label redundancy both theoretically and empirically.  Results show that with a fixed budget, it\u2019s better to label many examples once rather than fewer examples multiple times. \n\nThe model and algorithm in this paper are simple and straightforward.  However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy.  The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed.  The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing. \n\nAbout the technique details, this paper is clearly written,  but some experimental comparisons and claims are not very convincing.  Here I list some of my questions:\n+About the MBEM algorithm, it\u2019s better to make clear the difference between MBEM and a standard EM. Will it always converge? What\u2019s its objective? \n+The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical? \n+When n = O(m log m), the result that \\epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this? \n+During CIFAR-10 experiments when r=1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels? \n+For ImageNet and MS-COCO experiments with a fixed budget, you reduced the training set when increasing the redundancy, which is unfair.  The reduction of performance could mainly cause by seeing fewer raw images, but not the labels.  It\u2019s better to train some semi-supervised model to make the settings more comparable[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]