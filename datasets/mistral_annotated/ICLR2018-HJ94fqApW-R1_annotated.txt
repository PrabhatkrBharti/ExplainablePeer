In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations.  The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a \u201cgate\u201d to the information flow . Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency . In the experiments section, the authors apply their pruning approach on a few representative problems and networks.  \n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights . However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as \u201csmaller-norm-less-informative\u201d assumption.  Hence, the title seems a bit misleading.  \n\nThe quality and clarity of the paper can be improved in some sections . Some specific comments by section:\n\n3. Rethinking Assumptions:\n-\tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily . For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case.  Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017 . Can the authors provide better examples here? \n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity? \n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing \u201csum_reduced\u201d\n-\tISTA \u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative.  Can the authors address the earlier comment about \u201ca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms\u201d ?\n-\tCan the authors address the earlier comment on \u201chow to set thresholds for weights across different layers\u201d, by providing motivation for choice of penalty for each layer?  \n-\tCan the authors address the earlier comment on how their approach provides \u201cguarantees for preserving neural net functionality approximately\u201d?\n\n 5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size?  (like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this.  However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\ n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem? \n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising . The authors have started with right motivation and the initial section asks the right questions, however,  some of those questions are left unanswered in the subsequent work as detailed above[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]