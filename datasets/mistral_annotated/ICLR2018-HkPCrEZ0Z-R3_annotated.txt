This paper presents a model-based approach to variance reduction in policy gradient methods.   The basic idea is to use a multi-step dynamics model as a \"baseline\" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.   The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks. \n\nThis paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission.  But right now (at least as of the version I'm reviewing), the paper reads as being half-finished.   Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of \"embedding\" an \"imaginary trajectory\" remains completely opaque to me.   Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is \"we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates,\" but I see literally no section that hints at anything like this (no mention of \"dynamic unfolding\" or \"latent space trajectory embedding\" ever occurs later in the paper). \n\nIn a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.   But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick.   But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about. \n\nThe paper frequently refers to \"embedding\" \"imaginary trajectories\" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).   I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here.   But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper. \n\nFinally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently.   The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach.   And the experiments are described with very little detail or discussion about the experimental setup. \n\nNor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix.   Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.   My honest impression is simply that this is still work in progress and that the write up was done rather hastily.  I think it will eventually become a good paper, but it is not ready yet[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]