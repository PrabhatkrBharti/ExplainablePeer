This paper proposes a new way of sampling data for updates in deep-Q networks.  The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode. \n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques. \n\nThe results presented by this paper shows improvement over the baseline.  But the Atari results is still significantly worse than the current SOTA.  Some (theoretical) analysis would be nice.  It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case. \n\nThere has been a number of highly relevant papers.  Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. \n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method.  Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline.  The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]