This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors.  This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \"unused\" until the present task in hand.\n\n The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.  In Section 2 the authors review conceptors.  This method is algebraic method closely related to spanning sub spaces and SVD.  The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally.  In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.  The authors provide a version with batch SGD as well.\n\n In Section 4, the authors show their method on permuted MNIST.  They compare the method to EWC with the same architecture.  They show that their method more efficiently suffers on permuted MNIST from less degradation.  Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance. \n\nIn general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life.  Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\n I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.   What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods.  Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.   [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]