The experimental results are similar to previously proposed methods.  \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results.  However, the method is not properly motivated.  As far as I can tell, the paper never answers the questions: Why do we need a guide actor?  What problem does the guide actor solve?  \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and  (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.  Using second order methods is not an end in itself.  The experimental results show the authors have found a way to use second order methods without making performance *worse*.  Given the high variability of deep RL, they have not convincingly shown it performs better. \n\nThe paper does not discuss the computational cost of the method.  How does it compare to other methods?  My worry is that the method is more complicated and slower than existing methods, without significantly improved performance. \n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]