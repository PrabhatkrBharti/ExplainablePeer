The paper compares some recently proposed method for validation of properties\nof piece-wise linear neural networks and claims to propose a novel method for\nthe same.  Unfortunately, the proposed \"branch and bound method\" does not explain\nhow to implement the \"bound\" part (\"compute lower bound\") -- and has been used \nseveral times in the same application,;  incl.:\n\nRuediger Ehlers. Planet. https://github.com/progirep/planet,\nChih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis\nAlessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351 \n\nSpecifically, the authors say: \"In our experiments, we use the result of \nminimising the variable corresponding to the output of the network, subject \nto the constraints of the linear approximation introduced by Ehlers (2017a)\"\nwhich sounds a bit like using linear programming relaxations, which is what\nthe approaches using branch and bound cited above use.  If that is the case,\nthe paper does not have any original contribution.  If that is not the case,\nthe authors may have some contribution to make, but have not made it in this\npaper, as it does not explain the lower bound computation other than the one\nbased on LPs. \n\nGenerally, I find a jarring mis-fit between the motivation (deep learning\nfor driving, presumably involving millions or billions of parameters) and\nthe actual reach of the methods proposed (hundreds of parameters). \nThis reach is NOT inherent in integer programming, per se.  Modern solvers\nroutinely solve instances with tens of millions of non-zeros in the constraint\nmatrix, but require a strong relaxation.  The authors may hence consider\nimproving the LP relaxation, noting that the big-M constraint are notorious\nfor producing weak relaxations.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]