This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences.  The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence. \n\nThis is an intriguing idea . I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model.  I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans. \nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart? \n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear.  Once they\nhave been calculated in Algorithm 1, how are they used?  Do the \\rho values\nsomewhere treat these two quantities differently? \n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately. \nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \"at a large venue\" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]