The authors propose using piecewise linear activation functions with contraints to make it continous.  They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article.  Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron. \n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required.  Authors present no clear explanation on why the shift should result in improved performance.[[CLA-POS],[JUS-NEG],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-NEG],[NOV-NEG],[ETH-NEG]]