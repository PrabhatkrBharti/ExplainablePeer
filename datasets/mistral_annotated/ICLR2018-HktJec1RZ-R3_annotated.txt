This paper introduces a new architecture for end to end neural machine translation.  Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right.  \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.   \n\nThe idea is nice.  The proposed approach does not rely on attention based model.  This opens nice perpectives for better and faster inference.  \n\nMy first concern is about the architecture description.  For instance, the swan part is not really stand alone.  For reader who does not already know this net, I'm not sure this is really clear.  Moreover, there is no link between notations used for the swan part and the ones used in the reordering part.  \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence.  Maybe you could motivate your choice at this point.  This is the main contribution of the paper, since swan already exists. \n\nFinally, the experimental part shows nice improvements  but: 1/ you must provide baseline results with a well tuned phrase based mt system;  2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]