\nThe paper was fairly easy to follow,  but I would not say it was well written.  These are minor annoyances; there were some typos and a strange citation format.  There is nothing wrong with the fundamental idea itself,  but given the experimental results it just is not clear that it is working.\ n\nThe bot performance significantly better than the fully trained agent.  This leads to a few questions:\n\n1. What was the performance of the \"regression policy\", that was learned during the supervised pretraining phase?\ n2. Given enough time would the basic RL agent reach similar performance? (Guessing no...) Why not?\n3.  Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing? \n\nPros:\nThe regularization of the Q-values w.r.t. the policy of another agent is interesting\n\n Cons:\nNot very well setup experiments\nPerformance is lower than you would expect just using supervised training\nNot clear what parts are working and what parts are not\n\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]