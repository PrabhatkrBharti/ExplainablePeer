This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means).  Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples. \n\nHowever, several important concepts in the paper are not well explained or motivated.  For example, it is a bit misleading to use the word \"covariance\" throughout the paper, when the best model only employs a scalar estimate of the variance.  A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of \"optimal\" under eq.6 is not explained.  More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate.  The distance function (eq. 4) is d Mahalanobis distance, instead of \"linear Euclidean distance\".  While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky). \n\nIn addition, the experiments are too limited to support the claimed benefits from encoding uncertainty.  Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant.  In addition, more analysis may better support existing claims.  For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points. \n\nPros:\n-Interesting problem and interesting direction[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]