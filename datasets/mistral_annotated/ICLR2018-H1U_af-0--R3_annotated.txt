This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature. \n\nPros\n\n1. It is new to apply quadrature rules to improve kernel approximation.  The only other work I found is\nGaussian Quadrature for Kernel Features NIPS 2017. \nThe work is pretty recent so the author might not know it when submitting the paper.  But in either case, it will be good to discuss the connections. \n\n2. The proposed method is shown to outperform a few baselines empirically. \n\nCons\n\n1. I don\u2019t find the theoretical analysis to be very useful.  In particular, the theorem shows that the kernel approximation error is O(1/D), which is the same as the original RFF paper.  Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method.  Unlike deep neural networks, since RFF is such a simple model, I think providing precise theoretical understanding is crucial.  \n\n2. Approximating an integral is a well-studied topic. I do not find a good discussion on all the possible methods.  Why is Genz & Monahan 1998 better than other alternatives such as Monte-Carlo, QMC etc?  One argument seems to be \u201cfor kernels with specific specific integrand one can improve on its properties\u201d. But this trick can be used for Monte-Carlo as well. And I do not see benefit of this trick in the curves. \n\n3. When choosing the orthogonal matrix, I think one obvious choice is to sample a matrix from the Stiefel manifold (the Q matrix of a random Gaussian). This baseline should be added in additional to H and B. \n\n4. A wall-time experiment is needed to justify the speedup. \n\nMinor comments:\n\u201cFor kennels with q(w) other than Gaussian\u2026 obtain very accurate results with little effort by using Gaussian approximation of q(w)\u201d.  What is the citation of this in the kernel approximation context?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-POS]]