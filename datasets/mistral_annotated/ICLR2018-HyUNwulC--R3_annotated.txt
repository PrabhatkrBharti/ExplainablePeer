This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation.  The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM.  Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997).  A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure. \n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal.  Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures. \n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs.  While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets. \n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]