This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem.  They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.  I think this is a nice contribution that does yield to some interesting insights.  However I do have some concerns about the way the paper is currently written and I find some claims misleading. \n\nPrior convergence proofs: I think the way the paper is currently written is misleading.  The authors quote the paper from Ian Goodfellow: \u201cFor GANs, there is no theoretical prediction as to\nwhether simultaneous gradient descent should converge or not. \u201d. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf.  A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent.  Another problem is of course the assumptions required for the proof that typically don\u2019t hold in practice (see comment below). \n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution.  This typically yields a non-convex non-concave optimization problem.  This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm. \n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2.  This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf \nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this?  I think this would also shed some light as to why this approach alleviates the problem of mode collapse. \n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality.  For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work  but I\u2019m not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse.  The inception score fails to capture this property.  Could the authors explore other quantitative measure?  Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)? \n\nExperiments\nTypo: Should say \u201cThe data distribution is p_d(x) = 1{x=1}\u201d.\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]