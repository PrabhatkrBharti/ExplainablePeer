This paper introduces an appealing application of deep learning: use a deep network to approximate the behavior of a complex physical system, and then design optimal devices (eg airfoil shapes) by optimizing this network with respect to its inputs.  Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn't leave the manifold of reasonable designs.  \n\n On one hand, I would suggest that this work would be better placed in an engineering venue focused on fluid dynamics.  On the other hand, I think the ICLR community would benefit from about the opportunities to work on problems of this nature. \n\n =Quality=\nThe authors seem to be experts in their field.  They could have done a better job explaining the quality of their final results, though.  It is unclear if they are comparing to strong baselines. \n\n=Clarity=\nThe overall setup and motivation is clear. \n\n=Originality=\nThis is an interesting problem that will be novel to most member of the ICLR community.  I think that this general approach deserves further attention from the community. \n\n\n=Major Comments=\n* It's hard for me to understand if the performance of your method is actually good.  You show that it outperforms simulated annealing. Is this the state of the art?  How would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil, without relying on any computational simulation at all? \n\n* You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs.  Why not do lots of random initializations for the optimization?  Isn't that a good way to help avoid local optima? \n\n* I'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simulator.  For example, you could evaluate the deep-net-predicted drag ratio vs. the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shape.  If there's a gap, it suggests that your NN approximation might have not been that accurate. \n\n=Minor Comments=\n* \"We also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optima\"\nGenerally, people add noise to the gradients, not the values of the parameters.  See, for example, uses of Langevin dynamics as a non-convex optimization method. \n\n* You have a complicated method for constraining the parameters to be in [-0.5,0.5].  Why not just enforce this constraint by doing projected gradient descent?  For the constraint structure you have, projection is trivial (just clip the values) . \n\n * \"The gradient decent approach required roughly 150 iterations to converge where as the simulated annealing approach needed at least 800.\"\nThis is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach.  I'd point out that this construction can be done in parallel, so it's less of a computational burden. \n\n* I'd like to hear more about the effects of different parametrizations of the airfoil surface.  You optimize the coefficients of a polynomial. Did you try anything else? \n\n* Fig 6: What does 'clean gradients' mean?  Can you make this more precise? \n\n* The caption for Fig 5 should explain what each of the sub figures is[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]