# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP).  The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).  In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations.  This network is trained through supervised learning for the output and reinforcement learning for discrete operations.  The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline. \n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new. \n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description).  So, it is hard to understand the core idea of this paper. \n- The proposed method and problem setting are not well-justified.  \n- The results are not very convincing. \n\n# Novelty and Significance\n- The problem considered in this paper is new,  but it is unclear why the problem should be formulated in such a way..  To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem.  I do not see why this should be formulated as a \"sequential\" decision problem.  Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks.  This does not require any \"memory\" because all necessary information is available to the network. \n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications.  \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary.  The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network.  In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective.  It would be good to justify (empirically) the proposed reward function.  \n- The results are not fully-convincing.  If my understanding is correct, the LTMN is trained to predict the baseline solver's output.  But, the LTMN significantly outperforms the baseline solver even in the training set.  Can you explain why this is possible? \n\n# Clarity\n- The problem statement and model description are not described well.  \n1) Is the network given a sequence of program/solution input?  If yes, is it given during evaluation as well?  \n2) Many notations are not formally defined.  What is the output (o_t) of the network?   Is it the optimal solution (x_t)?   \n3) There is no mathematical definition of memory addressing mechanism used in this paper.  \n- The overall objective function is missing.  \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]