The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article).  The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.    Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same.  \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU.  When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input.  While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features.  Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-NEG],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]