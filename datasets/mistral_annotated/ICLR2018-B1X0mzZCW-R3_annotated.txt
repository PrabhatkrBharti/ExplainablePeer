The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.   This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.   The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.   The authors evaluate their proposed methods on one toy problem and two real-world problems.   The paper is well written, easy to follow, and have good experimental study.    My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work  . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?   To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?   (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?   (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?   Why not learning the representation using an unsupervised learning method (unsupervised pre training)?   This should be at least one of the baselines.  \n(4) the idea of using surrogate labels to learn representation is also not new.   One example work is \"Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\". The authors didn't compare their method with this one.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-NEG]]