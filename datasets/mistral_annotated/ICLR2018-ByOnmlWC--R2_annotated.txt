The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \"strong\" network).  The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \"crossover\" network is presumably time-consuming. \n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment.  There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \"evolutionary\" computing.  Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning.  I would perhaps even call this a distillation network rather than a crossover network.  In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \"expert\" trajectories can be generated in unlimited quantities[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]