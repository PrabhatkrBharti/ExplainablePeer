This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation.  This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations.  These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.  However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported.  In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets.  Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.  \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs.  However, I was hoping to see a direct comparison between FP16 and INT16.   \n\nThe paper is written clearly and the English is fine.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-NEG],[NOV-POS],[ETH-NEG]]