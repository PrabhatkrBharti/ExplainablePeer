The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that \"obfuscates\" the document, i.e. changing words in the document.  The authors mention that word dropout can be considered as its special case which randomly drops words without any prior.  Then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent, which in turn would improve the performance of the reader as well.  Hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer, which predicts the word that would make the reader fail if the word is obfuscated..  While there have been numerous GAN-like approaches for language understanding, very few, if any, have shown worthy results.  So if this works, it could be an impactful achievement..  \n\nHowever, I am concerned with the experimental results. \n\nFirst, CBT: NE and CN numbers are too low.  Even a pure LSTM achieves (no attention, no memory) 44% and 45%, respectively (Yu et al., 2017).  These are 9% and 6% higher than the reported numbers for adversarial GMemN2N.  So it is very difficult to determine if the model is appropriate for the dataset in the first place, and whether the gain from the non-adversarial setting is due to the adversarial setup or not. \n\nSecond, Cambridge dialogs: the dataset's metric is not accuracy-based (while the paper reports accuracy), so I assume some preprocessing and altering have been done on the dataset.  So there is no baseline to compare.  Though I understand that the point of the paper is the improvement via the adversarial setting, it is hard to gauge how good the numbers are. \n\nThird, TripAdvisor: the dataset paper by Wang et al. (2010) is not evaluated on accuracy (rather on ranking, etc.).  Did you also make changes to the dataset?  Again, this makes the paper less strong because there is no baseline to compare. \n\nIn short, the only comparable dataset is CBT, which has too low accuracy compared to a very simple baseline. \nIn order to improve the paper, I recommend the authors to evaluate on more common datasets and/or use more appropriate reading models. \n\n---\n\nTypos:\npage 1 first para: \"One the first hand\" -> \"On the first hand\"\npage 1 first para: \"minimize to probability\" -> \"minimize the probability\"\npage 3 first para: \"compensate\" -> \"compensated\"\npage 3 last para: \"softmaxis\" -> \"softmax is\"\npage 4 sec 2.4: \"similar to the reader\" -> \"similarly to the reader\"\npage 4 sec 2.4: \"unknow\" -> \"unknown\ "\npage 4 sec 3 first para: missing reference at \"a given dialog\"\npage 5 first para: \"Concretly\" -> \"Concretely\"\nTable 1: \"GMenN2N\" -> \"GMemN2N\"\nTable 1: what is difference between \"mean\" and \"average\"?\npage 8 last para: missing reference at \"Iterative Attentive Reader\"\npage 9 sec 6.2 last para: several citations missing, e.g. which paper is by \"Tesauro\"? \n\n\n[Yu et al. 2017] Adams Wei Yu, Hongrae Kim, and Quoc V. Le. Learning to Skim Text. ACL 2017\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]