This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer.  In addition, attention context is computed for each layer, then, combined together as a single context.  Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters. \n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable.  However I have some concerns to this paper. \n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from.  I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms. \n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers.  Do you have experiments by fixing the number of layers and varying the hidden size? \n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]