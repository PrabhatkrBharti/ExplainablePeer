The paper presents a method for improving the diversity of Generative Adversarial Network (GAN) by promoting the Gnet's weights to be as informative as possible.  This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node.  Numerical experiments that demonstrate the diversity increment on the generated samples are shown. \n\nConcerns.\n\nThe paper is hard do tear and it is deficit to identify the precise contribution of the authors.  Such contribution can, in my opinion, be summarized  in a potential of the form\n\nwith\n\n$$\nR_BRE = a R_ME+ b R_AC = a \\sum_k  \\sum_i s_{ki}^2   +  b \\sum_{<k,l>} \\sum_i \\{ s_{ki} s_{li} \\}   \n$$\n(Note that my version of R_ME is different to the one proposed by the authors, but it could have the same effect)\n\nWhere a and b are parameters that weight the relative contribution of each term  (maybe computed as suggested in the paper). \n\nIn this formulation:\n\nThen R_ME has a high response if the node has saturated responses -1\u2019s or 1``s, as one desire such saturated responses, a should be negative. \n\nThe R_AC, penalizes correlation between responses of different nodes. \n\nThe point is, \n\na) The second term will introduce  low correlation in saturated vectors, then the will be informative.  \n \nb) why the authors use the softsign instead the tanh:  $tahnh \\in C^2 $! Meanwhile the derivative id softsign is discontinuous. \n\nc)  It is not clear is the softsign is used besides the activation function: In page 5 is said \u201cR_BRE can be applied on ant rectified layer before the nolinearity\u201d  . This seems tt the authors propose to add a second activation function (the softsign), why not use the one is in teh layer? \n\nd) The authors found hard to regularize the gradient $\\nabla_x D(x)$, even they tray tanh and cosine based activations.  It seems that effectively, the  introduce their additional softsign in the process. \n\ne) En the definition of R_AC, I denoted by <k,l> the pair of nodes (k \\ne l).  However, I think that it should be for pair in the same layer. It is not clear in the paper. \n\nf) It is supposed that the L_1 regularization motes the weights to be informative, this work is doing something similar.  How is it compared  the L_1 regularization vs. the proposal? \n\nRecommendation\nI tried to read the paper several times and I accept that it was very hard to me.  The most difficult part is the lack of precision on the maths, it is hard to figure out what the authors contribution indeed are.  I think there is some merit in the work.  However, it is not very well organized and many points are not defined.  In my opinion, the paper is in a preliminary stage and should be refined.  I recommend a \u201cSOFT\u201d REJECT[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]