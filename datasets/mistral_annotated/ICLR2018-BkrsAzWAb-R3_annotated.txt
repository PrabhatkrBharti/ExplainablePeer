\nThis paper revisits an interesting and important trick to automatically adapt the stepsize.  They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize.  Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice. \n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance.  But before that, there are several issues need to be addressed.  \n\n1, the derivation of the update of \\alpha relies on the expectation formulation.  I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick.  \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing.  \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments.  Moreover, the empirical comparisons are only conducted on MNIST.  To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet.  \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes.  Could you please explain why such phenomenon happens?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]