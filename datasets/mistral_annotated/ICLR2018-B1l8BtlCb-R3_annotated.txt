This paper can be seen as an extension of the paper \"attention is all you need\" that will be published at nips in a few weeks (at the time I write this review).  \n\nThe goal here is to make the target sentence generation non auto regressive.  The authors propose to introduce a set of latent variables to represent the fertility of each source words.  The number of target words can be then derived and they're all predicted in parallel. \n\nThe idea is interesting and trendy.  However, the paper is not really stand alone.  A lot of tricks are stacked to reduce the performance degradation.  However, they're sometimes to briefly described to be understood by most readers.  \n\nThe training process looks highly elaborate with a lot of hyper parameters.  Maybe you could comment on this.  \n\nFor instance, the use fertility supervision during training could be better motivated and explained.  Your choice of IBM 2 is wired since it doesn't include fertility.  Why not IBM 4, for instance ?  How you use IBM model for supervision.  This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated. [[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]