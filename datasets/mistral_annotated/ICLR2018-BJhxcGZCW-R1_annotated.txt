SUMMARY.\n\nThe paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting. \nThe model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities. \nThe encoder maps the input to a probabilistic latent space. \nFinally, a generator is used to generate entity pairs give a relation. \n\n----------\n\nOVERALL JUDGMENT\nThe paper presents a clever use of VAEs for generating entity pairs conditioning on relations. \nMy main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set. \nIf this is the case, all the analysis and results obtained are almost meaningless. \nI suggest the authors make clear if they used the split training, validation, test. \nUntil then it is not possible to draw any conclusion from this work. \n\nAssuming the experimental setting is correct, it is not clear to me the reason of having the representation of r (one-hot-vector of the relation) also in the decoding/generation part. \nThe hidden representation obtained by the encoder should already capture information about the relation. \nIs there a specific reason for doing so?\n\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-POS]]