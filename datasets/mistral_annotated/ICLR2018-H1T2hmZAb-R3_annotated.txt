Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions.  Their \"related work section\" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations.  However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued. \n\nTheir contributions are:\n\n1. Formulate complex valued convolution \n2. Formulate two complex-valued alternatives to ReLU and compare them \n3. Formulate complex batch normalization as a \"whitening\" operation on complex domain \n4. Formulate complex analogue of Glorot weight normalization scheme \n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case.  For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward.  However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic.  Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]