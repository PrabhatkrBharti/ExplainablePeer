The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the \"fourier feature\" corresponding to the kernel at a set of randomly sampled quadrature points.  This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks.  The idea is straightforward,  the analysis seems correct,  and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels.  The work is original, but I would say incremental, and the relevant literature is cited. \n\nThe method seems to give significantly lower kernel approximation errors,  but the significance of the performance difference in downstream ML tasks is unclear  --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort.  Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art.  The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g. equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral. \n\nIt would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error.  It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs).  On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me) ,  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density.[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]