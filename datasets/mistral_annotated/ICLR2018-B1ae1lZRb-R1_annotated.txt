The authors investigate knowledge distillation as a way to learn low precision networks.  They propose three training schemes to train a low precision student network from a teacher network.  They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\n Pros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\n Cons:\n(-) The gap with full precision models is still large \n(-)  Transferability of the learned low precision models to other tasks is not discussed\n\n The authors tackle a very important problem, the one of learning low precision models without comprosiming performance.  For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks.  One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3).  As a matter of fact, under some scenarios performance even decreases.  \n\nThe authors do not discuss the gains of their best low-precision regime in terms of computation and memory. \n\nFinally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k).  How good are the low-precision models trained by the authors at transferring to other tasks?  Is it possible to transfer student-teacher training practices to other tasks?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]