This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree.  The contributions of the work are very limited.   Basic attention models, which have been shown to help model structures, are not included (or compared).  Method-wise, the encoder is not novel and decoder is rather straightforward.  The contributions of the work are in general very limited.  Moreover, this manuscript contains many grammatical errors.   In general, it is not ready for publication.  \n\nPros:\n- Investigating the ability of distributed representation in encoding input structured is in general interesting.  Although there have been much previous work, this paper is along this line. \n\nCons:\n- The contributions of the work are very limited.  For example, attention, which have been widely used and been shown to help capture structures in many tasks, are not included and compared in this paper. \n- Evaluation is not very convincing.  The baseline performance in MT is too low.  It is unclear if the proposed model is still helpful when other components are considered (e.g., attention).  \n- For the objective function defined in the paper, it may be hard to balance the \"structure loss\" and \"content loss\" in different problems, and moreover, the loss function may not be even useful in real tasks (e.g, in MT), which often have their own objectives (as discussed in this paper).  Earlier work on tree kernels (in terms of defining tree distances) may be related to this work.  \n- The manuscript is full of grammatical errors, and the following are some of them:\n\"encoder only only need to\"\n\"For for tree reconstruction task\" \n\"The Socher et al. (2011b) propose a basic form \"\n\"experiments and theroy analysis are done\"\n[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-NEG],[ENG-POS],[ACC-POS],[CST-POS],[NOV-POS],[ETH-NEG]]