The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning.  Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods.  The experiment results seem solid and the proposed structure is with simple design and highly generalizable.  The concern is that the contribution is quite incremental from the theoretical side  though it involves large amount of experimental efforts, which could be impactful.  Please see the major comment below.\n\nOne major comment:\n- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side.  \nSpecifically, for two folds: \n(1) in general, some more work in investigating the task space would be nice.  The paper assumes the tasks are \u201crelated\u201d or \u201csimilar\u201d and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution.  But what exactly should the distribution be like to be learnable and how to quantify such \u201crelated\u201d or \u201csimilar\u201d relationship across tasks?  \n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods?  The result from the paper seems to answer with \u201cin all cases\u201d but then that always brings the issue of \u201coverfitting\u201d or parameter tuning issue. \n\nMore detailed comments:\n- On Page 1, \u201cthe optimal strategy for an arbitrary range of tasks\u201d lacks definition of \u201crange\u201d; also, in the setting in this paper, these tasks should share \u201csimilarity\u201d or follow the same \u201cdistribution\u201d and thus such \u201carbitrariness\u201d is actually constrained. \n\n- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined.  It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the \u201ctask distribution\u201d is too large to be meaningful.  One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable. \n\n- For results in Table 1 and Table 2, how are the confidence intervals computed?  Is it over multiple runs or within the same run?  It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests.  The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming \u201cnot statistically-significantly different\u201d because no significance test is reported.  \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?[[CLA-POS],[JUS-POS],[DEP-POS],[FAI-POS],[CON-POS],[ENG-NEG],[ACC-POS],[CST-POS],[NOV-NEG],[ETH-POS]]